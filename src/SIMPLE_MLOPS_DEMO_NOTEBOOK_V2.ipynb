{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2c058-b29e-4791-9747-fdd5033e2884",
   "metadata": {
    "language": "python",
    "name": "cell86"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from streamlit import dataframe as sdf\n",
    "import streamlit as st\n",
    "\n",
    "def create_retail_dataset(seed=42):\n",
    "    \"\"\"\n",
    "    Creates an artificial retail dataset with specified parameters.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: The generated dataset\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Constants\n",
    "    NUM_CUSTOMERS = 10000\n",
    "    START_DATE = datetime(2024, 1, 1)\n",
    "    END_DATE = datetime(2025, 1, 3)\n",
    "    CHANNEL_SWITCH_DATE = datetime(2024, 6, 1)\n",
    "    MEMBERSHIP_CHANGE_START = datetime(2024, 6, 1)\n",
    "    MEMBERSHIP_CHANGE_END = datetime(2024, 7, 1)\n",
    "    MIN_AMOUNT = 10\n",
    "    MAX_AMOUNT = 150\n",
    "    INITIAL_MEMBER_RATIO = 0.25\n",
    "    ADDITIONAL_CONVERSION_RATIO = 0.15\n",
    "    \n",
    "    # Generate customer data\n",
    "    customer_ids = [f\"CUST_{i:06d}\" for i in range(1, NUM_CUSTOMERS + 1)]\n",
    "    genders = np.random.choice(['MALE', 'FEMALE'], size=NUM_CUSTOMERS)\n",
    "    \n",
    "    # Initial membership status - 25% are MEMBERs\n",
    "    initial_memberships = np.random.choice(\n",
    "        ['MEMBER', 'NON_MEMBER'], \n",
    "        size=NUM_CUSTOMERS, \n",
    "        p=[INITIAL_MEMBER_RATIO, 1-INITIAL_MEMBER_RATIO]\n",
    "    )\n",
    "    \n",
    "    # Determine which non-members will convert to members during the conversion period\n",
    "    # First identify all non-members\n",
    "    non_member_indices = [i for i, status in enumerate(initial_memberships) if status == 'NON_MEMBER']\n",
    "    # Calculate how many should convert (15% of non-members)\n",
    "    num_to_convert = int(len(non_member_indices) * ADDITIONAL_CONVERSION_RATIO / (1 - INITIAL_MEMBER_RATIO))\n",
    "    # Randomly select which ones convert\n",
    "    convert_indices = np.random.choice(non_member_indices, size=num_to_convert, replace=False)\n",
    "    \n",
    "    # Create customer lookup dictionary with conversion date for those who will convert\n",
    "    customers = {}\n",
    "    for i in range(NUM_CUSTOMERS):\n",
    "        conversion_date = None\n",
    "        if i in convert_indices:\n",
    "            # Randomly select a date in the conversion period\n",
    "            days_in_period = (MEMBERSHIP_CHANGE_END - MEMBERSHIP_CHANGE_START).days\n",
    "            random_days = random.randint(0, days_in_period - 1)\n",
    "            conversion_date = MEMBERSHIP_CHANGE_START + timedelta(days=random_days)\n",
    "            \n",
    "        customers[customer_ids[i]] = {\n",
    "            'gender': genders[i],\n",
    "            'initial_membership': initial_memberships[i],\n",
    "            'conversion_date': conversion_date\n",
    "        }\n",
    "    \n",
    "    # Calculate weeks in the date range\n",
    "    current_date = START_DATE\n",
    "    weeks = []\n",
    "    while current_date <= END_DATE:\n",
    "        weeks.append(current_date)\n",
    "        current_date += timedelta(days=7)\n",
    "    \n",
    "    # Generate transactions\n",
    "    data = []\n",
    "    \n",
    "    for week_start in weeks:\n",
    "        week_end = week_start + timedelta(days=6)\n",
    "        for customer_id, attributes in customers.items():\n",
    "            # 1-3 transactions per week per customer\n",
    "            num_transactions = random.randint(1, 3)\n",
    "            \n",
    "            for _ in range(num_transactions):\n",
    "                # Generate transaction date within the week\n",
    "                days_offset = random.randint(0, 6)\n",
    "                transaction_date = week_start + timedelta(days=days_offset)\n",
    "                \n",
    "                # Determine membership status for this transaction date\n",
    "                if attributes['initial_membership'] == 'MEMBER':\n",
    "                    membership_status = 'MEMBER'\n",
    "                elif attributes['conversion_date'] and transaction_date >= attributes['conversion_date']:\n",
    "                    membership_status = 'MEMBER'\n",
    "                else:\n",
    "                    membership_status = 'NON_MEMBER'\n",
    "                \n",
    "                # Determine transaction channel based on date\n",
    "                if transaction_date < CHANNEL_SWITCH_DATE:\n",
    "                    channel_prob = 0.75\n",
    "                    transaction_channel = 'IN_SHOP' if random.random() < channel_prob else 'ONLINE'\n",
    "                else:\n",
    "                    channel_prob = 0.25\n",
    "                    transaction_channel = 'IN_SHOP' if random.random() < channel_prob else 'ONLINE'\n",
    "                \n",
    "                # Base transaction amount\n",
    "                base_amount = random.uniform(MIN_AMOUNT, MAX_AMOUNT)\n",
    "                \n",
    "                # Apply gender modifier\n",
    "                gender_modifier = random.uniform(1.1, 1.3) if attributes['gender'] == 'MALE' else 1.0\n",
    "                \n",
    "                # Apply membership modifier\n",
    "                membership_modifier = random.uniform(1.1, 1.3) if membership_status == 'MEMBER' else 1.0\n",
    "                \n",
    "                # Final transaction amount\n",
    "                transaction_amount = round(base_amount * gender_modifier * membership_modifier, 2)\n",
    "                \n",
    "                # Add transaction to data\n",
    "                data.append({\n",
    "                    'CUSTOMER_ID': customer_id,\n",
    "                    'DATE': transaction_date.strftime('%Y-%m-%d'),\n",
    "                    'TRANSACTION_AMOUNT': transaction_amount,\n",
    "                    'TRANSACTION_CHANNEL': transaction_channel,\n",
    "                    'GENDER': attributes['gender'],\n",
    "                    'MEMBERSHIP_STATUS': membership_status\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Balance monthly transaction amounts\n",
    "    df['MONTH'] = pd.to_datetime(df['DATE']).dt.to_period('M')\n",
    "    monthly_totals = df.groupby('MONTH')['TRANSACTION_AMOUNT'].sum()\n",
    "    target_monthly_average = monthly_totals.mean()\n",
    "    \n",
    "    for month in monthly_totals.index:\n",
    "        month_mask = df['MONTH'] == month\n",
    "        current_total = monthly_totals[month]\n",
    "        adjustment_factor = target_monthly_average / current_total\n",
    "        df.loc[month_mask, 'TRANSACTION_AMOUNT'] = df.loc[month_mask, 'TRANSACTION_AMOUNT'] * adjustment_factor\n",
    "    \n",
    "    # Round transaction amounts to 2 decimal places\n",
    "    df['TRANSACTION_AMOUNT'] = df['TRANSACTION_AMOUNT'].round(2)\n",
    "    \n",
    "    # Drop temporary column and sort by date\n",
    "    df = df.drop(columns=['MONTH']).sort_values('DATE')\n",
    "\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['DATE'] = df['DATE'].dt.date\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1028b-358c-485f-b26e-9464dcb4cb96",
   "metadata": {
    "language": "python",
    "name": "cell91"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Generate and store customers and transactions \n",
    "transactions = create_retail_dataset()\n",
    "\n",
    "# Create dataset with transactions\n",
    "transactions_df = session.write_pandas(transactions.drop(['GENDER','MEMBERSHIP_STATUS'], axis=1), table_name='_TRANSACTIONS', database='SIMPLE_MLOPS_DEMO', schema='_DATA_GENERATION', auto_create_table=True, overwrite=True)\n",
    "\n",
    "\n",
    "# Create dataset with customer history \n",
    "# Get the status for each customer on 2024-01-01\n",
    "first_row_per_customer = transactions.drop(['TRANSACTION_AMOUNT','TRANSACTION_CHANNEL'], axis=1).sort_values(by=['CUSTOMER_ID', 'DATE']).drop_duplicates(subset=['CUSTOMER_ID'], keep='first')\n",
    "first_row_per_customer['DATE'] = '2024-01-01'\n",
    "\n",
    "# Get the customer history (changes for membership status) for every customer\n",
    "customers_df = transactions.sort_values(by=['CUSTOMER_ID', 'DATE'])\n",
    "customers_df['STATUS_CHANGE'] = customers_df['MEMBERSHIP_STATUS'] != customers_df.groupby('CUSTOMER_ID')['MEMBERSHIP_STATUS'].shift()\n",
    "customers_df = customers_df[customers_df['STATUS_CHANGE']].drop(['TRANSACTION_AMOUNT', 'TRANSACTION_CHANNEL', 'STATUS_CHANGE'], axis=1)\n",
    "\n",
    "# Combine both\n",
    "customers_df = pd.concat([customers_df, first_row_per_customer])\n",
    "customers_df['DATE'] = pd.to_datetime(customers_df['DATE'])\n",
    "customers_df['DATE'] = customers_df['DATE'].dt.date\n",
    "customers_df = customers_df.sort_values(by=['CUSTOMER_ID', 'DATE'])\n",
    "customers_df = customers_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "customers_df = session.write_pandas(customers_df, table_name='_CUSTOMERS', database='SIMPLE_MLOPS_DEMO', schema='_DATA_GENERATION', auto_create_table=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2c2c3-d0a9-4ffa-a857-2357b675eb0d",
   "metadata": {
    "language": "python",
    "name": "cell83"
   },
   "outputs": [],
   "source": [
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c17d4-067c-4358-be38-87a046dce257",
   "metadata": {
    "language": "python",
    "name": "cell89"
   },
   "outputs": [],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a292b8-0fb7-4a0c-9aa7-1f41eeb4d7bf",
   "metadata": {
    "language": "python",
    "name": "cell75"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from the records\n",
    "revenue_df = pd.DataFrame(records, columns=['DATE', 'REVENUE'])\n",
    "revenue_df = session.create_dataframe(revenue_df)\n",
    "revenue_in_shop = revenue_df.filter(col('DATE') < lit('2024-06-01'))\n",
    "revenue_online = revenue_df.filter(col('DATE') >= lit('2024-06-01'))\n",
    "\n",
    "revenue_in_shop.join_table_function('SIMPLE_MLOPS_DEMO._DATA_GENERATION.GENERATE_TRANSACTIONS', col('REVENUE'), lit([0.75,0.25])).drop('REVENUE').write.save_as_table(table_name='SIMPLE_MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS', mode='overwrite')\n",
    "revenue_online.join_table_function('SIMPLE_MLOPS_DEMO._DATA_GENERATION.GENERATE_TRANSACTIONS',col('REVENUE'), lit([0.25,0.75])).drop('REVENUE').write.save_as_table(table_name='SIMPLE_MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS', mode='append')\n",
    "session.table('SIMPLE_MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').select('CUSTOMER_ID').distinct().write.save_as_table('SIMPLE_MLOPS_DEMO._DATA_GENERATION._CUSTOMERS')\n",
    "return \"Demo Environment is setup.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86104e-665c-451d-9ecf-0b840bc1522e",
   "metadata": {
    "language": "sql",
    "name": "cell74"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "-- Enable logs, metrics and traces\n",
    "ALTER DATABASE SIMPLE_MLOPS_DEMO SET LOG_LEVEL = INFO;\n",
    "ALTER DATABASE SIMPLE_MLOPS_DEMO SET METRIC_LEVEL = ALL;\n",
    "ALTER DATABASE SIMPLE_MLOPS_DEMO SET TRACE_LEVEL = ALWAYS;\n",
    "\n",
    "-- Create warehouses\n",
    "CREATE WAREHOUSE IF NOT EXISTS COMPUTE_WH WITH WAREHOUSE_SIZE='X-SMALL';\n",
    "CREATE WAREHOUSE IF NOT EXISTS FEATURE_STORE_WH WITH WAREHOUSE_SIZE='MEDIUM';\n",
    "\n",
    "-- Create schema for setup\n",
    "CREATE OR REPLACE SCHEMA SIMPLE_MLOPS_DEMO._DATA_GENERATION;\n",
    "\n",
    "CREATE OR REPLACE FUNCTION SIMPLE_MLOPS_DEMO._DATA_GENERATION.GENERATE_TRANSACTIONS (REVENUE FLOAT, CHANNEL ARRAY)\n",
    "  returns TABLE (CUSTOMER_ID STRING, TRANSACTION_AMOUNT FLOAT, TRANSACTION_CHANNEL STRING)\n",
    "  language python\n",
    "  runtime_version = '3.11'\n",
    "  packages=('numpy','scipy')\n",
    "  handler = 'GenerateTransactions'\n",
    "as\n",
    "$$\n",
    "from scipy.stats import truncnorm\n",
    "import numpy as np\n",
    "\n",
    "class GenerateTransactions:\n",
    "    # Draw a number from a normal distribution with defined mean, std_dev, lower and upper bounds\n",
    "    def get_norm_value(self, min, max, mean, std_dev):\n",
    "        # Calculate the a and b parameters for truncnorm\n",
    "        min = (min - mean) / std_dev\n",
    "        max = (max - mean) / std_dev\n",
    "        \n",
    "        # Generate the truncated normal distribution\n",
    "        truncated_data = truncnorm.rvs(min, max, loc=mean, scale=std_dev, size=1)[0]\n",
    "        return truncated_data\n",
    "        \n",
    "    def process(self, revenue, in_shop_online):\n",
    "        customer_id = 0\n",
    "        while revenue > 0:\n",
    "            # customer id\n",
    "            if (customer_id >= 0) and (customer_id < 100):\n",
    "                transaction_amount = np.round(self.get_norm_value(5, 25, 20, 5),2)\n",
    "            elif (customer_id >= 100) and (customer_id < 200):\n",
    "                transaction_amount = np.round(self.get_norm_value(25, 50, 40, 5),2)\n",
    "            elif (customer_id >= 200) and (customer_id < 300):\n",
    "                transaction_amount = np.round(self.get_norm_value(50, 100, 80, 10),2)\n",
    "            else:\n",
    "                transaction_amount = np.round(self.get_norm_value(100, 150, 120, 10),2)\n",
    "            transaction_channel = np.random.choice(['IN_SHOP','ONLINE'],p=in_shop_online)\n",
    "            revenue = revenue - transaction_amount\n",
    "            if customer_id == 1000:\n",
    "                customer_id = 0\n",
    "            customer_id = customer_id +1\n",
    "            customer_id_str = f\"CUST_{customer_id:06d}\"\n",
    "            yield (customer_id_str, transaction_amount, transaction_channel) \n",
    "$$\n",
    ";\n",
    "\n",
    "-- Setup Procedure\n",
    "CREATE OR REPLACE PROCEDURE SIMPLE_MLOPS_DEMO._DATA_GENERATION.DATA_GENERATION()\n",
    "  RETURNS STRING\n",
    "  LANGUAGE PYTHON\n",
    "  RUNTIME_VERSION = '3.11'\n",
    "  PACKAGES = ('snowflake-snowpark-python','pandas','numpy')\n",
    "  HANDLER = 'run'\n",
    "  AS\n",
    "$$\n",
    "def run(session):\n",
    "    from snowflake.snowpark.functions import lit, col\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Define the date range\n",
    "    start_date = '2024-01-01'\n",
    "    end_date = '2025-01-31'\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Define base revenue (this is a baseline you can adjust)\n",
    "    base_revenue = 50000\n",
    "    \n",
    "    # Prepare a list to store computed records\n",
    "    records = []\n",
    "    \n",
    "    for date in dates:\n",
    "        # Determine the day-of-week: Monday=0, ... , Sunday=6\n",
    "        weekday = date.weekday()\n",
    "        \n",
    "        # Weekday effect: Saturdays have the highest revenue,\n",
    "        # Sundays are lower, and the rest are normal.\n",
    "        if weekday == 5:       # Saturday\n",
    "            weekday_factor = 1.5\n",
    "        elif weekday == 6:     # Sunday\n",
    "            weekday_factor = 0.9\n",
    "        else:\n",
    "            weekday_factor = 1.0\n",
    "        \n",
    "        # Month effect: June, July, August, and December get a boost.\n",
    "        if date.month in [6, 7, 8, 12]:\n",
    "            month_factor = 1.15\n",
    "        else:\n",
    "            month_factor = 1.0\n",
    "        \n",
    "        # Add random noise (mean 1, small standard deviation) to simulate natural fluctuations\n",
    "        noise_factor = np.random.normal(loc=1, scale=0.05)\n",
    "        \n",
    "        # Compute the final revenue\n",
    "        revenue = base_revenue * weekday_factor * month_factor * noise_factor\n",
    "        revenue = max(0, revenue)  # Ensure no negative revenue\n",
    "        \n",
    "        # Append the result as a tuple (DATE, REVENUE)\n",
    "        records.append((date, revenue))\n",
    "    \n",
    "    # Create a DataFrame from the records\n",
    "    revenue_df = pd.DataFrame(records, columns=['DATE', 'REVENUE'])\n",
    "    revenue_df = session.create_dataframe(revenue_df)\n",
    "    revenue_in_shop = revenue_df.filter(col('DATE') < lit('2024-06-01'))\n",
    "    revenue_online = revenue_df.filter(col('DATE') >= lit('2024-06-01'))\n",
    "    \n",
    "    revenue_in_shop.join_table_function('SIMPLE_MLOPS_DEMO._DATA_GENERATION.GENERATE_TRANSACTIONS', col('REVENUE'), lit([0.75,0.25])).drop('REVENUE').write.save_as_table(table_name='SIMPLE_MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS', mode='overwrite')\n",
    "    revenue_online.join_table_function('SIMPLE_MLOPS_DEMO._DATA_GENERATION.GENERATE_TRANSACTIONS',col('REVENUE'), lit([0.25,0.75])).drop('REVENUE').write.save_as_table(table_name='SIMPLE_MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS', mode='append')\n",
    "    session.table('SIMPLE_MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').select('CUSTOMER_ID').distinct().write.save_as_table('SIMPLE_MLOPS_DEMO._DATA_GENERATION._CUSTOMERS')\n",
    "    return \"Demo Environment is setup.\"\n",
    "$$\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56187de4-f4f6-40cc-9408-bc059ec535ce",
   "metadata": {
    "language": "sql",
    "name": "cell77"
   },
   "outputs": [],
   "source": [
    "CALL SIMPLE_MLOPS_DEMO._DATA_GENERATION.DATA_GENERATION();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090cc90-29da-47e1-bd9b-2f7ac5eedad5",
   "metadata": {
    "collapsed": false,
    "name": "SETUP_0"
   },
   "source": [
    "# Use Case: Predicting Future Customer Revenue Using Historical Transaction Data 📊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59214b-4111-4c5d-8a06-1e9f091a4620",
   "metadata": {
    "collapsed": false,
    "name": "SETUP_1"
   },
   "source": [
    "# 1 - Setup Demo 🛠️\n",
    "* Import required libraries\n",
    "* Create a Snowpark session\n",
    "\n",
    "| Library    | Use |\n",
    "| -------- | ------- |\n",
    "| `snowflake.snowpark` | Main Python Developer Framework for Snowflake including the DataFrame-API     |\n",
    "| `snowflake.ml`    | Snowflake ML specific functions including Feature Store & Model Registry APIs    |\n",
    "| `snowflake.cortex`    | Snowflake APIs to access Cortex Services (e.g. LLMs)    |\n",
    "| `notebook_extras`  | Convenience Functions for Snowflake Notebooks. More details [here](google.com).    |\n",
    "| `demo_extras`  | Demo-specific functions (Data Generation, Use Case flow, etc.)     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fbb66-2845-488b-9878-1a90da9edc53",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "SETUP_2"
   },
   "outputs": [],
   "source": [
    "# Helper functions for this demo\n",
    "from demo_extras.flow import Demoflow\n",
    "from demo_extras.model_trainer import ModelTrainer\n",
    "from notebook_extras.cortex import CortexPilot\n",
    "from notebook_extras.model_registry import ModelRegistryHelper\n",
    "from notebook_extras.lineage import LineageHelper\n",
    "from notebook_extras.misc import get_snowsight_url\n",
    "\n",
    "\n",
    "# Import python packages\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st\n",
    "from streamlit import dataframe as sdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import warnings\n",
    "import logging\n",
    "from opentelemetry import trace\n",
    "from snowflake import telemetry\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Import Snowflake packages\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import lit, col, sproc\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.monitoring.entities.model_monitor_config import ModelMonitorSourceConfig, ModelMonitorConfig\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.core import Root, CreateMode\n",
    "\n",
    "demo_flow = Demoflow()\n",
    "demo_flow.setup()\n",
    "\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23627ed5-7e8c-4017-a270-c27af9c06676",
   "metadata": {
    "collapsed": false,
    "name": "EXPLORATION_0"
   },
   "source": [
    "# 2 - Data Exploration & Visualization\n",
    "\n",
    "* `session.table()` creates a reference to a table\n",
    "* `count()`, `order_by()`, `describe()` are dataframe operations\n",
    "* `describe()` gives us insights into the transaction amounts (e.g. min, average, max, count).\n",
    "\n",
    "We can see that we have roughly 50K transactions across 350 customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15bb06-d009-458a-8627-fe407729cbf6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "EXPLORATION_1"
   },
   "outputs": [],
   "source": [
    "customers_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.CUSTOMERS')\n",
    "transactions_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n",
    "\n",
    "print(f'Number of customers: {customers_df.select(\"CUSTOMER_ID\").distinct().count()}')\n",
    "print('Customer Data:')\n",
    "customers_df.order_by(col('CUSTOMER_ID'),col('DATE')).show(n=5)\n",
    "\n",
    "print('Quick Variable Analysis:')\n",
    "customers_df.describe().order_by('SUMMARY').show()\n",
    "\n",
    "print(f'Number of transactions: {transactions_df.count()}')\n",
    "\n",
    "print('Transactions Data:')\n",
    "transactions_df.order_by(col('CUSTOMER_ID'),col('DATE')).show(n=5)\n",
    "\n",
    "print('Quick Variable Analysis:')\n",
    "transactions_df.describe().order_by('SUMMARY').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d374a-b912-4600-a0a4-a3f82d2d1b4d",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "### Plotting Data\n",
    "You can use libraries such as plotly or matplotlib to visualize your data. However, instead of coding the plots manually, we'll leverage GenAI models hosted natively in Snowflake to automatically generate the visualizations.\n",
    "\n",
    "* This notebook comes with your own personal 🤖 **CortexPilot** powered by Cortex LLMs   \n",
    "    * `ui_plotting()` -> UI-driven plotting with GenAI\n",
    "    * `f_cortex_helper_visualize_query()` -> function that receives a Snowpark or Pandas Dataframe and a prompt (in case you already know the dataframe and query)\n",
    "\n",
    "Both functions utilize Snowflake's [complete()](https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/api/cortex/snowflake.cortex.complete) function to access LLMs natively hosted in Snowflake.\n",
    "\n",
    "Try asking the following questions:  \n",
    "* ***What was the overall revenue per channel and month? Use a stacked bar plot and use YY-Monthname for the x-axis. Make sure the x-axis is ordered.***\n",
    "* ***What was the total transaction amount per channel? Use a pie chart.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea49b0-155f-4bae-b086-0dc15f66c5c9",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "from snowflake.cortex import complete, CompleteOptions\n",
    "from snowflake.snowpark.functions import approx_count_distinct, col\n",
    "from snowflake.snowpark.window import Window\n",
    "\n",
    "USER_PROMPT_TEMPLATE_DESCRIBE_DATA = \"\"\"\n",
    "You are provided with a dataframe that has the following statistics for every column of my dataframe:\n",
    "* count (total count of values)\n",
    "* max (max value for numerical columns, first alphabetical value for categorical columns)\n",
    "* mean (mean value for numerical columns, not applicable for categorical columns)\n",
    "* min (min value for numerical columns, last alphabetical value for categorical columns)\n",
    "* stddev (standard deviation for numerical columns, not applicable for categorical columns)\n",
    "* top (most common value for categorical variables)\n",
    "* freq (most common value’s frequency)\n",
    "* unique (number of unique values for categorical columns, not applicable for numerical columns)\n",
    "* datatype (string which is a categorical column, other values are numerical columns)\n",
    "\n",
    "Based on this information, provide insights into potential data quality issues that could impact training of a machine learning model.  \n",
    "\n",
    "Specifically, please identify and discuss:  \n",
    "\n",
    "1. **Missing Values:** Columns with significantly lower `count` values compared to others, which could indicate missing data.  \n",
    "2. **Outliers:** Columns where `min` or `max` values are far from the `mean`, or where `stddev` is unusually high, suggesting extreme values.  \n",
    "3. **Data Skewness:** Columns where the `mean` is significantly different from the `min` and `max`, indicating skewed distributions.  \n",
    "4. **Potential Data Type Issues:** Columns where numerical statistics may indicate categorical or incorrectly formatted data.  \n",
    "5. **Feature Scaling Issues:** Columns with very large or very small values that might require normalization or standardization for ML models.  \n",
    "6. **Other Anomalies:** Any unusual patterns that could suggest data corruption, incorrect data entry, or inconsistencies.  \n",
    "\n",
    "Provide actionable recommendations on how to address any detected issues to improve the dataset's quality for machine learning.  \n",
    "Make sure to base your recommendations on the type of model that will be trained which will be:\n",
    "Model: {model_type}\n",
    "\n",
    "Return these recommendations as a markdown table for streamlit's st.markdown() function with the following columns:\n",
    "* Column Name\n",
    "* Issue found (description of the actual issue found)\n",
    "* Recommendation (description of steps recommended to mitigate the found issue)\n",
    "\n",
    "Make sure there is one row per combination of column and issue.\n",
    "Only return the markdown table.\n",
    "\n",
    "Here is the output of `df.describe()`:  \n",
    "\n",
    "{dataframe_sample}\n",
    "\"\"\"\n",
    "\n",
    "class CortexPilot():\n",
    "    def __init__(self, session=None, llm='mistral-large2', temperature=0, top_p=0):\n",
    "        if session is None:\n",
    "            self.session = get_active_session()\n",
    "        else:\n",
    "            self.session = session\n",
    "        self.llm = llm\n",
    "        self.llm_options = CompleteOptions(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        self.dataframe_type_icons = {\n",
    "            \"pandas.core.frame.DataFrame\": \"🐼\",\n",
    "            \"snowflake.snowpark.dataframe.DataFrame\": \"❄️\",\n",
    "            \"snowflake.snowpark.table.Table\": \"❄️\"\n",
    "        }\n",
    "        st.session_state['suggested_prompts'] = None\n",
    "        \n",
    "    def f_cortex_helper_describe_data(self, df, model_type='XGBoost Classifier'):\n",
    "        \"\"\"\n",
    "        Requests the LLM to analyze potential issues in your data when used as training data for a machine learning model, based on the output of Snowpark's `describe()` function.  \n",
    "        Returns the analyzed dataframe along with a summary of detected issues and recommendations for improvement given a model type.\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            df = self.session.create_dataframe(df)\n",
    "        df_describe = self._get_dataframe_description(df).reset_index(drop=True)\n",
    "        #df_describe = df.describe().to_pandas()\n",
    "        df_for_prompt = df_describe.to_markdown()\n",
    "        user_query = USER_PROMPT_TEMPLATE_DESCRIBE_DATA.format(dataframe_sample = df_for_prompt, model_type=model_type)\n",
    "        llm_input = [{\"role\": \"user\", \"content\": user_query}]\n",
    "        llm_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n",
    "        return df_describe, llm_response\n",
    "\n",
    "    def _analyze_unique_values(self, df):\n",
    "        \"\"\"\n",
    "        Get unique counts per string column.\n",
    "        \"\"\"\n",
    "        categorical_columns = [col[0] for col in df.dtypes if col[1].startswith(\"string\")]\n",
    "        df_unique_counts = df.select(\n",
    "            [approx_count_distinct(col(c)).alias(f\"{c}\") for c in categorical_columns]\n",
    "        ).to_pandas()\n",
    "        \n",
    "        df_unique_counts['SUMMARY'] = 'unique'\n",
    "        return df_unique_counts\n",
    "    \n",
    "    def _analyze_column_datatypes(self, df, describe_columns):\n",
    "        \"\"\"\n",
    "        Create a dataframe with column names and their datatype\n",
    "        \"\"\"\n",
    "        df_dtypes = pd.DataFrame(df.select(describe_columns).dtypes)\n",
    "        #df_dtypes[1] = df_dtypes[1].apply(lambda x: x.replace('(16777216)',''))\n",
    "        df_dtypes[1] = df_dtypes[1].str.replace(r\"\\(.*\\)\", \"\", regex=True)\n",
    "        df_dtypes = df_dtypes.set_index(0).T\n",
    "        df_dtypes['SUMMARY'] = 'datatype'\n",
    "        return df_dtypes\n",
    "    \n",
    "    def _describe(self, df):\n",
    "        \"\"\"\n",
    "        Run describe on provided dataframe and return list of of columns\n",
    "        \"\"\"\n",
    "        describe_result = df.describe().to_pandas().round(3)\n",
    "        describe_columns = list(describe_result.drop('SUMMARY',axis=1).columns)\n",
    "        return describe_result, describe_columns\n",
    "    \n",
    "    def _get_dataframe_description(self, df):\n",
    "        \"\"\"\n",
    "        Describe dataframe using Snowpark describe() and enrich it with additional analysis.\n",
    "        \"\"\"\n",
    "        describe_result, describe_columns = self._describe(df)\n",
    "        df_unique_counts = self._analyze_unique_values(df)\n",
    "        df_top_freq = self._get_freq_top_(df)\n",
    "        df_dtypes = self._analyze_column_datatypes(df, describe_columns)\n",
    "        df_describe = pd.concat([describe_result, df_top_freq, df_unique_counts, df_dtypes])\n",
    "        df_describe = df_describe.where(pd.notna(df_describe), '')\n",
    "        return df_describe\n",
    "\n",
    "    def _get_freq_top_(self, df):\n",
    "        categorical_columns = [col[0] for col in df.dtypes if col[1].startswith(\"string\")]\n",
    "        for col_i, _col in enumerate(categorical_columns):\n",
    "            window_spec = Window.order_by(col(\"count\").desc(), col(_col))\n",
    "            _col_df = (\n",
    "                df.group_by(_col)\n",
    "                .agg(F.count(\"*\").alias(\"count\"))\n",
    "                .with_column(\"rank\", F.rank().over(window_spec))\n",
    "                .filter(col(\"rank\") == 1)\n",
    "                .select(col(_col).alias('\"top\"'), col(\"count\").alias('\"freq\"'))\n",
    "            )\n",
    "            if col_i == 0:\n",
    "                top_freq_df = _col_df\n",
    "            else:\n",
    "                top_freq_df = top_freq_df.union_all_by_name(_col_df)\n",
    "        top_freq_df = top_freq_df.to_pandas()\n",
    "        top_freq_df.index = categorical_columns\n",
    "        top_freq_df = top_freq_df.T\n",
    "        top_freq_df = top_freq_df.reset_index(names='SUMMARY')\n",
    "        return top_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b1910-6162-49c2-adf8-3294baa80b89",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": [
    "# Get an instance of CortexPilot\n",
    "my_pilot = CortexPilot(llm='claude-3-5-sonnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d93b22-bb3c-472a-8e08-dc5b2e3185d2",
   "metadata": {
    "language": "python",
    "name": "cell82"
   },
   "outputs": [],
   "source": [
    "transactions_df2 = transactions_df.with_column('A', lit('abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f7066-a411-4455-a97c-21a4769ffeb0",
   "metadata": {
    "language": "python",
    "name": "cell65"
   },
   "outputs": [],
   "source": [
    "df_describe, llm_response = my_pilot.f_cortex_helper_describe_data(transactions_df2.drop('CUSTOMER_ID'), model_type='linear regression')\n",
    "st.dataframe(df_describe)\n",
    "st.markdown(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc1172-4ee6-4ea1-83b4-cba362307243",
   "metadata": {
    "language": "python",
    "name": "cell79"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "#from snowflake.snowpark.functions import col, count, rank\n",
    "from snowflake.snowpark.window import Window\n",
    "\n",
    "def most_frequent_value(df, columns):\n",
    "\n",
    "    for col_i, _col in enumerate(columns):\n",
    "        print(_col)\n",
    "        window_spec = Window.order_by(col(\"count\").desc(), col(_col))\n",
    "        _col_df = (\n",
    "            df.group_by(_col)\n",
    "            .agg(F.count(\"*\").alias(\"count\"))\n",
    "            .with_column(\"rank\", F.rank().over(window_spec))\n",
    "            .filter(col(\"rank\") == 1)\n",
    "            .select(col(_col).alias('\"top\"'), col(\"count\").alias('\"freq\"'))\n",
    "        )\n",
    "        _col_df.show()\n",
    "        if col_i == 0:\n",
    "            top_freq_df = _col_df\n",
    "        else:\n",
    "            top_freq_df = top_freq_df.union_all_by_name(_col_df)\n",
    "    top_freq_df = top_freq_df.to_pandas()\n",
    "    top_freq_df.index = columns\n",
    "    return top_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a4f62-a4b8-44e2-999c-94cb32b1b915",
   "metadata": {
    "language": "python",
    "name": "cell80"
   },
   "outputs": [],
   "source": [
    "a = most_frequent_value(transactions_df, ['TRANSACTION_CHANNEL'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0549e0d-eacf-4db5-9653-dda860bd01e6",
   "metadata": {
    "language": "python",
    "name": "cell81"
   },
   "outputs": [],
   "source": [
    "a.T.reset_index(names='SUMMARY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d492d29-0f5e-46d7-826c-a9fc09744564",
   "metadata": {
    "language": "python",
    "name": "cell67"
   },
   "outputs": [],
   "source": [
    "transactions_df.limit(20).to_pandas().describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cb160-a059-4b55-99e0-6b163a012d05",
   "metadata": {
    "language": "python",
    "name": "cell73"
   },
   "outputs": [],
   "source": [
    "df_describe, llm_response = my_pilot.f_cortex_helper_describe_data(transactions_df.drop('CUSTOMER_ID'))\n",
    "st.dataframe(df_describe)\n",
    "st.markdown(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db1c4ae-12f4-48ef-a0d4-75675e0280d3",
   "metadata": {
    "language": "python",
    "name": "cell78"
   },
   "outputs": [],
   "source": [
    "df = transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed834b6-0709-47c6-b539-f28f79f90633",
   "metadata": {
    "language": "python",
    "name": "cell69"
   },
   "outputs": [],
   "source": [
    "features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b7720-a8cb-4a3e-8530-071daa6200bd",
   "metadata": {
    "language": "python",
    "name": "cell58"
   },
   "outputs": [],
   "source": [
    "df_describe, llm_response = my_pilot.f_cortex_helper_analyze_describe_output(transactions_df)\n",
    "st.markdown(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36330fe-dd5f-41c3-854f-f62d92fa48b6",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# Open the UI\n",
    "my_pilot.ui_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ed6e6-70a3-4f8b-a61c-446cf9eaada8",
   "metadata": {
    "collapsed": false,
    "name": "cell23"
   },
   "source": [
    "When we plot the distribution of ONLINE vs. IN_SHOP revenue, we can see that 75% of our revenue comes from customer transactions that go into our shops.  \n",
    "A model trained on this data should recognize that IN_SHOP transactions are the major driver of future customer revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5466e-03ee-4b0d-a851-6e132ca07f1d",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "my_pilot.f_cortex_helper_visualize_query(transactions_df, 'What was the total transaction amount per channel? Use a pie chart.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0114ca-d50b-4a6b-a33d-ed29b98eb692",
   "metadata": {
    "collapsed": false,
    "name": "cell17"
   },
   "source": [
    "# 3 - Feature Store & Feature Engineering\n",
    "The Snowflake Feature Store enables data scientists and ML engineers to create, manage, and utilize machine learning features within machine learning pipelines.  \n",
    "A feature store consists of feature views, which encapsulate Python or SQL pipelines that transform raw data into one or more related features.  \n",
    "All features within a feature view are refreshed simultaneously from the source data.\n",
    "\n",
    "Feature store objects are implemented as Snowflake objects and all feature store objects are therefore subject to Snowflake access control rules.\n",
    "| Feature Store Object    | Snowflake Object |\n",
    "| -------- | ------- |\n",
    "| `FeatureStore` | Schema     |\n",
    "| `Entity`    | Tag    |\n",
    "| `FeatureView`  | Dynamic Table or View    |\n",
    "| `Feature`  | Column in a Dynamic Table or View    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c188a-b9d0-4da7-b6ab-7b85c3f3172b",
   "metadata": {
    "collapsed": false,
    "name": "cell26"
   },
   "source": [
    "### Setup the Feature Store\n",
    "We are creating (or referencing if it already exists) a Feature Store that is stored in the schema `FEATURE_STORE`.  \n",
    "The `default_warehouse` will be used to refresh features automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db378dc4-44e5-4b5b-b55c-e1a3e14ec663",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database='SIMPLE_MLOPS_DEMO', \n",
    "    name='FEATURE_STORE', \n",
    "    default_warehouse='FEATURE_STORE_WH',\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00471e-18fd-47d0-99ae-8b945e7b0bdb",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "### Create a Feature Store Entity\n",
    "Feature views are organized in the feature store according to the entities to which they apply. An entity is a higher-level abstraction that represents the subject matter of a feature.  \n",
    "In our example, the main entity is the `CUSTOMER` and the features we will create will be linked to this entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286517a-2a63-4565-aa22-63104d78f21b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "# Create a new entity for the Feature Store\n",
    "entity = Entity(name=\"CUSTOMER\", join_keys=[\"CUSTOMER_ID\"], desc='Unique identifier for customers.')\n",
    "fs.register_entity(entity)\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc666d-c1be-484d-b203-4fc26b99cf88",
   "metadata": {
    "collapsed": false,
    "name": "cell31"
   },
   "source": [
    "### Develop Features for Customer Transactions\n",
    "\n",
    "The Snowpark Python API provides analytics functions for easily defining many common feature types, such as windowed aggregations.  \n",
    "We will use `analytics.time_series_agg()` to quickly generate revenue for the past 1, 2 and 3 months per customer per channel which we will use as features for our machine learning model.\n",
    "\n",
    "The feature dataframe should have the following columns:\n",
    "| Column    | Purpose |\n",
    "| -------- | ------- |\n",
    "| `CUSTOMER_ID` | Identify relevant rows for the calculated feature (Join-Criteria)     |\n",
    "| `DATE`    | Allow correct Point-in-Time Joins   |\n",
    "| `Feature columns`  | Actual features per entity    |  \n",
    "\n",
    "You can find more functions for quickly generating featueres here:  \n",
    "[Common feature and query patterns](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cdfdb0-10fb-4891-8c06-81c1c7dd0af3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "def col_formatter(input_col, agg, window):\n",
    "    feature_name = f\"{agg.replace('SUM','TOTAL')}_{input_col}_{window.replace('-', 'past_').replace('MM','_MONTHS')}\"\n",
    "    return feature_name\n",
    "\n",
    "in_shop_transaction_features = (\n",
    "    transactions_df.filter(col('TRANSACTION_CHANNEL') == 'IN_SHOP')\n",
    "    .group_by(['CUSTOMER_ID','DATE']).agg(F.sum('TRANSACTION_AMOUNT').as_('REVENUE'))\n",
    "    .rename({'REVENUE':'REVENUE_IN_SHOP'})\n",
    "    .analytics.time_series_agg(\n",
    "        aggs={'REVENUE_IN_SHOP':['SUM']},\n",
    "        windows=['-1MM','-2MM','-3MM'],\n",
    "        sliding_interval=\"1D\",\n",
    "        group_by=['CUSTOMER_ID'],\n",
    "        time_col='DATE',\n",
    "        col_formatter=col_formatter\n",
    "    ).drop(['SLIDING_POINT','REVENUE_IN_SHOP'])\n",
    ")\n",
    "\n",
    "online_transaction_features = (\n",
    "    transactions_df.filter(col('TRANSACTION_CHANNEL') == 'ONLINE')\n",
    "    .group_by(['CUSTOMER_ID','DATE']).agg(F.sum('TRANSACTION_AMOUNT').as_('REVENUE'))\n",
    "    .rename({'REVENUE':'REVENUE_ONLINE'})\n",
    "    .analytics.time_series_agg(\n",
    "        aggs={'REVENUE_ONLINE':['SUM']},\n",
    "        windows=['-1MM','-2MM','-3MM'],\n",
    "        sliding_interval=\"1D\",\n",
    "        group_by=['CUSTOMER_ID'],\n",
    "        time_col='DATE',\n",
    "        col_formatter=col_formatter\n",
    "    ).drop(['SLIDING_POINT','REVENUE_ONLINE'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f6ecc-65d2-4b18-9386-ee4dbd649a2a",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "online_transaction_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32994363-a331-4cf0-b156-aa3575776d69",
   "metadata": {
    "collapsed": false,
    "name": "cell19"
   },
   "source": [
    "**Feature Descriptions**  \n",
    "To avoid manually writing descriptions, we can use `complete()` to have an LLM generate JSON files containing business descriptions.  \n",
    "The CortexPilot also offers a convenient function `f_describe_columns()` based on the complete() function.  \n",
    "These descriptions are stored in the Feature Store alongside our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25bcab-5a03-4745-bb60-399b3d4a2947",
   "metadata": {
    "language": "python",
    "name": "cell71"
   },
   "outputs": [],
   "source": [
    "feature_descriptions_in_shop_transactions = my_pilot.f_describe_columns(in_shop_transaction_features, exclude_columns=['CUSTOMER_ID','DATE'])\n",
    "feature_descriptions_online_transactions = my_pilot.f_describe_columns(online_transaction_features, exclude_columns=['CUSTOMER_ID','DATE'])\n",
    "\n",
    "st.json(feature_descriptions_in_shop_transactions)\n",
    "st.json(feature_descriptions_online_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bfb016-65ea-4690-813f-54084735c0a0",
   "metadata": {
    "collapsed": false,
    "name": "cell22"
   },
   "source": [
    "### Registering Feature Views\n",
    "The `FeatureView` class accepts a Snowpark DataFrame object that contains the feature transformation logic. This allows you to define your features using any method supported by the Snowpark DataFrame API or Snowflake SQL. You can pass the DataFrame directly to the `FeatureView` constructor.  \n",
    "\n",
    "Each `FeatureView` is associated with the corresponding `Entity`.  \n",
    "The `refresh_freq` parameter determines how often the Feature Store checks for new data and updates the features automatically. For demonstration purposes, this value is set to 1 minute, but it should be adjusted based on the specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d69f38-03c7-435e-8240-7795f752e418",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# Create Feature View\n",
    "in_shop_transaction_fv = FeatureView(\n",
    "    name=\"IN_SHOP_REVENUE_FEATURES\", \n",
    "    entities=[entity],\n",
    "    timestamp_col='DATE',\n",
    "    feature_df=in_shop_transaction_features, \n",
    "    refresh_freq=\"1 minute\",\n",
    "    refresh_mode='AUTO',\n",
    "    desc=\"Features for in-shop transactions\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Add descriptions for features\n",
    "in_shop_transaction_fv = in_shop_transaction_fv.attach_feature_desc(feature_descriptions_in_shop_transactions)\n",
    "\n",
    "in_shop_transaction_fv = fs.register_feature_view(\n",
    "    feature_view=in_shop_transaction_fv, \n",
    "    version=\"V1\", \n",
    "    block=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Create Feature View\n",
    "online_transaction_fv = FeatureView(\n",
    "    name=\"ONLINE_REVENUE_FEATURES\", \n",
    "    entities=[entity],\n",
    "    timestamp_col='DATE',\n",
    "    feature_df=online_transaction_features, \n",
    "    refresh_freq=\"1 minute\",\n",
    "    refresh_mode='AUTO',\n",
    "    desc=\"Features for online transactions\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Add descriptions for features\n",
    "online_transaction_fv = online_transaction_fv.attach_feature_desc(feature_descriptions_online_transactions)\n",
    "\n",
    "online_transaction_fv = fs.register_feature_view(\n",
    "    feature_view=online_transaction_fv, \n",
    "    version=\"V1\", \n",
    "    block=True,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70565143-a46f-4dd6-aaa4-483ea055f37f",
   "metadata": {
    "collapsed": false,
    "name": "cell63"
   },
   "source": [
    "### Discovering Features via Feature Store UI\n",
    "After creating entities and feature views, you can utilize the [Feature Store User Interface](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/feature-store-ui) in Snowsight to locate the objects you need.  \n",
    "\n",
    "Example of the Feature Store UI:  \n",
    "![text](https://github.com/michaelgorkow/snowflake_simple_mlops/blob/main/resources/feature_store.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc112af7-52a7-423f-8161-d5b27a20f53d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "URL_FEATURE_STORE"
   },
   "outputs": [],
   "source": [
    "get_snowsight_url(session, 'Link to Feature Store', '#/features/database/SIMPLE_MLOPS_DEMO/store/FEATURE_STORE/entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51761481-2cbe-45d8-9bb7-6529577b0871",
   "metadata": {
    "collapsed": false,
    "name": "cell64"
   },
   "source": [
    "### Discovering Features via Feature Store API\n",
    "If you don't want to use the UI or want to develop workflows based on data in the feature store you can use the [Feature Store APIs](https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/feature_store).  \n",
    "\n",
    "CortexPilot can also help you in understanding how certain columns in a SQL query are calculated. In this example we are asking how a certain feature in the feature store is calculated but it works for any SQL query.  \n",
    "Simply call `f_explain_column_sql` with the column name and the SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc6acb-fd8b-4c41-bf39-24e5fe74affb",
   "metadata": {
    "language": "python",
    "name": "cell66"
   },
   "outputs": [],
   "source": [
    "st.markdown('### List of all Feature Views:')\n",
    "sdf(fs.list_feature_views())\n",
    "\n",
    "# Retrieve a Feature View\n",
    "retrieved_feature_view = fs.get_feature_view(name='IN_SHOP_REVENUE_FEATURES',version='V1')\n",
    "\n",
    "st.markdown('### Feature View Columns:')\n",
    "sdf(retrieved_feature_view.list_columns())#.show(max_width=200)\n",
    "\n",
    "# Manually refresh a Feature View\n",
    "fs.refresh_feature_view(retrieved_feature_view)\n",
    "\n",
    "st.markdown('### Feature View Refresh History:')\n",
    "sdf(fs.get_refresh_history(retrieved_feature_view).order_by(col('REFRESH_END_TIME').desc()).limit(3))\n",
    "\n",
    "# Explore lineage information\n",
    "st.markdown('### Feature View Lineage:')\n",
    "st.json(retrieved_feature_view.lineage(direction='both'))\n",
    "\n",
    "# Use an LLM and the underlying SQL query to explain how the feature is calculated\n",
    "st.markdown('### LLM Explanation for a Feature in the Feature Store:')\n",
    "sql_explanation = my_pilot.f_explain_column_sql(column='TOTAL_REVENUE_IN_SHOP_PAST_1_MONTHS', sql_query=retrieved_feature_view.query)\n",
    "st.markdown(sql_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade4851-3069-4d9a-af29-8c650c1f1cf5",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "# 4 - Model Training\n",
    "\n",
    "### Generate the Training Dataset with Features from Feature Store\n",
    "Our goal is to predict each customer's revenue for the next month based on their transactions from the past three months.  \n",
    "\n",
    "We have data from January to April 2024. To define our target variable, `NEXT_MONTH_REVENUE`, we sum all transactions from April for each customer. To ensure proper point-in-time feature retrieval and avoid using future data, we only include transaction features up to **April 1st, 2024**, and mark this cutoff with the `FEATURE_CUTOFF_DATE` column.  \n",
    "\n",
    "The DataFrame you just created is a **spine DataFrame**, which acts as a reference table linking customers (`CUSTOMER_ID`) with a timestamp (`FEATURE_CUTOFF_DATE`). It ensures consistent and reproducible feature retrieval in a **feature store**.  \n",
    "\n",
    "Using this spine, you can generate a training dataset with [`generate_dataset()`](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/modeling#generating-snowflake-datasets-for-training). The Feature Store will automatically retrieve features as they were valid on that date and add them to the dataset.  \n",
    "\n",
    "A [Snowflake Dataset](https://docs.snowflake.com/en/developer-guide/snowflake-ml/dataset) is a schema-level object designed for machine learning. It stores data in versions, ensuring immutability, efficient access, and compatibility with ML frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ebd1e-0700-4646-a3ff-d871e51d3fc6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "target_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n",
    "target_df = (\n",
    "    target_df.filter(col('DATE').between('2024-04-02','2024-05-01'))    # Generate Target Variable for April 2024\n",
    "    .group_by('CUSTOMER_ID')\n",
    "    .agg(F.sum('TRANSACTION_AMOUNT').as_('NEXT_MONTH_REVENUE'))\n",
    "    .with_column('FEATURE_CUTOFF_DATE', F.to_date(lit('2024-04-01')))   # Features until End of March 2024\n",
    ")\n",
    "\n",
    "# Get list of all customers\n",
    "customers_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n",
    "\n",
    "# Create spine dataframe\n",
    "spine_df = target_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\n",
    "spine_df = spine_df.fillna(0, subset='NEXT_MONTH_REVENUE')\n",
    "spine_df.order_by('CUSTOMER_ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43830e-d929-4fdf-a94a-f6761e09fa6b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "train_dataset = fs.generate_dataset(\n",
    "    name=\"SIMPLE_MLOPS_DEMO.FEATURE_STORE.NEXT_MONTH_REVENUE_DATASET\",\n",
    "    spine_df=spine_df,\n",
    "    features=[in_shop_transaction_fv, online_transaction_fv],\n",
    "    version=\"V1\",\n",
    "    spine_timestamp_col=\"FEATURE_CUTOFF_DATE\",\n",
    "    spine_label_cols=[\"NEXT_MONTH_REVENUE\"],\n",
    "    include_feature_view_timestamp_col=False,\n",
    "    desc=\"Initial Training Dataset\"\n",
    ")\n",
    "\n",
    "df = train_dataset.read.to_snowpark_dataframe()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d081cd-ff03-4efa-9134-ae34a29e0190",
   "metadata": {
    "collapsed": false,
    "name": "cell32"
   },
   "source": [
    "### Train an XGBoost Model\n",
    "We randomly split the data, allocating **90% for training** and **10% for validation**.  \n",
    "The training data is then used to train an **XGBoost regression model** with the `XGBRegressor` from the **Snowflake ML library**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbea5f6-0020-4465-add6-8a87653644ac",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df, test_df = df.random_split(weights=[0.9, 0.1], seed=0)\n",
    "\n",
    "print(f'Number of samples in train: {train_df.count()}')\n",
    "print(f'Number of samples in test: {test_df.count()}')\n",
    "\n",
    "feature_columns = train_df.drop(['CUSTOMER_ID','FEATURE_CUTOFF_DATE','NEXT_MONTH_REVENUE']).columns\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    input_cols=feature_columns,\n",
    "    label_cols=['NEXT_MONTH_REVENUE'],\n",
    "    output_cols=['NEXT_MONTH_REVENUE_PREDICTION'],\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_model = xgb_model.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca74d9a-0504-48ae-a6ab-b399a56f2d56",
   "metadata": {
    "collapsed": false,
    "name": "cell24"
   },
   "source": [
    "### Evaluate the XGBoost Model\n",
    "You can immediately use the model’s `predict()` function to generate predictions on the test data.  \n",
    "Snowflake ML also provides built-in metric functions, such as **Mean Absolute Percentage Error (MAPE)**, for evaluating model performance.  \n",
    "\n",
    "Additionally, you can convert the model back to its native open-source format using `xgb_model.to_xgboost()`.  \n",
    "This allows you to access feature importance values, which we visualize to better understand what influences the model’s predictions.  \n",
    "\n",
    "As shown in the plot, the model correctly identified that **IN_SHOP transactions** are the primary driver of the target variable, `NEXT_MONTH_REVENUE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737ef58-d313-4138-91e7-bb5becf601ea",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict(test_df)\n",
    "# Analyze results\n",
    "mape = mean_absolute_percentage_error(\n",
    "    df=predictions, \n",
    "    y_true_col_names=\"NEXT_MONTH_REVENUE\", \n",
    "    y_pred_col_names=\"NEXT_MONTH_REVENUE_PREDICTION\"\n",
    ")\n",
    "\n",
    "st.info(f\"**Mean absolute percentage error:** {mape:.5f}\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    # Plot Feature Importance\n",
    "    plot_data = pd.DataFrame(\n",
    "        list(zip(feature_columns, xgb_model.to_xgboost().feature_importances_)), \n",
    "        columns=['FEATURE','IMPORTANCE']\n",
    "    )\n",
    "    \n",
    "    fig = px.bar(\n",
    "        plot_data.sort_values('IMPORTANCE', ascending=True).head(10),\n",
    "        x=\"IMPORTANCE\",\n",
    "        y=\"FEATURE\",\n",
    "        title=\"Feature Importance\",\n",
    "        labels={\"FEATURE\": \"Feature\", \"IMPORTANCE\": \"Importance\"},\n",
    "        orientation=\"h\"\n",
    "    )\n",
    "    fig.update_layout(title_font=dict(size=20, family=\"Arial\", color=\"black\"))\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "with col2:\n",
    "    # Plot Predictions\n",
    "    fig = px.scatter(\n",
    "        predictions[\"NEXT_MONTH_REVENUE\", \"NEXT_MONTH_REVENUE_PREDICTION\"].to_pandas().astype(\"float64\"),\n",
    "        x=\"NEXT_MONTH_REVENUE\",\n",
    "        y=\"NEXT_MONTH_REVENUE_PREDICTION\",\n",
    "        title=\"Actual vs. Predicted Revenue\",\n",
    "        labels={\n",
    "            \"NEXT_MONTH_REVENUE\": \"Actual Revenue\",\n",
    "            \"NEXT_MONTH_REVENUE_PREDICTION\": \"Predicted Revenue\"\n",
    "        },\n",
    "        trendline=\"ols\",\n",
    "        trendline_color_override=\"red\"\n",
    "    )\n",
    "    fig.update_layout(title_font=dict(size=20, family=\"Arial\", color=\"black\"))\n",
    "    st.plotly_chart(fig, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad317-43d5-44d8-a66e-18abf67e4a28",
   "metadata": {
    "collapsed": false,
    "name": "cell33"
   },
   "source": [
    "# 5 - Snowflake Model Registry\n",
    "### Setup Model Registry\n",
    "After training a model, the first step in operationalizing it and running inference in Snowflake is to **log the model in the Snowflake Model Registry**.  \n",
    "\n",
    "The **Model Registry** allows you to securely manage models and their metadata in Snowflake, regardless of their origin or type, while also simplifying inference.  \n",
    "It stores machine learning models as **first-class schema-level objects** within Snowflake.  \n",
    "\n",
    "By setting `enable_monitoring` to True, the **Model Registry** can also be used for model monitoring, which we will implement in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570b13e-2b1d-480f-8a0b-e73488f7e3c6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "# Create reference to model registry\n",
    "reg = Registry(\n",
    "    session=session, \n",
    "    database_name='SIMPLE_MLOPS_DEMO', \n",
    "    schema_name='MODEL_REGISTRY', \n",
    "    options={'enable_monitoring':True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d2ca7-7f49-4692-ad0a-aedcc614d322",
   "metadata": {
    "collapsed": false,
    "name": "cell34"
   },
   "source": [
    "### Register Model in Model Registry\n",
    "The Model Registry's `log_model()` function takes the model object and logs it to the registry.  \n",
    "The **name** and **version** help ensure the correct model is retrieved for inference.  \n",
    "\n",
    "Additionally, we log relevant metrics/information, including:  \n",
    "- **MAPE (Mean Absolute Percentage Error)** calculated on the test dataset  \n",
    "- **FEATURE_CUTOFF_DATE**   \n",
    "\n",
    "We also specify the following parameters:  \n",
    "\n",
    "| Variable               | Description  |\n",
    "|------------------------|-------------|\n",
    "| `sample_input_data`    | Sample input data used to infer model signatures, serve as background data for explanations, and capture data lineage. |\n",
    "| `conda_dependencies`   | Specifies model dependencies, such as the XGBoost library. |\n",
    "| `relax_version`        | Enforces specific dependency versions for compatibility and reproducibility. |\n",
    "| `enable_explainability` | Adds an explainability function to the model, allowing us to better understand its predictions using SHAP values. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18434961-96fe-482a-90c5-40ea4ba8402b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "registered_model = reg.log_model(\n",
    "    xgb_model,\n",
    "    model_name=\"CUSTOMER_REVENUE_MODEL\",\n",
    "    version_name='V1',\n",
    "    metrics={\n",
    "        'MAPE':mape, \n",
    "        \"TRAINING_DATA\":{'FEATURE_CUTOFF_DATE':'2024-04-01'}\n",
    "    },\n",
    "    comment=\"Model trained using XGBoost to predict revenue per customer for next month.\",\n",
    "    conda_dependencies=['xgboost'],\n",
    "    sample_input_data=train_df.select(feature_columns).limit(100),\n",
    "    options={\"relax_version\": False, \"enable_explainability\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6492927-bcca-4f8d-93c2-bb7fe490d466",
   "metadata": {
    "collapsed": false,
    "name": "cell41"
   },
   "source": [
    "### Operationalize Models\n",
    "There are multiple ways to operationalize models using Snowflake's Model Registry.  \n",
    "One simple approach is to use **aliases** for the model. By assigning the alias **`PRODUCTION`**, any inference pipeline referencing this alias will automatically use the correct production-ready model.  \n",
    "\n",
    "When a new model version is trained and ready for deployment, you can seamlessly update production by **removing the alias from the current model** and **assigning it to the new model**.  \n",
    "This method ensures that existing ML pipelines remain unchanged, reducing the need for manual updates while maintaining a smooth model deployment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141827b-939f-4e43-983d-7854519e5ab7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": [
    "registered_model.set_alias('PRODUCTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c8eac-0ad9-4ed6-9f7d-c31c45b91cc8",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": [
    "# Retrieve the production model in your pipelines like this\n",
    "production_model = reg.get_model('CUSTOMER_REVENUE_MODEL').version('PRODUCTION')\n",
    "production_model.show_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebd58e-640b-4442-8155-c64f8bad43d5",
   "metadata": {
    "collapsed": false,
    "name": "cell62"
   },
   "source": [
    "### Explore Models in the Model Registry UI\n",
    "The [Model Registry UI]((https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/snowsight-ui)) in Snowsight enables you to discover and explore machine learning models available for use in Snowflake.  \n",
    "\n",
    "To view a model's details, click on its corresponding row in the Models list.  \n",
    "The details page provides essential information, including the model's description, tags, and versions.\n",
    "\n",
    "Example of the Model Registry UI:  \n",
    "![text](https://github.com/michaelgorkow/snowflake_simple_mlops/blob/main/resources/model_registry_ui.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08a60d-bdc8-45e8-9558-8b999d94e2b0",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "URL_MODEL_REGISTRY"
   },
   "outputs": [],
   "source": [
    "get_snowsight_url(session, 'Link to Model Registry', '#/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6642c-5fe5-4361-b970-2955611795cb",
   "metadata": {
    "collapsed": false,
    "name": "cell48"
   },
   "source": [
    "### **Model Explainability**\n",
    "Since we enabled `model_explainability` during model registration, we can now use the auto-generated `explain` function to compute SHAP values for each feature.\n",
    "\n",
    "#### **What are SHAP (SHapley Additive exPlanations) values?**  \n",
    "SHAP values provide a game-theoretic approach to interpreting machine learning predictions by fairly attributing contributions to each feature. They help explain both **global feature importance** and **individual predictions**, showing how each feature increases or decreases the model’s output. **Positive SHAP values** indicate features that push the prediction higher, while **negative values** lower it relative to the model’s baseline prediction.\n",
    "\n",
    "Once computed, we can convert the SHAP values into a native [`shap.Explanation`](https://shap.readthedocs.io/en/latest/generated/shap.Explanation.html) object, which includes:\n",
    "\n",
    "| Variable        | Description |\n",
    "|----------------|-------------|\n",
    "| `values`       | Contribution of each feature to the prediction (output from Snowflake’s `explain` function). |\n",
    "| `base_values`  | Expected model output (typically the mean prediction). |\n",
    "| `data`         | Feature values (used for visualization). |\n",
    "| `feature_names` | Names of the features (optional but recommended). |\n",
    "\n",
    "You can then leverage SHAP's built-in functions for visualization and deeper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f54e45-44e8-481d-ba8c-486f7a54080b",
   "metadata": {
    "language": "python",
    "name": "cell88"
   },
   "outputs": [],
   "source": [
    "# Calculate Shap values predictions\n",
    "explanations = production_model.run(predictions, function_name=\"explain\")\n",
    "explanations = explanations.rename({col:col.replace('\"\"\"', '').upper() for col in explanations.columns})\n",
    "shap_columns = [col for col in explanations.columns if '_EXPLANATION' in col]\n",
    "explanations = explanations.to_pandas()\n",
    "\n",
    "# Create the native shap Explanation object\n",
    "shap_exp = shap.Explanation(\n",
    "    values = explanations[shap_columns].values,\n",
    "    base_values = np.full((len(explanations),), explanations['NEXT_MONTH_REVENUE_PREDICTION'].mean()),\n",
    "    data = explanations[feature_columns].values,\n",
    "    feature_names=feature_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6badc161-aaf8-42f0-ac1f-c8a3a5fcd653",
   "metadata": {
    "collapsed": false,
    "name": "cell90"
   },
   "source": [
    "### Global Explainibility using SHAP\n",
    "\n",
    "The **left plot** is a SHAP **summary plot**, which displays the impact of each feature on the model’s output. Each dot represents a single instance, with the color indicating the feature value (blue = low, pink = high). Features at the top are the most influential, and the x-axis shows whether they push predictions higher (positive SHAP values) or lower (negative SHAP values).  \n",
    "\n",
    "The **right plot** is a SHAP **violin plot**, which shows the distribution of SHAP values for each feature. The width of the violin indicates the density of SHAP values, helping visualize how much a feature’s impact varies across different predictions. Both plots highlight that recent revenue metrics (e.g., \"TOTAL_REVENUE_IN_SHOP_PAST_1_MONTH\") strongly influence the model, with high revenue values generally increasing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ab482-1ce4-4359-b7ef-2fb035406855",
   "metadata": {
    "language": "python",
    "name": "cell92"
   },
   "outputs": [],
   "source": [
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    shap.summary_plot(shap_exp)\n",
    "with col2:\n",
    "    shap.plots.violin(shap_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24ab41-8665-495e-8917-d3e9546aea64",
   "metadata": {
    "collapsed": false,
    "name": "cell93"
   },
   "source": [
    "### Local Explainibility using SHAP\n",
    "\n",
    "On the left you see a **SHAP waterfall plot** which explains how a model arrived at a specific prediction by showing feature contributions. It starts with the **expected value** (baseline prediction) and adjusts it based on **SHAP values** of individual features. **Blue bars** represent features that **lowered** the prediction, while **red bars** indicate those that **increased** it. The final predicted value is obtained by sequentially adding these contributions to the baseline. This plot helps identify which features had the most impact and whether they pushed the prediction up or down.\n",
    "\n",
    "On the right we are plotting the distribution of **online vs. in-shop transaction** for this specific customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f9eff-c039-4401-bc33-7f7c053ed2b4",
   "metadata": {
    "language": "python",
    "name": "cell87"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "selected_customer = st.selectbox('Select Customer:', explanations.sort_values(by='CUSTOMER_ID')['CUSTOMER_ID'].values)\n",
    "index = explanations.index[explanations['CUSTOMER_ID'] == selected_customer][0]\n",
    "col1, col2 = st.columns([0.6,0.4])\n",
    "with col1:\n",
    "    st.subheader('Shap Waterfall Plot')\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"\", fontsize=16)\n",
    "    plt.sca(ax)\n",
    "    shap.plots.waterfall(shap_exp[index], show=False)\n",
    "    plt.close()\n",
    "    st.pyplot(fig)\n",
    "with col2:\n",
    "    st.subheader('Customer Transaction ')\n",
    "    demo_flow.get_customer_revenue_plot(transactions_df, int(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a85689-b808-48ca-8c7c-cb3c307b59c0",
   "metadata": {
    "collapsed": false,
    "name": "cell35"
   },
   "source": [
    "### Continious Model Monitoring\n",
    "Model behavior can change over time due to factors such as **input drift, stale training assumptions, data pipeline issues, hardware and software updates**.\n",
    "\n",
    "**ML Observability** enables you to monitor the quality of models registered in the **Snowflake Model Registry** across multiple dimensions, including **performance, drift, and volume**.  \n",
    "\n",
    "To measure drift for model monitoring, we use two tables:  \n",
    "\n",
    "| Table      | Description  |\n",
    "|------------|-------------|\n",
    "| `BASELINE` | Contains a snapshot of data similar to `SOURCE`. It is used as a reference for comparing future feature values and predictions. |\n",
    "| `SOURCE`   | Stores future predictions and feature values for monitoring. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10552343-1009-4e62-989d-6afbbef0e1cb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell44"
   },
   "outputs": [],
   "source": [
    "# Save baseline predictions\n",
    "predictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\n",
    "predictions.write.save_as_table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1', mode='overwrite')\n",
    "predictions.write.save_as_table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_SOURCE_V1', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3f43e-0b16-4e68-a0f8-79d9be5edd1c",
   "metadata": {
    "collapsed": false,
    "name": "cell54"
   },
   "source": [
    "### Creating predictions for the next month\n",
    "We now use the trained model on our **April data** to predict each customer's **revenue for May**.  \n",
    "The workflow looks like this:  \n",
    "1. Build a spine DataFrame\n",
    "2. Retrieve Features from Feature Store \n",
    "3. Generate predictions\n",
    "\n",
    "Now, let’s take this a step further with a **real-world challenge**:  \n",
    "Imagine you didn’t train the model yourself. How would you determine which input features are required and where to source them?\n",
    "\n",
    "The answer is simple:  \n",
    "Query the **automatically captured lineage** information in Snowflake!  \n",
    "The model is directly linked to the dataset used for training, which in turn is connected to the relevant Feature Views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e3b1f-6058-499d-90b7-ff1af87abb26",
   "metadata": {
    "language": "python",
    "name": "cell51"
   },
   "outputs": [],
   "source": [
    "feature_views = production_model.lineage(direction='upstream')[0].lineage(domain_filter=['feature_view'], direction='upstream')\n",
    "[fv.name for fv in feature_views]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0419c0-d00d-47ae-baf5-6737ef8383be",
   "metadata": {
    "language": "python",
    "name": "cell47"
   },
   "outputs": [],
   "source": [
    "def get_feature_df(model, feature_cutoff_date):\n",
    "    # Use lineage information to retrieve the feature views of this model\n",
    "    feature_views = model.lineage(direction='upstream')[0].lineage(domain_filter=['feature_view'], direction='upstream')\n",
    "    \n",
    "    # Create the spine dataframe containing all customers\n",
    "    spine_df = (\n",
    "        session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.CUSTOMERS')\n",
    "        .select('CUSTOMER_ID')\n",
    "        .with_column('FEATURE_CUTOFF_DATE', F.to_date(lit(feature_cutoff_date)))\n",
    "    )\n",
    "\n",
    "    # Retrieve feature values from the Feature Store for the specified cutoff date.\n",
    "    feature_df = fs.retrieve_feature_values(\n",
    "        spine_df=spine_df,\n",
    "        features=feature_views,\n",
    "        spine_timestamp_col=\"FEATURE_CUTOFF_DATE\"\n",
    "    )\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a038b00-0e99-47fc-8dcf-e5828e05410a",
   "metadata": {
    "language": "python",
    "name": "cell50"
   },
   "outputs": [],
   "source": [
    "feature_df = get_feature_df(\n",
    "    production_model, \n",
    "    feature_cutoff_date='2024-05-01', \n",
    ")\n",
    "\n",
    "predictions = production_model.run(feature_df, function_name='PREDICT')\n",
    "predictions.write.save_as_table(table_name='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_SOURCE_V1', mode='append', column_order='name')\n",
    "\n",
    "# View predictions\n",
    "print('Predictions [column=NEXT_MONTH_REVENUE_PREDICTION]:')\n",
    "session.table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_SOURCE_V1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca5a9b-1bbb-44a4-a8c0-05d360c8f5e8",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "### Creating a Model Monitor  \n",
    "\n",
    "We are setting up a **model monitor** to continuously calculate and track model performance and drift over time.  \n",
    "\n",
    "These calculations are based on the **`BASELINE`** and **`SOURCE`** tables created earlier.  \n",
    "Each model requires its own dedicated **model monitor** to ensure accurate tracking and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2614e2f-4856-4e5b-a5a5-d78b4f3eeaea",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "source_config = ModelMonitorSourceConfig(\n",
    "    source='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_SOURCE_V1',\n",
    "    timestamp_column='FEATURE_CUTOFF_DATE',\n",
    "    id_columns=['CUSTOMER_ID'],\n",
    "    prediction_score_columns=['NEXT_MONTH_REVENUE_PREDICTION'],\n",
    "    actual_score_columns=['NEXT_MONTH_REVENUE'],\n",
    "    baseline='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1'\n",
    ")\n",
    "\n",
    "monitor_config = ModelMonitorConfig(\n",
    "    model_version=reg.get_model('CUSTOMER_REVENUE_MODEL').version('PRODUCTION'),\n",
    "    model_function_name='predict',\n",
    "    background_compute_warehouse_name='COMPUTE_WH',\n",
    "    refresh_interval='1 minute',\n",
    "    aggregation_window='1 day'\n",
    ")\n",
    "\n",
    "model_monitor = reg.add_monitor(\n",
    "    name='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_V1',\n",
    "    source_config=source_config,\n",
    "    model_monitor_config=monitor_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64990a-9de7-42f5-af03-525cf41a057a",
   "metadata": {
    "collapsed": false,
    "name": "cell53"
   },
   "source": [
    "### Simulating the next month of Customer Transactions\n",
    "Our model has predicted each customer's **revenue for May 2024** and stored the results in the **`SOURCE`** table.  \n",
    "Next, we simulate the actual transactions for May and update the **true revenue values** for each customer in the **`SOURCE`** table.  \n",
    "When the **model monitor** refreshes, it will use these updated values to calculate various **model performance metrics**, including the MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa1731-d7d4-4aff-bf0a-1e4f2b36a286",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell49"
   },
   "outputs": [],
   "source": [
    "# Add new transactions (created as part of the initial demo setup)\n",
    "new_transactions = session.table('SIMPLE_MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').filter(col('DATE').between('2024-05-02','2024-06-01'))\n",
    "new_transactions.write.save_as_table(table_name='SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS', mode='append')\n",
    "\n",
    "# Calculate actual values\n",
    "actual_values_df = (\n",
    "    session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n",
    "    .filter(col('DATE').between('2024-05-02','2024-06-01'))\n",
    "    .group_by(['CUSTOMER_ID'])\n",
    "    .agg(F.sum('TRANSACTION_AMOUNT').as_('TOTAL_REVENUE'))\n",
    "    .with_column('DATE', F.to_date(lit('2024-05-01')))\n",
    ")\n",
    "\n",
    "# Get list of all customers\n",
    "customers_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n",
    "\n",
    "# Assume 0 revenue for customers without transactions\n",
    "actual_values_df = actual_values_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\n",
    "actual_values_df = actual_values_df.fillna(0,subset='TOTAL_REVENUE')\n",
    "\n",
    "# Update source table from model monitor\n",
    "source_table = session.table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_SOURCE_V1')\n",
    "source_table.update(\n",
    "    condition=(\n",
    "        (source_table['FEATURE_CUTOFF_DATE'] == actual_values_df['DATE']) &\n",
    "        (source_table['CUSTOMER_ID'] == actual_values_df['CUSTOMER_ID'])\n",
    "    ),\n",
    "    assignments={\n",
    "        \"NEXT_MONTH_REVENUE\": actual_values_df['TOTAL_REVENUE'],\n",
    "    },\n",
    "    source=actual_values_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e684bbe-dac6-4f97-8bc4-ca1e6afdd5ee",
   "metadata": {
    "collapsed": false,
    "name": "cell39"
   },
   "source": [
    "## Simulate Customer Transactions until February 2025\n",
    "For convenience, I encapsulated all the logic for simulating future months into the helper function `simulate_model_performance()`.  \n",
    "We use this function to simulate the model's behavior until February 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafe355-8bb7-4d24-9fa7-80bd55514f0d",
   "metadata": {
    "language": "python",
    "name": "cell43"
   },
   "outputs": [],
   "source": [
    "start_date = '2024-06-01'\n",
    "end_date = '2025-01-01'\n",
    "demo_flow.simulate_model_performance(production_model, start_date, end_date, generate_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c08704-b178-40b7-888b-7b4b0c75eac7",
   "metadata": {
    "collapsed": false,
    "name": "cell46"
   },
   "source": [
    "## Explore the Model Monitor\n",
    "Navigate to the Model Monitor and observe the `MAPE` and `Jensen-Shannon Distance`  for the last months.  \n",
    "\n",
    "You will notice the following:\n",
    "* Declining Model Performance\n",
    "    * :chart_with_upwards_trend: MAPE (Mean Average Percentage Error)\n",
    "* Feature Drift\n",
    "    * :chart_with_upwards_trend: Distance for TOTAL_REVENUE_IN_SHOP_PAST_1_MONTHS (less in shop transaction volume)\n",
    "    * :chart_with_downwards_trend: Difference for TOTAL_REVENUE_ONLINE_PAST_1_MONTHS (more online transaction volume)\n",
    "\n",
    "Why is that?  \n",
    "Well, if we visualize the monthly revenue distribution, we can see that online revenue grew while in-shop transaction declined.\n",
    "\n",
    "Instead of using the builtin UI, you can also query model monitor metrics using the following table functions and build your own visuals:\n",
    "* [MODEL_MONITOR_PERFORMANCE_METRIC](https://docs.snowflake.com/en/sql-reference/functions/model-monitor-performance-metric)\n",
    "* [MODEL_MONITOR_DRIFT_METRIC](https://docs.snowflake.com/en/sql-reference/functions/model-monitor-drift-metric)\n",
    "* [MODEL_MONITOR_STAT_METRIC](https://docs.snowflake.com/en/sql-reference/functions/model-monitor-stat-metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd70e5-503c-4952-89b8-f80a1a33434e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "URL_MODEL_MONITOR"
   },
   "outputs": [],
   "source": [
    "get_snowsight_url(session, 'Link to Model Monitor', '#/data/databases/SIMPLE_MLOPS_DEMO/schemas/MODEL_REGISTRY/model/CUSTOMER_REVENUE_MODEL/version/V1/monitors/MM_V1/dashboard')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace518d-69fb-4eeb-ba99-ef17a5707655",
   "metadata": {
    "collapsed": false,
    "name": "cell36"
   },
   "source": [
    "### Query the model monitor\n",
    "With the built-in functions, querying metrics from the model monitor becomes effortless, allowing for further analysis and visualization.\n",
    "Additionally, this notebook includes a small helper for the model registry, enabling you to quickly navigate through your registered models, visualize their metrics, and compare multiple models in a single graph.\n",
    "\n",
    "Moreover, the outputs from these model registry functions can be leveraged to create alerts and trigger automated tasks. Example use cases include:\n",
    "* Sending an email notification to your ML engineer if a model's performance drops below a predefined threshold.\n",
    "* Initiating an automated retraining of a model with fresh data using [Snowflake Tasks](https://docs.snowflake.com/en/user-guide/tasks-intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae94833-4b55-463d-877c-8425010dae1f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "session.table_function(\n",
    "    \"MODEL_MONITOR_DRIFT_METRIC\",\n",
    "    lit('MM_V1'),\n",
    "    lit('WASSERSTEIN'),\n",
    "    lit('TOTAL_REVENUE_IN_SHOP_PAST_1_MONTHS'),\n",
    "    lit('1 day'),\n",
    "    lit('2024-01-01'),\n",
    "    lit('2024-12-31')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1392667-4209-483c-bbf4-ea8cc3a2cb93",
   "metadata": {
    "language": "python",
    "name": "cell61"
   },
   "outputs": [],
   "source": [
    "model_registry_helper = ModelRegistryHelper(session, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b727af-068b-43a3-94e3-e4f277b61127",
   "metadata": {
    "language": "python",
    "name": "cell70"
   },
   "outputs": [],
   "source": [
    "model_registry_helper.plot_model_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7327c-9ebf-455b-b1c7-74ca9c416592",
   "metadata": {
    "collapsed": false,
    "name": "cell72"
   },
   "source": [
    "### Why is the new model performing better?\n",
    "Let's ask CortexPilot what changed.\n",
    "\n",
    "The bar chart displays the overall revenue per month, categorized by transaction channel (Online and In-Shop). It shows a notable increase in online transactions starting in June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c579eb-0cf3-4fce-bcf3-110bc19ccc9b",
   "metadata": {
    "language": "python",
    "name": "cell59"
   },
   "outputs": [],
   "source": [
    "my_pilot.f_cortex_helper_visualize_query(\n",
    "    transactions_df, \n",
    "    'What was the overall revenue per channel and month? Use a stacked bar plot and use YY-Monthname for the x-axis. Make sure the x-axis is ordered by month.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2686a6c-4210-45f7-ab2d-32fb61a9d11f",
   "metadata": {
    "collapsed": false,
    "name": "NEW_MODEL_1"
   },
   "source": [
    "## Train a new Model Version  \n",
    "\n",
    "Since **user behavior has changed**, we will train a **new version of our model** using fresh data.  \n",
    "\n",
    "To streamline this process, I have encapsulated the entire training workflow into the helper function `train_new_model()`, which automates the following steps:  \n",
    "\n",
    "- **Creates the spine DataFrame**, including the target variable.  \n",
    "- **Retrieves features** from the Feature Store.  \n",
    "- **Creates a Snowflake Dataset** from the training data (ensuring reproducibility with a snapshot).  \n",
    "- **Trains a new XGBoost model**.  \n",
    "- **Registers the model** in the Snowflake Model Registry.  \n",
    "- **Sets up a new model monitor** to track performance and drift.  \n",
    "- **Compares model performance** against the existing production model.  \n",
    "- **Deploys the new model** if it outperforms the current one by assigning it the **\"PRODUCTION\"** alias.  \n",
    "\n",
    "Since the training data includes **June, July, and August 2024** (covering training data up to **Septemer 1st, 2024**, and looking back three months), the model should recognize that **ONLINE transactions** have become a major driver of customer revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b851b4-ec0d-4efc-aa13-cdfd81ef8fd0",
   "metadata": {
    "language": "python",
    "name": "cell60"
   },
   "outputs": [],
   "source": [
    "@sproc(\n",
    "    name='SIMPLE_MLOPS_DEMO.PUBLIC.TRAIN_CUSTOMER_REVENUE_MODEL',\n",
    "    is_permanent=True,\n",
    "    replace=True,\n",
    "    stage_location='SIMPLE_MLOPS_DEMO.PUBLIC.PIPELINES',\n",
    "    packages=['snowflake-snowpark-python','snowflake-ml-python==1.7.4','snowflake-telemetry-python'],\n",
    "    imports=['@SIMPLE_MLOPS_DEMO.PUBLIC.GITHUB_REPOSITORY_SNOWFLAKE_SIMPLE_MLOPS/branches/main/src/demo_extras/model_trainer.py'],\n",
    "    execute_as='caller'\n",
    ")\n",
    "def train_new_model(session: Session, feature_views: dict, feature_cutoff_date: str, target_start_date: str, target_end_date: str, model_version: str) -> float:\n",
    "    from model_trainer import ModelTrainer\n",
    "    logger = logging.getLogger(\"logger.ModelTrainer\")\n",
    "    tracer = trace.get_tracer(\"tracer.ModelTrainer\")\n",
    "    \n",
    "    logger.info('Starting model training.')\n",
    "    with tracer.start_as_current_span(\"XGBoost Pipeline\"):\n",
    "        model_trainer = ModelTrainer(session)\n",
    "        mape = model_trainer.train_new_model(feature_views, feature_cutoff_date, target_start_date, target_end_date, model_version)\n",
    "    logger.info('Model training finished.')\n",
    "    \n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab36dd-614d-418e-a1cb-6f43fa335ee6",
   "metadata": {
    "language": "python",
    "name": "cell68"
   },
   "outputs": [],
   "source": [
    "feature_views = {'IN_SHOP_REVENUE_FEATURES':'V1', 'ONLINE_REVENUE_FEATURES':'V1'}\n",
    "feature_cutoff_date = '2024-09-01'\n",
    "target_start_date = '2024-09-02'\n",
    "target_end_date = '2024-10-01'\n",
    "model_version = 'V2'\n",
    "\n",
    "mape_value = train_new_model(session, feature_views, feature_cutoff_date, target_start_date, target_end_date, model_version)\n",
    "st.info(mape_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77417d-c7d0-4186-ac00-c29712b61cad",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from snowflake.core.task import StoredProcedureCall\n",
    "from snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\n",
    "from snowflake.core import Root\n",
    "\n",
    "root = Root(session)\n",
    "\n",
    "with DAG('ML_TRAINING_PIPELINE', schedule=timedelta(minutes=3)) as dag:\n",
    "    task_train = DAGTask(\n",
    "        'ML_TRAINING',\n",
    "        StoredProcedureCall(train_new_model, args=[feature_views, feature_cutoff_date, target_start_date, target_end_date, '']),\n",
    "        warehouse='COMPUTE_WH'\n",
    "    )\n",
    "\n",
    "schema = root.databases[\"SIMPLE_MLOPS_DEMO\"].schemas[\"PUBLIC\"]\n",
    "dag_op = DAGOperation(schema)\n",
    "dag_op.deploy(dag, mode=CreateMode.or_replace)\n",
    "dag_op.run(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc77a6f-4035-4d60-8826-fd5f06d9bbf7",
   "metadata": {
    "collapsed": false,
    "name": "cell52"
   },
   "source": [
    "# Snowflake Logging & Tracing\n",
    "Snowflake is capturing logs, metrics and traces for your Stored Procedures, UDFs, etc. automatically in [Event Tables](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up).  \n",
    "\n",
    "**Note:**  \n",
    "You need to [enable telemetry collection](https://docs.snowflake.com/en/developer-guide/logging-tracing/logging-tracing-enabling). This was done during the demo setup for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f883a33-6d2e-469e-8577-8130a94085d9",
   "metadata": {
    "language": "python",
    "name": "cell55"
   },
   "outputs": [],
   "source": [
    "get_snowsight_url(session, 'Task History', '#/compute/history/tasks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75783fe-e8ed-435f-ab39-8d592166576d",
   "metadata": {
    "collapsed": false,
    "name": "NEW_MODEL_3"
   },
   "source": [
    "### Simulate Model performance for Model Version V2 until 2025-01-31\n",
    "Once again, we are simulating **model performance** based on customer transactions up to **February 2025**.  \n",
    "Make sure to check the **model monitor** to evaluate whether the new model version trained on more recent data performs better.  \n",
    "Additionally, analyze the **feature drift**, where you’ll notice that the trend for the **V2 model** is much more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe2c80-0c46-4f7f-8f1d-292226bb7469",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": [
    "model = reg.get_model('CUSTOMER_REVENUE_MODEL').version('V2')\n",
    "start_date = '2024-10-01'\n",
    "end_date = '2025-01-01'\n",
    "demo_flow.simulate_model_performance(model, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a3938-eccc-4fcf-8fb4-0e26d9848dd1",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "model_registry_helper.plot_model_performance(update_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a76de0-74ac-4f25-84f3-4717b33c51fb",
   "metadata": {
    "collapsed": false,
    "name": "cell56"
   },
   "source": [
    "### Comparing the two Model Versions\n",
    "We have already observed that the new model provides **significantly better predictions** for future customer revenue. However, we want to gain deeper insights into **why** this improvement occurred.  \n",
    "\n",
    "To analyze this, we are generating the SHAP Summary Plot for both models for the latest data. This reveals that the new model recognizes a **much stronger influence** of past **ONLINE transactions** on future customer revenue.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359172b6-0c73-4998-9fd0-bd04f87c9f9e",
   "metadata": {
    "language": "python",
    "name": "cell76"
   },
   "outputs": [],
   "source": [
    "model_registry_helper.update_registry_data()\n",
    "\n",
    "shap_exp1 = model_registry_helper.get_model_explanations(\n",
    "    reg.get_model('CUSTOMER_REVENUE_MODEL').version('V1'), feature_columns=feature_columns, feature_cutoff_date='2025-01-01'\n",
    ")\n",
    "\n",
    "shap_exp2 = model_registry_helper.get_model_explanations(\n",
    "    reg.get_model('CUSTOMER_REVENUE_MODEL').version('V2'), feature_columns=feature_columns, feature_cutoff_date='2025-01-01'\n",
    ")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    shap.summary_plot(shap_exp1)\n",
    "with col2:\n",
    "    shap.summary_plot(shap_exp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177a3c1-8d98-4900-83ea-f0ec4a910b20",
   "metadata": {
    "collapsed": false,
    "name": "cell25"
   },
   "source": [
    "## ML Lineage\n",
    "Even though you may not have noticed, you’ve been capturing **lineage information** throughout the development of your machine learning pipeline.  \n",
    "\n",
    "You can retrieve this information using the built-in function `lineage.trace()` for further analysis.  \n",
    "For example, you can use this data to **visualize the lineage directly in the notebook**.  \n",
    "\n",
    "Additionally, Snowflake provides a **more user-friendly and interactive UI** that allows you to explore and monitor your machine learning pipeline:  \n",
    "![text](https://github.com/michaelgorkow/snowflake_simple_mlops/blob/main/resources/ml_lineage3.png?raw=true)\n",
    "\n",
    "As shown, the lineage captures a **comprehensive view** of your pipeline, tracking data transformations and dependencies from the **source tables**, through the **feature view**, the **training dataset**, and ultimately the **registered model** in the Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775030b5-6034-4827-a44f-60fc2ec01392",
   "metadata": {
    "language": "python",
    "name": "cell57"
   },
   "outputs": [],
   "source": [
    "get_snowsight_url(session, 'Link to Lineage View', '#/data/databases/SIMPLE_MLOPS_DEMO/schemas/MODEL_REGISTRY/model/CUSTOMER_REVENUE_MODEL/version/V2/lineage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fab3ee-19e7-4a34-a86a-cd09efec9065",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell45"
   },
   "outputs": [],
   "source": [
    "trace = session.lineage.trace(\n",
    "    object_name='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.CUSTOMER_REVENUE_MODEL',\n",
    "    object_version='V2',\n",
    "    object_domain='model',\n",
    "    direction='both',\n",
    "    distance=2\n",
    ")\n",
    "trace.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924a8b0-75ca-46b3-bd57-c3edd59a16a4",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "lineage_helper = LineageHelper()\n",
    "lineage_helper.visualize_lineage(trace.to_pandas(), short_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "michael.gorkow@snowflake.com",
   "authorId": "2022491430521",
   "authorName": "ADMIN",
   "lastEditTime": 1742372847229,
   "notebookId": "36xm4wunpv7wg5vh5tkx",
   "sessionId": "9ae89340-d743-4a03-a32c-7a21443f4575"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
