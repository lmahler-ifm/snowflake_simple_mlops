{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "zkjxkclq7xrbgn3r5dez",
   "authorId": "3266042296343",
   "authorName": "ADMIN",
   "authorEmail": "michael.gorkow@snowflake.com",
   "sessionId": "2be24804-4197-43d9-a3c6-c8256dfdeaf2",
   "lastEditTime": 1739407007228
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e090cc90-29da-47e1-bd9b-2f7ac5eedad5",
   "metadata": {
    "name": "cell38",
    "collapsed": false
   },
   "source": "# Use Case: Predict Future Customer Revenue based on Historical Transactions\nTBD"
  },
  {
   "cell_type": "markdown",
   "id": "1332de60-1d04-489f-9130-9013b5e0e74c",
   "metadata": {
    "name": "cell40",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "3f59214b-4111-4c5d-8a06-1e9f091a4620",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": "# 1 - Setup Demo (üõ†Ô∏è)\n* Import required libraries\n* Create a Snowpark session\n\n| Library    | Use |\n| -------- | ------- |\n| `snowflake.snowpark` | Main Python Developer Framework for Snowflake including the DataFrame-API     |\n| `snowflake.ml`    | Snowflake ML specific functions including Feature Store & Model Registry APIs    |\n| `snowflake.cortex`    | Snowflake APIs to access Cortex Services (e.g. LLMs)    |\n| `helper_functions`  | Demo-specific functions that are nort part of any official module    |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fbb66-2845-488b-9878-1a90da9edc53",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell52"
   },
   "outputs": [],
   "source": "# Helper functions for this demo\nfrom helper_functions.setup_environment import setup_demo\nfrom helper_functions.plotting import extract_python_code, plot_inshop_vs_online_revenue, visualize_lineage, compare_two_models\nfrom helper_functions.mlops import train_new_model, simulate_model_performance\n\n\n# Import python packages\nimport plotly.express as px\nimport streamlit as st\nfrom streamlit import dataframe as sdf\nimport pandas as pd\nimport json\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Import Snowflake packages\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark.functions import lit, col\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.monitoring.entities.model_monitor_config import ModelMonitorSourceConfig, ModelMonitorConfig\nfrom snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,\n    CreationMode\n)\nfrom snowflake.cortex import Complete\n\n# Create a session\nsession = get_active_session()\nsetup_demo(session)"
  },
  {
   "cell_type": "markdown",
   "id": "23627ed5-7e8c-4017-a270-c27af9c06676",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": "# 2 - Data Exploration & Visualization\n\n* `session.table()` creates a reference to a table\n* `count()`, `order_by()`, `describe()` are dataframe operations\n* `describe()` gives us insights into the transaction amounts (e.g. min, average, max, count).\n\nWe can see that we have roughly 50K transactions across 350 customers."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15bb06-d009-458a-8627-fe407729cbf6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "transactions_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n\nprint(f'Number of transactions: {transactions_df.count()}')\nprint(f'Number of customers: {transactions_df.select(\"CUSTOMER_ID\").distinct().count()}')\n\nprint('Transactions Data:')\ntransactions_df.order_by(col('DATE').desc()).show()\n\nprint('Quick Variable Analysis:')\ntransactions_df.describe().order_by('SUMMARY').show()"
  },
  {
   "cell_type": "markdown",
   "id": "894d374a-b912-4600-a0a4-a3f82d2d1b4d",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": "### Plotting Data\n* Use a Cortex LLM to create code for aggregating the Snowpark dataframe and plotting the total revenue per month using plotly.\n* `Complete` is the Python-API to access LLMs in Snowflake. It receives the model + prompt.\n* Sometimes the LLM might generate wrong code, so I added the expected code for demo purposes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce90ae8-6848-46da-b967-f2103d3ff1de",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": "model = 'mistral-large2'\nprompt = f\"\"\"\nI have a Snowpark Dataframe called transactions_df with the following columns: {transactions_df.columns}\nWrite code using Snowpark Python to aggregate the data showing the total monthly revenue (TOTAL_REVENUE) from all channels and month (MONTH).\nAfterwards use the data to create a plotly bar chart to show total revenue per month. For the x-axis use dtick=\"M1\".\nMake sure to set use_container_width=True for the streamlit plotly chart.\nOnly return the code to transform the dataframe and plot the data using Plotly in Streamlit.\n\"\"\"\ntry:\n    result = Complete(model, prompt)\n    result = extract_python_code(result)\n    exec(result)\nexcept Exception as e:\n    print('LLM generated code contains errors.')\n    st.error(e)\n    # Aggregate the data to show total monthly revenue\n    monthly_revenue_df = (\n        transactions_df\n        .with_column(\"MONTH\", F.date_trunc(\"month\", F.col(\"DATE\")))\n        .group_by(\"MONTH\")\n        .agg(F.sum(\"TRANSACTION_AMOUNT\").as_(\"TOTAL_REVENUE\"))\n    ).to_pandas()\n    \n    # Create a Plotly bar chart\n    fig = px.bar(\n        monthly_revenue_df, \n        x=\"MONTH\", \n        y=\"TOTAL_REVENUE\", \n        title=\"Total Revenue per Month\", \n        labels={\"MONTH\": \"Month\", \"TOTAL_REVENUE\": \"Total Revenue\"},\n    )\n    \n    fig.update_xaxes(\n        dtick=\"M1\",\n        tickformat=\"%b %Y\"  # Format tick labels as \"Jan 2023\", adjust as needed\n    )\n    \n    st.plotly_chart(fig, use_container_width=True)"
  },
  {
   "cell_type": "markdown",
   "id": "ec7ed6e6-70a3-4f8b-a61c-446cf9eaada8",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "When we plot the distribution of ONLINE vs. IN_SHOP revenue, we can see that 75% of our revenue comes from customer transactions that go into our shops.  \nA model trained on this data should recognize that IN_SHOP transactions are the major driver of future customer revenue."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181530c-0dba-48aa-9489-771be296842b",
   "metadata": {
    "language": "python",
    "name": "cell47",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_inshop_vs_online_revenue(transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0114ca-d50b-4a6b-a33d-ed29b98eb692",
   "metadata": {
    "collapsed": false,
    "name": "cell17"
   },
   "source": "# 3 - Feature Store & Feature Engineering\nThe Snowflake Feature Store enables data scientists and ML engineers to create, manage, and utilize machine learning features within machine learning pipelines.  \nA feature store consists of feature views, which encapsulate Python or SQL pipelines that transform raw data into one or more related features.  \nAll features within a feature view are refreshed simultaneously from the source data.\n\nFeature store objects are implemented as Snowflake objects and all feature store objects are therefore subject to Snowflake access control rules.\n| Feature Store Object    | Snowflake Object |\n| -------- | ------- |\n| `FeatureStore` | Schema     |\n| `Entity`    | Tag    |\n| `FeatureView`  | Dynamic Table or View    |\n| `Feature`  | Column in a Dynamic Table or View    |"
  },
  {
   "cell_type": "markdown",
   "id": "c55c188a-b9d0-4da7-b6ab-7b85c3f3172b",
   "metadata": {
    "collapsed": false,
    "name": "cell26"
   },
   "source": "### Setup the Feature Store\nWe are creating (or referencing if it already exists) a Feature Store that is stored in the schema `FEATURE_STORE`.  \nThe `default_warehouse` will be used to refresh features automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db378dc4-44e5-4b5b-b55c-e1a3e14ec663",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "fs = FeatureStore(\n    session=session, \n    database=session.get_current_database(), \n    name='FEATURE_STORE', \n    default_warehouse='FEATURE_STORE_WH',\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "8c00471e-18fd-47d0-99ae-8b945e7b0bdb",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": "### Create a Feature Store Entity\nFeature views are organized in the feature store according to the entities to which they apply. An entity is a higher-level abstraction that represents the subject matter of a feature.  \nIn our example, the main entity is the `CUSTOMER` and the features we will create will be linked to this entity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286517a-2a63-4565-aa22-63104d78f21b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "# Create a new entity for the Feature Store\n",
    "entity = Entity(name=\"CUSTOMER\", join_keys=[\"CUSTOMER_ID\"], desc='Unique identifier for customers.')\n",
    "fs.register_entity(entity)\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc666d-c1be-484d-b203-4fc26b99cf88",
   "metadata": {
    "collapsed": false,
    "name": "cell31"
   },
   "source": "### Develop Features for Customer Transactions\n\nThe Snowpark Python API provides analytics functions for easily defining many common feature types, such as windowed aggregations.  \nWe will use `analytics.time_series_agg()` to quickly generate revenue for the past 1, 2 and 3 months per customer per channel which we will use as features for our machine learning model.\n\nThe feature dataframe should have the following columns:\n| Column    | Purpose |\n| -------- | ------- |\n| `CUSTOMER_ID` | Identify relevant rows for the calculated feature (Join-Criteria)     |\n| `DATE`    | Allow correct Point-in-Time Joins   |\n| `Feature columns`  | Actual features per entity    |  \n\nYou can find more functions for quickly generating featueres here:  \n[Common feature and query patterns](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/examples)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cdfdb0-10fb-4891-8c06-81c1c7dd0af3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "def col_formatter(input_col, agg, window):\n",
    "    feature_name = f\"{agg.replace('SUM','TOTAL')}_{input_col}_{window.replace('-', 'past_').replace('MM','_MONTHS')}\"\n",
    "    return feature_name\n",
    "\n",
    "in_shop_transaction_features = (\n",
    "    transactions_df.filter(col('TRANSACTION_CHANNEL') == 'IN_SHOP')\n",
    "    .group_by(['CUSTOMER_ID','DATE']).agg(F.sum('TRANSACTION_AMOUNT').as_('REVENUE'))\n",
    "    .rename({'REVENUE':'REVENUE_IN_SHOP'})\n",
    "    .analytics.time_series_agg(\n",
    "        aggs={'REVENUE_IN_SHOP':['SUM']},\n",
    "        windows=['-1MM','-2MM','-3MM'],\n",
    "        sliding_interval=\"1D\",\n",
    "        group_by=['CUSTOMER_ID'],\n",
    "        time_col='DATE',\n",
    "        col_formatter=col_formatter\n",
    "    ).drop(['SLIDING_POINT','REVENUE_IN_SHOP'])\n",
    ")\n",
    "\n",
    "online_transaction_features = (\n",
    "    transactions_df.filter(col('TRANSACTION_CHANNEL') == 'ONLINE')\n",
    "    .group_by(['CUSTOMER_ID','DATE']).agg(F.sum('TRANSACTION_AMOUNT').as_('REVENUE'))\n",
    "    .rename({'REVENUE':'REVENUE_ONLINE'})\n",
    "    .analytics.time_series_agg(\n",
    "        aggs={'REVENUE_ONLINE':['SUM']},\n",
    "        windows=['-1MM','-2MM','-3MM'],\n",
    "        sliding_interval=\"1D\",\n",
    "        group_by=['CUSTOMER_ID'],\n",
    "        time_col='DATE',\n",
    "        col_formatter=col_formatter\n",
    "    ).drop(['SLIDING_POINT','REVENUE_ONLINE'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "id": "452f6ecc-65d2-4b18-9386-ee4dbd649a2a",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "online_transaction_features.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "32994363-a331-4cf0-b156-aa3575776d69",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "**Feature Descriptions**  \nTo avoid manually writing descriptions, we can use `Complete()` to have an LLM generate JSON files containing business descriptions.  \nThese descriptions are stored in the Feature Store alongside our features."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff3e4b-b8dc-4eb8-b110-563b587d8142",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": [
    "# Use LLM to generate feature descriptions\n",
    "model = 'mistral-large2'\n",
    "\n",
    "feature_columns = in_shop_transaction_features.drop('CUSTOMER_ID','DATE').columns\n",
    "prompt = f'Return a JSON string with column names as keys and a short business description as values. The columns are: {feature_columns}. Do not wrap the json codes in JSON markers.'\n",
    "llm_response = Complete(model, prompt, stream=False)\n",
    "feature_descriptions_in_shop_transactions = json.loads(llm_response)\n",
    "\n",
    "feature_columns = online_transaction_features.drop('CUSTOMER_ID','DATE').columns\n",
    "prompt = f'Return a JSON string with column names as keys and a short business description as values. The columns are: {feature_columns}. Do not wrap the json codes in JSON markers.'\n",
    "llm_response = Complete(model, prompt, stream=False)\n",
    "feature_descriptions_online_transactions = json.loads(llm_response)\n",
    "\n",
    "st.json(feature_descriptions_in_shop_transactions)\n",
    "st.json(feature_descriptions_online_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bfb016-65ea-4690-813f-54084735c0a0",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "### Registering Feature Views\nThe `FeatureView` class accepts a Snowpark DataFrame object that contains the feature transformation logic. This allows you to define your features using any method supported by the Snowpark DataFrame API or Snowflake SQL. You can pass the DataFrame directly to the `FeatureView` constructor.  \n\nEach `FeatureView` is associated with the corresponding `Entity`.  \nThe `refresh_freq` parameter determines how often the Feature Store checks for new data and updates the features automatically. For demonstration purposes, this value is set to 1 minute, but it should be adjusted based on the specific use case."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d69f38-03c7-435e-8240-7795f752e418",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# Create Feature View\n",
    "in_shop_transaction_fv = FeatureView(\n",
    "    name=\"IN_SHOP_REVENUE_FEATURES\", \n",
    "    entities=[entity],\n",
    "    timestamp_col='DATE',\n",
    "    feature_df=in_shop_transaction_features, \n",
    "    refresh_freq=\"1 minute\",\n",
    "    refresh_mode='AUTO',\n",
    "    desc=\"Features for in-shop transactions\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Add descriptions for some features\n",
    "in_shop_transaction_fv = in_shop_transaction_fv.attach_feature_desc(feature_descriptions_in_shop_transactions)\n",
    "\n",
    "in_shop_transaction_fv = fs.register_feature_view(\n",
    "    feature_view=in_shop_transaction_fv, \n",
    "    version=\"V1\", \n",
    "    block=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Create Feature View\n",
    "online_transaction_fv = FeatureView(\n",
    "    name=\"ONLINE_REVENUE_FEATURES\", \n",
    "    entities=[entity],\n",
    "    timestamp_col='DATE',\n",
    "    feature_df=online_transaction_features, \n",
    "    refresh_freq=\"1 minute\",\n",
    "    refresh_mode='AUTO',\n",
    "    desc=\"Features for online transactions\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Add descriptions for some features\n",
    "online_transaction_fv = online_transaction_fv.attach_feature_desc(feature_descriptions_online_transactions)\n",
    "\n",
    "online_transaction_fv = fs.register_feature_view(\n",
    "    feature_view=online_transaction_fv, \n",
    "    version=\"V1\", \n",
    "    block=True,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51761481-2cbe-45d8-9bb7-6529577b0871",
   "metadata": {
    "name": "cell64",
    "collapsed": false
   },
   "source": "### Discovering Features via Feature Store API"
  },
  {
   "cell_type": "code",
   "id": "f2cc6acb-fd8b-4c41-bf39-24e5fe74affb",
   "metadata": {
    "language": "python",
    "name": "cell66"
   },
   "outputs": [],
   "source": "st.markdown('### List of all Feature Views:')\nsdf(fs.list_feature_views())\n\n# Retrieve a Feature View\nretrieved_feature_view = fs.get_feature_view(name='IN_SHOP_REVENUE_FEATURES',version='V1')\n\nst.markdown('### Feature View Columns:')\nsdf(retrieved_feature_view.list_columns())#.show(max_width=200)\n\n# Manually refresh a Feature View\nfs.refresh_feature_view(retrieved_feature_view)\n\nst.markdown('### Feature View Refresh History:')\nsdf(fs.get_refresh_history(retrieved_feature_view).limit(3))\n\n# Explore lineage information\nst.markdown('### Feature View Lineage:')\nst.json(retrieved_feature_view.lineage(direction='both'))\n\n# Use an LLM and the underlying SQL query to explain how the feature is calculated\nprompt = f'You are given a SQL query. Explain how the column TOTAL_REVENUE_IN_SHOP_PAST_1_MONTHS is calculated. The SQL query: {retrieved_feature_view.query}'\nresp = Complete('mistral-large2', prompt)\nst.markdown(f'### LLM Feature Calculation Explanation based on SQL Query:\\n {resp}')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ade4851-3069-4d9a-af29-8c650c1f1cf5",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": "# 4 - Model Training\n\n### Generate the Training Dataset with Features from Feature Store\nOur goal is to predict each customer's revenue for the next month based on their transactions from the past three months.  \n\nWe have data from January to April 2024. To define our target variable, `NEXT_MONTH_REVENUE`, we sum all transactions from April for each customer. To ensure proper point-in-time feature retrieval and avoid using future data, we only include transaction features up to **March 31, 2024**, and mark this cutoff with the `FEATURE_CUTOFF_DATE` column.  \n\nThe DataFrame you just created is a **spine DataFrame**, which acts as a reference table linking customers (`CUSTOMER_ID`) with a timestamp (`FEATURE_CUTOFF_DATE`). It ensures consistent and reproducible feature retrieval in a **feature store**.  \n\nUsing this spine, you can generate a training dataset with [`generate_dataset()`](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/modeling#generating-snowflake-datasets-for-training). The Feature Store will automatically retrieve features as they were valid on that date and add them to the dataset.  \n\nA [Snowflake Dataset](https://docs.snowflake.com/en/developer-guide/snowflake-ml/dataset) is a schema-level object designed for machine learning. It stores data in versions, ensuring immutability, efficient access, and compatibility with ML frameworks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ebd1e-0700-4646-a3ff-d871e51d3fc6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "target_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\ntarget_df = (\n    target_df.filter(col('DATE').between('2024-04-01','2024-04-30'))    # Generate Target Variable for April 2024\n    .group_by('CUSTOMER_ID')\n    .agg(F.sum('TRANSACTION_AMOUNT').as_('NEXT_MONTH_REVENUE'))\n    .with_column('FEATURE_CUTOFF_DATE', F.to_date(lit('2024-03-31')))   # Features until End of March 2024\n)\n\n# Get list of all customers\ncustomers_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n\n# Create spine dataframe\nspine_df = target_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\nspine_df = spine_df.fillna(0, subset='NEXT_MONTH_REVENUE')\nspine_df.order_by('CUSTOMER_ID').show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43830e-d929-4fdf-a94a-f6761e09fa6b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "train_dataset = fs.generate_dataset(\n",
    "    name=\"SIMPLE_MLOPS_DEMO.FEATURE_STORE.NEXT_MONTH_REVENUE_DATASET\",\n",
    "    spine_df=spine_df,\n",
    "    features=[in_shop_transaction_fv, online_transaction_fv],\n",
    "    version=\"V1\",\n",
    "    spine_timestamp_col=\"FEATURE_CUTOFF_DATE\",\n",
    "    spine_label_cols=[\"NEXT_MONTH_REVENUE\"],\n",
    "    include_feature_view_timestamp_col=False,\n",
    "    desc=\"Initial Training Dataset\"\n",
    ")\n",
    "\n",
    "df = train_dataset.read.to_snowpark_dataframe()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d081cd-ff03-4efa-9134-ae34a29e0190",
   "metadata": {
    "collapsed": false,
    "name": "cell32"
   },
   "source": "### Train an XGBoost Model\nWe randomly split the data, allocating **90% for training** and **10% for validation**.  \nThe training data is then used to train an **XGBoost regression model** with the `XGBRegressor` from the **Snowflake ML library**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbea5f6-0020-4465-add6-8a87653644ac",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df, test_df = df.random_split(weights=[0.9, 0.1], seed=0)\n",
    "\n",
    "print(f'Number of samples in train: {train_df.count()}')\n",
    "print(f'Number of samples in test: {test_df.count()}')\n",
    "\n",
    "feature_columns = train_df.drop(['CUSTOMER_ID','FEATURE_CUTOFF_DATE','NEXT_MONTH_REVENUE']).columns\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    input_cols=feature_columns,\n",
    "    label_cols=['NEXT_MONTH_REVENUE'],\n",
    "    output_cols=['NEXT_MONTH_REVENUE_PREDICTION'],\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_model = xgb_model.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca74d9a-0504-48ae-a6ab-b399a56f2d56",
   "metadata": {
    "collapsed": false,
    "name": "cell24"
   },
   "source": "### Evaluate the XGBoost Model\nYou can immediately use the model‚Äôs `predict()` function to generate predictions on the test data.  \nSnowflake ML also provides built-in metric functions, such as **Mean Absolute Percentage Error (MAPE)**, for evaluating model performance.  \n\nAdditionally, you can convert the model back to its native open-source format using `xgb_model.to_xgboost()`.  \nThis allows you to access feature importance values, which we visualize to better understand what influences the model‚Äôs predictions.  \n\nAs shown in the plot, the model correctly identified that **IN_SHOP transactions** are the primary driver of the target variable, `NEXT_MONTH_REVENUE`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737ef58-d313-4138-91e7-bb5becf601ea",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict(test_df)\n",
    "# Analyze results\n",
    "mape = mean_absolute_percentage_error(\n",
    "    df=predictions, \n",
    "    y_true_col_names=\"NEXT_MONTH_REVENUE\", \n",
    "    y_pred_col_names=\"NEXT_MONTH_REVENUE_PREDICTION\"\n",
    ")\n",
    "\n",
    "print(f\"Mean absolute percentage error: {mape}\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    # Plot Feature Importance\n",
    "    plot_data = pd.DataFrame(\n",
    "        list(zip(feature_columns, xgb_model.to_xgboost().feature_importances_)), \n",
    "        columns=['FEATURE','IMPORTANCE']\n",
    "    )\n",
    "    \n",
    "    fig = px.bar(\n",
    "        plot_data.sort_values('IMPORTANCE', ascending=False).head(10),\n",
    "        x=\"IMPORTANCE\",\n",
    "        y=\"FEATURE\",\n",
    "        title=\"Feature Importance\",\n",
    "        labels={\"FEATURE\": \"Feature\", \"IMPORTANCE\": \"Importance\"},\n",
    "        orientation=\"h\"\n",
    "    )\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "with col2:\n",
    "    # Plot Predictions\n",
    "    fig = px.scatter(\n",
    "        predictions[\"NEXT_MONTH_REVENUE\", \"NEXT_MONTH_REVENUE_PREDICTION\"].to_pandas().astype(\"float64\"),\n",
    "        x=\"NEXT_MONTH_REVENUE\",\n",
    "        y=\"NEXT_MONTH_REVENUE_PREDICTION\",\n",
    "        title=\"Actual vs Predicted Revenue\",\n",
    "        labels={\n",
    "            \"NEXT_MONTH_REVENUE\": \"Actual Revenue\",\n",
    "            \"NEXT_MONTH_REVENUE_PREDICTION\": \"Predicted Revenue\"\n",
    "        },\n",
    "        trendline=\"ols\",\n",
    "        trendline_color_override=\"red\"\n",
    "    )\n",
    "    st.plotly_chart(fig, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad317-43d5-44d8-a66e-18abf67e4a28",
   "metadata": {
    "collapsed": false,
    "name": "cell33"
   },
   "source": "# 5 - Snowflake Model Registry\n### Setup Model Registry\nAfter training a model, the first step in operationalizing it and running inference in Snowflake is to **log the model in the Snowflake Model Registry**.  \n\nThe **Model Registry** allows you to securely manage models and their metadata in Snowflake, regardless of their origin or type, while also simplifying inference.  \nIt stores machine learning models as **first-class schema-level objects** within Snowflake.  \n\nBy setting `enable_monitoring` to True, the **Model Registry** can also be used for model monitoring, which we will implement in the next step.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570b13e-2b1d-480f-8a0b-e73488f7e3c6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "# Create reference to model registry\n",
    "reg = Registry(\n",
    "    session=session, \n",
    "    database_name=session.get_current_database(), \n",
    "    schema_name='MODEL_REGISTRY', \n",
    "    options={'enable_monitoring':True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d2ca7-7f49-4692-ad0a-aedcc614d322",
   "metadata": {
    "collapsed": false,
    "name": "cell34"
   },
   "source": "### Register Model in Model Registry\nThe Model Registry's `log_model()` function takes the model object and logs it to the registry.  \nThe **name** and **version** help ensure the correct model is retrieved for inference.  \n\nAdditionally, we log relevant metrics/information, including:  \n- **MAPE (Mean Absolute Percentage Error)** calculated on the test dataset  \n- **Feature importance values**  \n- **FEATURE_CUTOFF_DATE**   \n\nWe also specify the following parameters:  \n\n| Variable               | Description  |\n|------------------------|-------------|\n| `sample_input_data`    | Sample input data used to infer model signatures, serve as background data for explanations, and capture data lineage. |\n| `conda_dependencies`   | Specifies model dependencies, such as the XGBoost library. |\n| `relax_version`        | Enforces specific dependency versions for compatibility and reproducibility. |\n| `enable_explainability` | Adds an explainability function to the model, allowing us to better understand its predictions using SHAP values. |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18434961-96fe-482a-90c5-40ea4ba8402b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": "registered_model = reg.log_model(\n    xgb_model,\n    model_name=\"CUSTOMER_REVENUE_MODEL\",\n    version_name='V1',\n    metrics={\n        'MAPE':mape, \n        'FEATURE_IMPORTANCE':dict(zip(feature_columns, xgb_model.to_xgboost().feature_importances_.astype('float'))),\n        \"TRAINING_DATA\":{'FEATURE_CUTOFF_DATE':'2024-03-31'}\n    },\n    comment=\"Model trained using XGBoost to predict revenue per customer for next month.\",\n    conda_dependencies=['xgboost'],\n    sample_input_data=train_df.select(feature_columns).limit(100),\n    options={\"relax_version\": False, \"enable_explainability\": True}\n)"
  },
  {
   "cell_type": "markdown",
   "id": "e6492927-bcca-4f8d-93c2-bb7fe490d466",
   "metadata": {
    "name": "cell41",
    "collapsed": false
   },
   "source": "### Operationalize Models\nThere are multiple ways to operationalize models using Snowflake's Model Registry.  \nOne simple approach is to use **aliases** for the model. By assigning the alias **`PRODUCTION`**, any inference pipeline referencing this alias will automatically use the correct production-ready model.  \n\nWhen a new model version is trained and ready for deployment, you can seamlessly update production by **removing the alias from the current model** and **assigning it to the new model**.  \nThis method ensures that existing ML pipelines remain unchanged, reducing the need for manual updates while maintaining a smooth model deployment process."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141827b-939f-4e43-983d-7854519e5ab7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": "registered_model.set_alias('PRODUCTION')"
  },
  {
   "cell_type": "markdown",
   "id": "0ca6642c-5fe5-4361-b970-2955611795cb",
   "metadata": {
    "name": "cell48",
    "collapsed": false
   },
   "source": "### Model explainability\nSince we enabled model_explainibility when registering the model, we can now call the explain function of the model that was auto-generated.  \nThe standard SHAP library is then used to visualise the SHAP values.\n\n**What are SHAP (SHapley Additive exPlanations) values?**  \n* SHAP (SHapley Additive exPlanations) values measure how much each feature contributes to the prediction.\n* The x-axis represents the mean absolute SHAP value, indicating the magnitude of a feature's impact on the model's predictions.\n* The y-axis lists the feature names.\n* Longer bars mean the feature has a greater impact on predictions.\n\n**Interpretation**  \nWhat is the meaning of the values?  \nFor the General Feature Importance on the left, think about it like this:  \nOn average, the feature (e.g. TOTAL_REVENUE_IN_SHOP_PAST_2_MONTHS) affects the model‚Äôs output by approximately X units of revenue.  \n\nOn a more general note, in our case IN_STORE revenue consistently ranks higher than online revenue, implying that the model sees in-store purchases as a stronger signal for future revenue prediction.\n\nHowever on the left side, it can happen that certain customers have a strong record of ONLINE transactions so these features would be more important for them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408dc362-8497-4f08-8e34-9c7f7cdf7aed",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "explanations = registered_model.run(test_df, function_name=\"explain\")\nexplanations = explanations.rename({col:col.replace('\"\"\"', '').upper() for col in explanations.columns})\n\nshap_columns = [col for col in explanations.columns if '_EXPLANATION' in col]\n\nexplanations = explanations.select('CUSTOMER_ID', *shap_columns)\nexplanations = explanations.to_pandas()\n\nimport shap\ncol1, col2 = st.columns(2)\nwith col1:\n    st.markdown(f'### Global Feature Importance')\n    shap_exp = shap._explanation.Explanation(explanations[shap_columns].values, feature_names = [col.replace('_EXPLANATION','') for col in shap_columns]) # wrapping them into a SHAP recognized object\n    shap.plots.bar(shap_exp)\nwith col2:\n    st.markdown(f'### Local Feature Importance for CUSTOMER_ID {int(explanations.iloc[0][\"CUSTOMER_ID\"])}:')\n    shap.plots.bar(shap_exp[0])"
  },
  {
   "cell_type": "markdown",
   "id": "e0a85689-b808-48ca-8c7c-cb3c307b59c0",
   "metadata": {
    "collapsed": false,
    "name": "cell35"
   },
   "source": "### Continious Model Monitoring\nModel behavior can change over time due to factors such as **input drift, stale training assumptions, data pipeline issues, hardware and software updates**.\n\n**ML Observability** enables you to monitor the quality of models registered in the **Snowflake Model Registry** across multiple dimensions, including **performance, drift, and volume**.  \n\nTo measure drift for model monitoring, we use two tables:  \n\n| Table      | Description  |\n|------------|-------------|\n| `BASELINE` | Contains a snapshot of data similar to `SOURCE`. It is used as a reference for comparing future feature values and predictions. |\n| `SOURCE`   | Stores future predictions and feature values for monitoring. |"
  },
  {
   "cell_type": "code",
   "id": "10552343-1009-4e62-989d-6afbbef0e1cb",
   "metadata": {
    "language": "python",
    "name": "cell44",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Save baseline predictions\npredictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\npredictions = predictions.with_column('NEXT_MONTH_REVENUE_PREDICTION', F.col('NEXT_MONTH_REVENUE_PREDICTION').cast('number(38,2)'))\npredictions = predictions.with_column('NEXT_MONTH_REVENUE', F.col('NEXT_MONTH_REVENUE').cast('number(38,2)'))\npredictions.write.save_as_table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1', mode='overwrite')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9d3f43e-0b16-4e68-a0f8-79d9be5edd1c",
   "metadata": {
    "name": "cell54",
    "collapsed": false
   },
   "source": "### Creating predictions for the next month\nWe use the trained model on our **April data** to predict each customer's **revenue for May**.  \nThe `get_feature_df()` function is a helper utility that constructs the **spine DataFrame** and retrieves the correct **point-in-time features** based on the `FEATURE_CUTOFF_DATE`.  \nThe predictions are then stored in the `SOURCE` table, which we will link to the **model monitor** for tracking and evaluation."
  },
  {
   "cell_type": "code",
   "id": "ef48cc36-8414-4b3d-835c-d7a42ca5f1cf",
   "metadata": {
    "language": "python",
    "name": "cell58"
   },
   "outputs": [],
   "source": "# get feature views",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c73ee7d-61ec-4842-b435-fd975061d2ea",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "def build_feature_df(session, feature_cutoff_date, feature_views):\n    # Initialize the Feature Store.\n    fs = FeatureStore(\n        session=session, \n        database=session.get_current_database(), \n        name='FEATURE_STORE', \n        default_warehouse=session.get_current_warehouse(),\n        creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n    )\n    \n    # Retrieve all feature views (version 'V1') from the Feature Store.\n    fvs = [fs.get_feature_view(name=feature_view_name,version=feature_view_version) for feature_view_name, feature_view_version in feature_views.items()]\n    \n    # Create a base (spine) DataFrame containing distinct CUSTOMER_IDs and the feature cutoff date.\n    feature_df = session.table(f'{session.get_current_database()}.RETAIL_DATA.CUSTOMERS') \\\n                        .select('CUSTOMER_ID') \\\n                        .distinct() \\\n                        .with_column('FEATURE_CUTOFF_DATE', F.to_date(lit(feature_cutoff_date)))\n    \n    # Retrieve feature values from the Feature Store for the specified cutoff date.\n    feature_df = fs.retrieve_feature_values(\n        spine_df=feature_df,\n        features=fvs,\n        spine_timestamp_col=\"FEATURE_CUTOFF_DATE\"\n    )\n    \n    # Add a placeholder column for NEXT_MONTH_REVENUE\n    feature_df = feature_df.with_column('NEXT_MONTH_REVENUE', lit(None).cast('number(38,2)'))\n    \n    return feature_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01cb20-5055-48b1-a78a-9f70dd9d4a84",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": "feature_df = build_feature_df(\n    session, \n    feature_cutoff_date='2024-04-30', \n    feature_views={'IN_SHOP_REVENUE_FEATURES':'V1', 'ONLINE_REVENUE_FEATURES':'V1'}\n)\n\nprint('Feature DataFrame:')\nfeature_df.show()\n\n# Predict May values\npredictions = registered_model.run(feature_df, function_name='PREDICT')\npredictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\npredictions = predictions.with_column('NEXT_MONTH_REVENUE_PREDICTION', F.col('NEXT_MONTH_REVENUE_PREDICTION').cast('number(38,2)'))\npredictions.write.save_as_table(table_name='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1', mode='overwrite')"
  },
  {
   "cell_type": "markdown",
   "id": "b9ca5a9b-1bbb-44a4-a8c0-05d360c8f5e8",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": "### Creating a Model Monitor  \n\nWe are setting up a **model monitor** to continuously calculate and track model performance and drift over time.  \n\nThese calculations are based on the **`BASELINE`** and **`SOURCE`** tables created earlier.  \nEach model requires its own dedicated **model monitor** to ensure accurate tracking and evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33571a-500e-4f9b-b591-2e177c9c7224",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell51"
   },
   "outputs": [],
   "source": [
    "# Enable once 1.7.3 with bugfix is available\n",
    "# source_config = ModelMonitorSourceConfig(\n",
    "#     source='MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE',\n",
    "#     timestamp_column='FEATURE_CUTOFF_DATE',\n",
    "#     id_columns=['CUSTOMER_ID'],\n",
    "#     prediction_score_columns=['NEXT_MONTH_REVENUE_PREDICTION'],\n",
    "#     actual_score_columns=['NEXT_MONTH_REVENUE'],\n",
    "#     baseline='MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1'\n",
    "# )\n",
    "\n",
    "# monitor_config = ModelMonitorConfig(\n",
    "#     model_version=reg.get_model('CUSTOMER_REVENUE_MODEL').version('PRODUCTION'),\n",
    "#     model_function_name='predict',\n",
    "#     background_compute_warehouse_name='COMPUTE_WH',\n",
    "#     refresh_interval='1 minute',\n",
    "#     aggregation_window='1 day'\n",
    "# )\n",
    "\n",
    "# reg.add_monitor(\n",
    "#     name='MLOPS_DEMO.MODEL_REGISTRY.MM_V1',\n",
    "#     source_config=source_config,\n",
    "#     model_monitor_config=monitor_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398f68d-5182-473d-9079-c42ca4d6bd12",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "cell43"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE MODEL MONITOR SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_V1 WITH\n",
    "    MODEL=SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.CUSTOMER_REVENUE_MODEL VERSION=V1 FUNCTION=PREDICT\n",
    "    SOURCE=SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1\n",
    "    BASELINE=SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1,\n",
    "    TIMESTAMP_COLUMN='FEATURE_CUTOFF_DATE'\n",
    "    ID_COLUMNS=('CUSTOMER_ID')\n",
    "    PREDICTION_SCORE_COLUMNS=('NEXT_MONTH_REVENUE_PREDICTION')\n",
    "    ACTUAL_SCORE_COLUMNS=('NEXT_MONTH_REVENUE')\n",
    "    WAREHOUSE=COMPUTE_WH\n",
    "    REFRESH_INTERVAL='1 minute'\n",
    "    AGGREGATION_WINDOW='1 day'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64990a-9de7-42f5-af03-525cf41a057a",
   "metadata": {
    "name": "cell53",
    "collapsed": false
   },
   "source": "### Simulating the next month of Customer Transactions\nOur model has predicted each customer's **revenue for May 2024** and stored the results in the **`SOURCE`** table.  \nNext, we simulate the actual transactions for May and update the **true revenue values** for each customer in the **`SOURCE`** table.  \nWhen the **model monitor** refreshes, it will use these updated values to calculate various **model performance metrics**, including the MAPE."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa1731-d7d4-4aff-bf0a-1e4f2b36a286",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell49"
   },
   "outputs": [],
   "source": "# Add new transactions (created as part of the initial demo setup)\nnew_transactions = session.table('SIMPLE_MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').filter(col('DATE').between('2024-05-01','2024-05-31'))\nnew_transactions.write.save_as_table(table_name='SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS', mode='append')\n\n# Calculate actual values\nactual_values_df = (\n    session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n    .filter(col('DATE').between('2024-05-01','2024-05-31'))\n    .group_by(['CUSTOMER_ID'])\n    .agg(F.sum('TRANSACTION_AMOUNT').as_('TOTAL_REVENUE'))\n    .with_column('DATE', F.to_date(lit('2024-04-30')))\n)\n\n# Get list of all customers\ncustomers_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n\n# Assume 0 revenue for customers without transactions\nactual_values_df = actual_values_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\nactual_values_df = actual_values_df.fillna(0,subset='TOTAL_REVENUE')\n\n# Update source table from model monitor\nsource_table = session.table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1')\nsource_table.update(\n    condition=(\n        (source_table['FEATURE_CUTOFF_DATE'] == actual_values_df['DATE']) &\n        (source_table['CUSTOMER_ID'] == actual_values_df['CUSTOMER_ID'])\n    ),\n    assignments={\n        \"NEXT_MONTH_REVENUE\": actual_values_df['TOTAL_REVENUE'],\n    },\n    source=actual_values_df\n)"
  },
  {
   "cell_type": "markdown",
   "id": "4e684bbe-dac6-4f97-8bc4-ca1e6afdd5ee",
   "metadata": {
    "collapsed": false,
    "name": "cell39"
   },
   "source": "## Simulate Customer Transactions until 2025-01-31\nFor convenience, I encapsulated all the logic for simulating future months into the helper function `simulate_model_performance()`.  \nWe use this function to simulate the model's behavior until January 2025."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd81b52-4d83-4a22-be1d-39cbe2d9ec12",
   "metadata": {
    "language": "python",
    "name": "cell75",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date = '2024-06-01'\n",
    "end_date = '2025-01-31'\n",
    "model_version = 'V1'\n",
    "\n",
    "simulate_model_performance(session, start_date, end_date, model_version, generate_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c08704-b178-40b7-888b-7b4b0c75eac7",
   "metadata": {
    "collapsed": false,
    "name": "cell46"
   },
   "source": [
    "## Explore the Model Monitor\n",
    "Navigate to the Model Monitor and observe the `MAPE` and `Difference of means`  for the last months.  \n",
    "\n",
    "You will notice the following:\n",
    "* Declining Model Performance\n",
    "    * :arrow_up_small: MAPE (Mean Average Percentage Error)\n",
    "* Feature Drift\n",
    "    * :arrow_down_small: Difference of means for TOTAL_REVENUE_IN_SHOP_PAST_1_MONTHS (less in shop transaction volume)\n",
    "    * :arrow_up_small: Difference of means for TOTAL_REVENUE_ONLINE_PAST_1_MONTHS (more online transaction volume)\n",
    "\n",
    "If we visualize the monthly revenue distribution, we can see that online revenue grew while in-shop transaction declined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aa7e2-971d-4479-9d17-570bd066cc0b",
   "metadata": {
    "language": "python",
    "name": "cell50",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_inshop_vs_online_revenue(transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2686a6c-4210-45f7-ab2d-32fb61a9d11f",
   "metadata": {
    "collapsed": false,
    "name": "cell78"
   },
   "source": "## Train a new Model Version  \n\nSince **user behavior has changed**, we will train a **new version of our model** using fresh data.  \n\nTo streamline this process, I have encapsulated the entire training workflow into the helper function `train_new_model()`, which automates the following steps:  \n\n- **Creates the spine DataFrame**, including the target variable.  \n- **Retrieves features** from the Feature Store.  \n- **Creates a Snowflake Dataset** from the training data (ensuring reproducibility with a snapshot).  \n- **Trains a new XGBoost model**.  \n- **Registers the model** in the Snowflake Model Registry.  \n- **Sets up a new model monitor** to track performance and drift.  \n- **Compares model performance** against the existing production model.  \n- **Deploys the new model** if it outperforms the current one by assigning it the **\"PRODUCTION\"** alias.  \n\nSince the training data includes **June, July, and August 2024** (covering training data up to **August 31, 2024**, and looking back three months), the model should recognize that **ONLINE transactions** have become a major driver of customer revenue."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02127e0-da97-460b-9b3c-bbf82c6dc7ba",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell90"
   },
   "outputs": [],
   "source": [
    "feature_cutoff_date = '2024-08-31'\n",
    "target_start_date = '2024-09-01'\n",
    "target_end_date = '2024-09-30'\n",
    "model_version = 'V2'\n",
    "\n",
    "train_new_model(session, feature_cutoff_date, target_start_date, target_end_date, model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75783fe-e8ed-435f-ab39-8d592166576d",
   "metadata": {
    "name": "cell55",
    "collapsed": false
   },
   "source": "### Simulate Model performance for Model Version V2 until 2025-01-31\nOnce again, we are simulating **model performance** based on customer transactions up to **January 2025**.  \nBe sure to check the **model monitor** to evaluate whether the new model version trained on more recent data performs better.  \nAdditionally, analyze the **feature drift**, where you‚Äôll notice that the trend for the **V2 model** is much more stable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781793a-c85f-40a5-8264-bbb7e9e8b4d7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell61"
   },
   "outputs": [],
   "source": "start_date = '2024-10-31'\nend_date = '2025-01-31'\nmodel_version = 'V2'\n\nsimulate_model_performance(session, start_date, end_date, model_version, generate_data=False)"
  },
  {
   "cell_type": "markdown",
   "id": "55a76de0-74ac-4f25-84f3-4717b33c51fb",
   "metadata": {
    "name": "cell56",
    "collapsed": false
   },
   "source": "### Comparing the two Model Versions\nWe have already observed that the new model provides **significantly better predictions** for future customer revenue. However, we want to gain deeper insights into **why** this improvement occurred.  \n\nTo analyze this, I am plotting the **feature importance** for both models. This reveals that the new model recognizes a **much stronger influence** of past **ONLINE transactions** on future customer revenue.  \n\nAdditionally, we can leverage the model's **explainability features**, using **SHAP values**, to further visualize and understand these relationships."
  },
  {
   "cell_type": "code",
   "id": "694fbbff-0f53-4a6e-a83b-c2074f695ca9",
   "metadata": {
    "language": "python",
    "name": "cell57",
    "collapsed": false
   },
   "outputs": [],
   "source": "compare_two_models(session,'V1','V2')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "02310bfb-5eb4-4d2b-ae60-8492e6c8625b",
   "metadata": {
    "language": "python",
    "name": "cell59"
   },
   "outputs": [],
   "source": "explanations = registered_model.run(test_df, function_name=\"explain\")\nexplanations = explanations.rename({col:col.replace('\"\"\"', '').upper() for col in explanations.columns})\n\nshap_columns = [col for col in explanations.columns if '_EXPLANATION' in col]\n\nexplanations = explanations.select('CUSTOMER_ID', *shap_columns)\nexplanations = explanations.to_pandas()\n\nimport shap\ncol1, col2 = st.columns(2)\nwith col1:\n    st.markdown(f'### Global Feature Importance')\n    shap_exp = shap._explanation.Explanation(explanations[shap_columns].values, feature_names = [col.replace('_EXPLANATION','') for col in shap_columns]) # wrapping them into a SHAP recognized object\n    shap.plots.bar(shap_exp)\nwith col2:\n    st.markdown(f'### Local Feature Importance for CUSTOMER_ID {int(explanations.iloc[0][\"CUSTOMER_ID\"])}:')\n    shap.plots.bar(shap_exp[0])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64eaba4b-dfed-4268-8c47-fb3a5a59414e",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "collapsed": false
   },
   "outputs": [],
   "source": "import shap\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.markdown('### Global Feature Importance: Model V1')\n    explaination_df = session.table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V2').filter(col('FEATURE_CUTOFF_DATE') == '2025-01-31')\n    mv = reg.get_model('CUSTOMER_REVENUE_MODEL').version('V1')\n    explanations = mv.run(explaination_df, function_name=\"explain\")\n    explanations = explanations.rename({col:col.replace('\"\"\"', '').upper() for col in explanations.columns})\n    shap_columns = [col for col in explanations.columns if '_EXPLANATION' in col]\n    explanations = explanations.select('CUSTOMER_ID', *shap_columns)\n    explanations = explanations.to_pandas()\n    \n    shap_exp = shap._explanation.Explanation(explanations[shap_columns].values, feature_names = [col.replace('_EXPLANATION','') for col in shap_columns]) \n    shap.plots.bar(shap_exp)\nwith col2:\n    st.markdown('### Global Feature Importance: Model V2')\n    explaination_df = session.table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V2').filter(col('FEATURE_CUTOFF_DATE') == '2025-01-31')\n    mv = reg.get_model('CUSTOMER_REVENUE_MODEL').version('V2')\n    explanations = mv.run(explaination_df, function_name=\"explain\")\n    explanations = explanations.rename({col:col.replace('\"\"\"', '').upper() for col in explanations.columns})\n    shap_columns = [col for col in explanations.columns if '_EXPLANATION' in col]\n    explanations = explanations.select('CUSTOMER_ID', *shap_columns)\n    explanations = explanations.to_pandas()\n    \n    shap_exp = shap._explanation.Explanation(explanations[shap_columns].values, feature_names = [col.replace('_EXPLANATION','') for col in shap_columns]) \n    shap.plots.bar(shap_exp)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6177a3c1-8d98-4900-83ea-f0ec4a910b20",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "## ML Lineage\nEven though you may not have noticed, you‚Äôve been capturing **lineage information** throughout the development of your machine learning pipeline.  \n\nYou can retrieve this information using the built-in function `lineage.trace()` for further analysis.  \nFor example, you can use this data to **visualize the lineage directly in the notebook**.  \n\nAdditionally, Snowflake provides a **more user-friendly and interactive UI** that allows you to explore and monitor your machine learning pipeline.  \n\nAs shown, the lineage captures a **comprehensive view** of your pipeline, tracking data transformations and dependencies from the **source tables**, through the **feature view**, the **training dataset**, and ultimately the **registered model** in the Model Registry."
  },
  {
   "cell_type": "code",
   "id": "84fab3ee-19e7-4a34-a86a-cd09efec9065",
   "metadata": {
    "language": "python",
    "name": "cell45",
    "collapsed": false
   },
   "outputs": [],
   "source": "trace = session.lineage.trace(\n    object_name='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.CUSTOMER_REVENUE_MODEL',\n    object_version='V1',\n    object_domain='model',\n    direction='both',\n    distance=2\n)\ntrace.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3924a8b0-75ca-46b3-bd57-c3edd59a16a4",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "collapsed": false
   },
   "outputs": [],
   "source": "visualize_lineage(trace.to_pandas(), short_names=True)",
   "execution_count": null
  }
 ]
}