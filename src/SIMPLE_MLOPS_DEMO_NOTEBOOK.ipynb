{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f59214b-4111-4c5d-8a06-1e9f091a4620",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "## Setup Demo / Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1c9a5-fb1b-43ae-b2cd-ac9eb579f3e0",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell45"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from src.setup_environment import setup_demo\n",
    "session = get_active_session()\n",
    "\n",
    "setup_demo(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d90b4-f64e-481e-b92f-41f52a7a6d45",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO.RETAIL_DATA').collect()\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO._DATA_GENERATION').collect()\n",
    "session.sql('CREATE OR REPLACE STAGE MLOPS_DEMO._DATA_GENERATION.FUNCTIONS').collect()\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO.FEATURE_STORE').collect()\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO.MODEL_REGISTRY').collect()\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO.FEATURE_STORE').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e5eed-d9b8-402b-bdc9-0cb76c1eb914",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import Session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fad4c0-d9d8-427d-a0d1-f2f4966cb367",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "from snowflake.core import Root, CreateMode\n",
    "from snowflake.core.schema import Schema\n",
    "from snowflake.core.stage import Stage\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.snowpark.types import (\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    ArrayType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from snowflake.snowpark.functions import udtf, lit, col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def generate_supermarket_revenue_data():\n",
    "    # Define the date range from 01.01.2022 to 31.01.2025\n",
    "    start_date = '2024-01-01'\n",
    "    end_date = '2025-01-31'\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Define base revenue (this is a baseline you can adjust)\n",
    "    base_revenue = 20000\n",
    "    \n",
    "    # Prepare a list to store computed records\n",
    "    records = []\n",
    "    \n",
    "    for date in dates:\n",
    "        # Determine the day-of-week: Monday=0, ... , Sunday=6\n",
    "        weekday = date.weekday()\n",
    "        \n",
    "        # Weekday effect: Saturdays have the highest revenue,\n",
    "        # Sundays are lower, and the rest are normal.\n",
    "        if weekday == 5:       # Saturday\n",
    "            weekday_factor = 1.5\n",
    "        elif weekday == 6:     # Sunday\n",
    "            weekday_factor = 0.9\n",
    "        else:\n",
    "            weekday_factor = 1.0\n",
    "        \n",
    "        # Month effect: June, July, August, and December get a boost.\n",
    "        if date.month in [6, 7, 8, 12]:\n",
    "            month_factor = 1.15\n",
    "        else:\n",
    "            month_factor = 1.0\n",
    "        \n",
    "        # Add random noise (mean 1, small standard deviation) to simulate natural fluctuations\n",
    "        noise_factor = np.random.normal(loc=1, scale=0.05)\n",
    "        \n",
    "        # Compute the final revenue\n",
    "        revenue = base_revenue * weekday_factor * month_factor * noise_factor\n",
    "        revenue = max(0, revenue)  # Ensure no negative revenue\n",
    "        \n",
    "        # Append the result as a tuple (DATE, REVENUE)\n",
    "        records.append((date, revenue))\n",
    "    \n",
    "    # Create a DataFrame from the records\n",
    "    df = pd.DataFrame(records, columns=['DATE', 'REVENUE'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "class GenerateTransactions:\n",
    "    # Draw a number from a normal distribution with defined mean, std_dev, lower and upper bounds\n",
    "    def get_norm_value(self, min, max, mean, std_dev):\n",
    "        # Calculate the a and b parameters for truncnorm\n",
    "        min = (min - mean) / std_dev\n",
    "        max = (max - mean) / std_dev\n",
    "        \n",
    "        # Generate the truncated normal distribution\n",
    "        truncated_data = truncnorm.rvs(min, max, loc=mean, scale=std_dev, size=1)[0]\n",
    "        return truncated_data\n",
    "        \n",
    "    def process(self, revenue, in_shop_online):\n",
    "        customer_id = 0\n",
    "        while revenue > 0:\n",
    "            # customer id\n",
    "            if (customer_id >= 0) and (customer_id < 100):\n",
    "                transaction_amount = np.round(self.get_norm_value(5, 25, 20, 5),2)\n",
    "            elif (customer_id >= 100) and (customer_id < 200):\n",
    "                transaction_amount = np.round(self.get_norm_value(25, 50, 40, 5),2)\n",
    "            elif (customer_id >= 200) and (customer_id < 300):\n",
    "                transaction_amount = np.round(self.get_norm_value(50, 100, 80, 10),2)\n",
    "            else:\n",
    "                transaction_amount = np.round(self.get_norm_value(100, 150, 120, 10),2)\n",
    "            transaction_channel = np.random.choice(['IN_SHOP','ONLINE'],p=in_shop_online)\n",
    "            revenue = revenue - transaction_amount\n",
    "            if customer_id == 350:\n",
    "                customer_id = 0\n",
    "            customer_id = customer_id +1\n",
    "            yield (customer_id, transaction_amount, transaction_channel)\n",
    "\n",
    "def setup_objects(session:Session):\n",
    "    root = Root(session)\n",
    "    root.databases[\"MLOPS_DEMO\"].schemas.create(schema=Schema(name=\"RETAIL_DATA\"), mode=CreateMode.or_replace)\n",
    "    root.databases[\"MLOPS_DEMO\"].schemas.create(schema=Schema(name=\"_DATA_GENERATION\"), mode=CreateMode.or_replace)\n",
    "    root.databases[\"MLOPS_DEMO\"].schemas.create(schema=Schema(name=\"FEATURE_STORE\"), mode=CreateMode.or_replace)\n",
    "    root.databases[\"MLOPS_DEMO\"].schemas.create(schema=Schema(name=\"MODEL_REGISTRY\"), mode=CreateMode.or_replace)\n",
    "    root.databases[\"MLOPS_DEMO\"].schemas['_DATA_GENERATION'].stages.create(stage=Stage(name=\"FUNCTIONS\"), mode=CreateMode.or_replace)\n",
    "    root.warehouses.create(Warehouse(name='FEATURE_STORE_WH', warehouse_size='MEDIUM'), mode=CreateMode.or_replace)\n",
    "    session.udtf.register(\n",
    "        GenerateTransactions,\n",
    "        name='MLOPS_DEMO._DATA_GENERATION.GENERATE_TRANSACTIONS',\n",
    "        stage_location='MLOPS_DEMO._DATA_GENERATION.FUNCTIONS',\n",
    "        is_permanent=True,\n",
    "        replace=True,\n",
    "        output_schema=StructType([\n",
    "            StructField(\"CUSTOMER_ID\", IntegerType()),\n",
    "            StructField(\"TRANSACTION_AMOUNT\", FloatType()),\n",
    "            StructField(\"TRANSACTION_CHANNEL\", StringType())]),\n",
    "        input_types=[FloatType(), ArrayType()],\n",
    "        packages=[\"numpy\",\"scipy\"]\n",
    "    )\n",
    "\n",
    "def generate_data(session: Session):\n",
    "    setup_objects(session)\n",
    "    revenue_df = generate_supermarket_revenue_data()\n",
    "    revenue_df = session.create_dataframe(revenue_df)\n",
    "    revenue_in_shop = revenue_df.filter(col('DATE') < lit('2024-06-01'))\n",
    "    revenue_online = revenue_df.filter(col('DATE') >= lit('2024-06-01'))\n",
    "    \n",
    "    revenue_in_shop.join_table_function('MLOPS_DEMO._DATA_GENERATION.GENERATE_TRANSACTIONS', col('REVENUE'), lit([0.75,0.25])).drop('REVENUE').write.save_as_table(table_name='MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS', mode='overwrite')\n",
    "    revenue_online.join_table_function('MLOPS_DEMO._DATA_GENERATION.GENERATE_TRANSACTIONS',col('REVENUE'), lit([0.25,0.75])).drop('REVENUE').write.save_as_table(table_name='MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS', mode='append')\n",
    "    \n",
    "    # Setting up data for demo\n",
    "    session.table('MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').filter(col('DATE') <= lit('2024-04-30')).write.save_as_table(table_name='MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS', mode='overwrite')\n",
    "    session.table('MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').select('CUSTOMER_ID').distinct().order_by('CUSTOMER_ID').write.save_as_table(table_name='MLOPS_DEMO.RETAIL_DATA.CUSTOMERS', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edeca57-6e2f-4c4f-bfc6-3e93334d0f9b",
   "metadata": {
    "language": "python",
    "name": "cell44"
   },
   "outputs": [],
   "source": [
    "generate_data(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1910602-0e49-4992-8469-ab090a0a95eb",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "# Snowflake Snowpark imports\n",
    "from snowflake.snowpark import Session \n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import udtf, lit, col\n",
    "from snowflake.snowpark.types import (\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    ArrayType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "\n",
    "# Third-party imports\n",
    "import streamlit as st\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO.RETAIL_DATA').collect()\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO._DATA_GENERATION').collect()\n",
    "session.sql('CREATE OR REPLACE STAGE MLOPS_DEMO._DATA_GENERATION.FUNCTIONS').collect()\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO.FEATURE_STORE').collect()\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO.MODEL_REGISTRY').collect()\n",
    "session.sql('CREATE OR REPLACE SCHEMA MLOPS_DEMO.FEATURE_STORE').collect()\n",
    "\n",
    "def generate_supermarket_revenue_data():\n",
    "    # Define the date range from 01.01.2022 to 31.01.2025\n",
    "    start_date = '2024-01-01'\n",
    "    end_date = '2025-01-31'\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Define base revenue (this is a baseline you can adjust)\n",
    "    base_revenue = 20000\n",
    "    \n",
    "    # Prepare a list to store computed records\n",
    "    records = []\n",
    "    \n",
    "    for date in dates:\n",
    "        # Determine the day-of-week: Monday=0, ... , Sunday=6\n",
    "        weekday = date.weekday()\n",
    "        \n",
    "        # Weekday effect: Saturdays have the highest revenue,\n",
    "        # Sundays are lower, and the rest are normal.\n",
    "        if weekday == 5:       # Saturday\n",
    "            weekday_factor = 1.5\n",
    "        elif weekday == 6:     # Sunday\n",
    "            weekday_factor = 0.9\n",
    "        else:\n",
    "            weekday_factor = 1.0\n",
    "        \n",
    "        # Month effect: June, July, August, and December get a boost.\n",
    "        if date.month in [6, 7, 8, 12]:\n",
    "            month_factor = 1.15\n",
    "        else:\n",
    "            month_factor = 1.0\n",
    "        \n",
    "        # Add random noise (mean 1, small standard deviation) to simulate natural fluctuations\n",
    "        noise_factor = np.random.normal(loc=1, scale=0.05)\n",
    "        \n",
    "        # Compute the final revenue\n",
    "        revenue = base_revenue * weekday_factor * month_factor * noise_factor\n",
    "        revenue = max(0, revenue)  # Ensure no negative revenue\n",
    "        \n",
    "        # Append the result as a tuple (DATE, REVENUE)\n",
    "        records.append((date, revenue))\n",
    "    \n",
    "    # Create a DataFrame from the records\n",
    "    df = pd.DataFrame(records, columns=['DATE', 'REVENUE'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "revenue_df = generate_supermarket_revenue_data()\n",
    "revenue_df = session.create_dataframe(revenue_df)\n",
    "\n",
    "@udtf(\n",
    "    name=\"MLOPS_DEMO._DATA_GENERATION.GENERATE_TRANSACTIONS\",\n",
    "    stage_location='MLOPS_DEMO._DATA_GENERATION.FUNCTIONS',\n",
    "    is_permanent=True,\n",
    "    replace=True,\n",
    "    output_schema=StructType([\n",
    "        StructField(\"CUSTOMER_ID\", IntegerType()),\n",
    "        StructField(\"TRANSACTION_AMOUNT\", FloatType()),\n",
    "        StructField(\"TRANSACTION_CHANNEL\", StringType())]),\n",
    "    input_types=[FloatType(), ArrayType()],\n",
    "    packages=[\"numpy\",\"scipy\"])\n",
    "class GenerateTransactions:\n",
    "    # Draw a number from a normal distribution with defined mean, std_dev, lower and upper bounds\n",
    "    def get_norm_value(self, min, max, mean, std_dev):\n",
    "        # Calculate the a and b parameters for truncnorm\n",
    "        min = (min - mean) / std_dev\n",
    "        max = (max - mean) / std_dev\n",
    "        \n",
    "        # Generate the truncated normal distribution\n",
    "        truncated_data = truncnorm.rvs(min, max, loc=mean, scale=std_dev, size=1)[0]\n",
    "        return truncated_data\n",
    "        \n",
    "    def process(self, revenue, in_shop_online):\n",
    "        customer_id = 0\n",
    "        while revenue > 0:\n",
    "            # customer id\n",
    "            if (customer_id >= 0) and (customer_id < 100):\n",
    "                transaction_amount = np.round(self.get_norm_value(5, 25, 20, 5),2)\n",
    "            elif (customer_id >= 100) and (customer_id < 200):\n",
    "                transaction_amount = np.round(self.get_norm_value(25, 50, 40, 5),2)\n",
    "            elif (customer_id >= 200) and (customer_id < 300):\n",
    "                transaction_amount = np.round(self.get_norm_value(50, 100, 80, 10),2)\n",
    "            else:\n",
    "                transaction_amount = np.round(self.get_norm_value(100, 150, 120, 10),2)\n",
    "            transaction_channel = np.random.choice(['IN_SHOP','ONLINE'],p=in_shop_online)\n",
    "            revenue = revenue - transaction_amount\n",
    "            if customer_id == 350:\n",
    "                customer_id = 0\n",
    "            customer_id = customer_id +1\n",
    "            yield (customer_id, transaction_amount, transaction_channel)\n",
    "\n",
    "\n",
    "revenue_in_shop = revenue_df.filter(col('DATE') < lit('2024-06-01'))\n",
    "revenue_online = revenue_df.filter(col('DATE') >= lit('2024-06-01'))\n",
    "\n",
    "revenue_in_shop.join_table_function(GenerateTransactions(col('REVENUE'), lit([0.75,0.25]))).drop('REVENUE').write.save_as_table(table_name='MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS', mode='overwrite')\n",
    "revenue_online.join_table_function(GenerateTransactions(col('REVENUE'), lit([0.25,0.75]))).drop('REVENUE').write.save_as_table(table_name='MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS', mode='append')\n",
    "\n",
    "# Setting up data for demo\n",
    "session.table('MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').filter(col('DATE') <= lit('2024-04-30')).write.save_as_table(table_name='MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS', mode='overwrite')\n",
    "session.table('MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').select('CUSTOMER_ID').distinct().order_by('CUSTOMER_ID').write.save_as_table(table_name='MLOPS_DEMO.RETAIL_DATA.CUSTOMERS', mode='overwrite')\n",
    "\n",
    "def is_feature_store_updated(timestamp_df):\n",
    "    feature_store_refreshes = (\n",
    "        session.table_function('INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY')\n",
    "        .filter(col('QUALIFIED_NAME').startswith('MLOPS_DEMO.FEATURE_STORE.'))\n",
    "        .select('NAME','STATE','REFRESH_END_TIME')\n",
    "        .order_by(col('REFRESH_END_TIME').desc())\n",
    "        .group_by('NAME','STATE')\n",
    "        .agg(F.max('REFRESH_END_TIME').as_('REFRESH_END_TIME'))\n",
    "        .with_column('SECONDS_SINCE_LAST_REFRESH', F.datediff('second', col('REFRESH_END_TIME'),F.current_timestamp()))\n",
    "        .cross_join(timestamp_df)\n",
    "        .with_column('UPDATED', col('REFRESH_END_TIME') > col('TIMESTAMP'))\n",
    "    )\n",
    "    if feature_store_refreshes.filter(col('UPDATED') == False).count() > 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def wait_until_feature_store_updated(interval=3):\n",
    "    ts = session.range(1).with_column('TIMESTAMP', F.current_timestamp()).drop('ID').cache_result()\n",
    "    start_time = time.time()\n",
    "    while not is_feature_store_updated(ts):\n",
    "        print(f\"\\rWaiting for updated Feature Store ... ({int(time.time()-start_time)} seconds.)\", end=\"\", flush=True)\n",
    "        time.sleep(interval)\n",
    "\n",
    "def last_day_of_month(year: int, month: int) -> datetime:\n",
    "    \"\"\"Returns the last day of a given month and year.\"\"\"\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    return datetime(year, month, last_day)\n",
    "\n",
    "def generate_last_days(start_date: str, end_date: str):\n",
    "    \"\"\"Generates the last day of each month within the given date range.\"\"\"\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    results = []\n",
    "    current = last_day_of_month(start.year, start.month)\n",
    "    \n",
    "    while current <= end:\n",
    "        results.append(current.strftime(\"%Y-%m-%d\"))\n",
    "        next_month = current.month + 1\n",
    "        next_year = current.year + (1 if next_month > 12 else 0)\n",
    "        next_month = next_month if next_month <= 12 else 1\n",
    "        current = last_day_of_month(next_year, next_month)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_feature_df(session, feature_cutoff_date):\n",
    "    fs = FeatureStore(\n",
    "        session=session, \n",
    "        database=session.get_current_database(), \n",
    "        name='FEATURE_STORE', \n",
    "        default_warehouse=session.get_current_warehouse(),\n",
    "        creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    "    )\n",
    "    fvs = [fs.get_feature_view(n[0], 'V1') for n in fs.list_feature_views().select('NAME').to_pandas().values]\n",
    "    feature_df = session.table('MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct().with_column('FEATURE_CUTOFF_DATE', F.to_date(lit(feature_cutoff_date)))\n",
    "    feature_df = fs.retrieve_feature_values(\n",
    "        spine_df=feature_df,\n",
    "        features=fvs,\n",
    "        spine_timestamp_col=\"FEATURE_CUTOFF_DATE\"\n",
    "    )\n",
    "    feature_df = feature_df.with_column('NEXT_MONTH_REVENUE', lit(None).cast('number(38,2)'))\n",
    "    return feature_df\n",
    "\n",
    "def simulate_model_performance(session, start_date, end_date, model_version, generate_data=False):\n",
    "    if generate_data == True:\n",
    "        # add transactions\n",
    "        new_transactions = session.table('MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').filter(col('DATE').between(start_date,end_date))\n",
    "        new_transactions.write.save_as_table(table_name='MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS', mode='append')\n",
    "        \n",
    "        # wait for feature store\n",
    "        wait_until_feature_store_updated()\n",
    "        print('')\n",
    "\n",
    "    # Retrieve model from model registry\n",
    "    reg = Registry(\n",
    "        session=session, \n",
    "        database_name=session.get_current_database(), \n",
    "        schema_name='MODEL_REGISTRY', \n",
    "        options={'enable_monitoring':True},\n",
    "    )\n",
    "    registered_model = reg.get_model('CUSTOMER_REVENUE_MODEL').version(model_version)\n",
    "    \n",
    "    # Generate predictions \n",
    "    for date in generate_last_days(start_date, end_date):\n",
    "        print(F'Generated predictions with features until: {date}.')\n",
    "        feature_df = get_feature_df(session, feature_cutoff_date=date)\n",
    "    \n",
    "        # Predict values\n",
    "        predictions = registered_model.run(feature_df, function_name='PREDICT')\n",
    "        predictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\n",
    "        predictions = predictions.with_column('NEXT_MONTH_REVENUE_PREDICTION', F.col('NEXT_MONTH_REVENUE_PREDICTION').cast('number(38,2)'))\n",
    "        predictions.write.save_as_table(table_name=f'MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_{model_version}', mode='append')\n",
    "\n",
    "    # Add actual values\n",
    "    actual_values_df = (\n",
    "        session.table('MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n",
    "        .filter(col('DATE').between(start_date, end_date))\n",
    "        .with_column('YEAR', F.year(col('DATE')))\n",
    "        .with_column('MONTH', F.month(col('DATE')))\n",
    "        .group_by(['CUSTOMER_ID','YEAR','MONTH'])\n",
    "        .agg(F.sum('TRANSACTION_AMOUNT').cast('decimal(38,2)').as_('REVENUE'))\n",
    "        .with_column('DATE', F.date_add(F.date_from_parts(col('YEAR'),col('MONTH'),lit(1)), lit(-1)))\n",
    "        .drop(['YEAR','MONTH'])\n",
    "    )\n",
    "    \n",
    "    # Get list of all customers\n",
    "    customers_df = session.table('MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n",
    "    \n",
    "    # Assume 0 revenue for customers without transactions\n",
    "    actual_values_df = actual_values_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\n",
    "    actual_values_df = actual_values_df.fillna(0,subset='REVENUE')\n",
    "\n",
    "    # Update source table from model monitor\n",
    "    source_table = session.table(f'MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_{model_version}')\n",
    "    update_result = source_table.update(\n",
    "        condition=(\n",
    "            (source_table['FEATURE_CUTOFF_DATE'] == actual_values_df['DATE']) &\n",
    "            (source_table['CUSTOMER_ID'] == actual_values_df['CUSTOMER_ID'])\n",
    "        ),\n",
    "        assignments={\n",
    "            \"NEXT_MONTH_REVENUE\": actual_values_df['REVENUE'],\n",
    "        },\n",
    "        source=actual_values_df\n",
    "    )\n",
    "    print(f'Updated {update_result.rows_updated} rows in source table of model monitor.')\n",
    "    return\n",
    "\n",
    "def evaluate_against_production_model(session, new_model_version, test_df):\n",
    "    reg = Registry(\n",
    "        session=session, \n",
    "        database_name=session.get_current_database(), \n",
    "        schema_name='MODEL_REGISTRY', \n",
    "        options={'enable_monitoring':True},\n",
    "    )\n",
    "    production_model = reg.get_model('CUSTOMER_REVENUE_MODEL').version('PRODUCTION')\n",
    "    production_model_predictions = production_model.run(test_df, function_name='PREDICT')\n",
    "    production_model_mape = mean_absolute_percentage_error(\n",
    "        df=production_model_predictions, \n",
    "        y_true_col_names=\"NEXT_MONTH_REVENUE\", \n",
    "        y_pred_col_names=\"NEXT_MONTH_REVENUE_PREDICTION\"\n",
    "    )\n",
    "    \n",
    "    development_model = reg.get_model('CUSTOMER_REVENUE_MODEL').version(new_model_version)\n",
    "    development_model_predictions = development_model.run(test_df, function_name='PREDICT')\n",
    "    development_model_mape = mean_absolute_percentage_error(\n",
    "        df=development_model_predictions, \n",
    "        y_true_col_names=\"NEXT_MONTH_REVENUE\", \n",
    "        y_pred_col_names=\"NEXT_MONTH_REVENUE_PREDICTION\"\n",
    "    )\n",
    "    print(development_model_mape, production_model_mape)\n",
    "    if development_model_mape < production_model_mape:\n",
    "        print(f\"New model with version {new_model_version} has a lower MAPE compared to current production model.\")\n",
    "        mape_values_df = pd.DataFrame([['PRODUCTION', production_model_mape],[new_model_version,development_model_mape]], columns=['VERSION','MAPE'])\n",
    "        print(tabulate(mape_values_df, headers='keys', tablefmt='grid'))\n",
    "        production_model.unset_alias('PRODUCTION')\n",
    "        production_model.set_alias('DEPRECATED')\n",
    "        development_model.set_alias('PRODUCTION')\n",
    "    else:\n",
    "        print(f\"Existing production model has a lower MAPE compared to the developed model.\")\n",
    "\n",
    "def train_new_model(session, feature_cutoff_date, target_start_date, target_end_date, model_version):\n",
    "    fs = FeatureStore(\n",
    "        session=session, \n",
    "        database=session.get_current_database(), \n",
    "        name='FEATURE_STORE', \n",
    "        default_warehouse=session.get_current_warehouse(),\n",
    "        creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    "    )\n",
    "    fvs = [fs.get_feature_view(n[0], 'V1') for n in fs.list_feature_views().select('NAME').to_pandas().values]\n",
    "    # Create dataset for training\n",
    "    print('Creating training dataset...')\n",
    "    target_df = session.table('MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n",
    "    target_df = (\n",
    "        target_df.filter(col('DATE').between(target_start_date,target_end_date))\n",
    "        .group_by('CUSTOMER_ID')\n",
    "        .agg(F.sum('TRANSACTION_AMOUNT').as_('NEXT_MONTH_REVENUE'))\n",
    "        .with_column('FEATURE_CUTOFF_DATE', F.to_date(lit(feature_cutoff_date)))\n",
    "    )\n",
    "    customers_df = session.table('MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n",
    "    spine_df = target_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\n",
    "    spine_df = spine_df.fillna(0, subset='NEXT_MONTH_REVENUE')\n",
    "    train_dataset = fs.generate_dataset(\n",
    "        name=\"MLOPS_DEMO.FEATURE_STORE.NEXT_MONTH_REVENUE_DATASET\",\n",
    "        spine_df=spine_df,\n",
    "        features=fvs,\n",
    "        version=model_version,\n",
    "        spine_timestamp_col=\"FEATURE_CUTOFF_DATE\",\n",
    "        spine_label_cols=[\"NEXT_MONTH_REVENUE\"],\n",
    "        include_feature_view_timestamp_col=False,\n",
    "        desc=\"Training Dataset from September 2024\"\n",
    "    )\n",
    "    df = train_dataset.read.to_snowpark_dataframe()\n",
    "    print('Training dataset created.')\n",
    "    # Train new model\n",
    "    print(f'Training new model with version {model_version}...')\n",
    "    train_df, test_df = df.random_split(weights=[0.9, 0.1], seed=0)\n",
    "    feature_columns = train_df.drop(['CUSTOMER_ID','FEATURE_CUTOFF_DATE','NEXT_MONTH_REVENUE']).columns\n",
    "    xgb_model = XGBRegressor(\n",
    "        input_cols=feature_columns,\n",
    "        label_cols=['NEXT_MONTH_REVENUE'],\n",
    "        output_cols=['NEXT_MONTH_REVENUE_PREDICTION'],\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        random_state=0\n",
    "    )\n",
    "    xgb_model = xgb_model.fit(train_df)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(f'Evaluating model...')\n",
    "    predictions = xgb_model.predict(test_df)\n",
    "    mape = mean_absolute_percentage_error(\n",
    "        df=predictions, \n",
    "        y_true_col_names=\"NEXT_MONTH_REVENUE\", \n",
    "        y_pred_col_names=\"NEXT_MONTH_REVENUE_PREDICTION\"\n",
    "    )\n",
    "    \n",
    "    # Register new model version\n",
    "    reg = Registry(\n",
    "        session=session, \n",
    "        database_name=session.get_current_database(), \n",
    "        schema_name='MODEL_REGISTRY', \n",
    "        options={'enable_monitoring':True},\n",
    "    )\n",
    "    \n",
    "    registered_model = reg.log_model(\n",
    "        xgb_model,\n",
    "        model_name=\"CUSTOMER_REVENUE_MODEL\",\n",
    "        version_name=model_version,\n",
    "        metrics={\n",
    "            'MAPE':mape, \n",
    "            'FEATURE_IMPORTANCE':dict(zip(feature_columns, xgb_model.to_xgboost().feature_importances_.astype('float'))),\n",
    "            'TRAINING_DATA':{'FEATURE_CUTOFF_DATE':feature_cutoff_date}\n",
    "        },\n",
    "        comment=\"Model trained using XGBoost to predict revenue per customer for next month.\",\n",
    "        conda_dependencies=['xgboost'],\n",
    "        sample_input_data=train_df.select(feature_columns).limit(10),\n",
    "        options={\"relax_version\": False, \"enable_explainability\": True}\n",
    "    )\n",
    "    print(f'Registered new model with version {model_version} in model registry.')\n",
    "\n",
    "    # Evaluate model and set new model into production if better than old model\n",
    "    evaluate_against_production_model(session, model_version, test_df)\n",
    "\n",
    "    # Save baseline predictions\n",
    "    predictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\n",
    "    predictions = predictions.with_column('NEXT_MONTH_REVENUE_PREDICTION', F.col('NEXT_MONTH_REVENUE_PREDICTION').cast('number(38,2)'))\n",
    "    predictions = predictions.with_column('NEXT_MONTH_REVENUE', F.col('NEXT_MONTH_REVENUE').cast('number(38,2)'))\n",
    "    predictions.write.save_as_table(f'MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_{model_version}', mode='overwrite')\n",
    "    print(f'Baseline table for model monitor created with predictions until {feature_cutoff_date}.')\n",
    "    \n",
    "    # Create predictions for next month to create model monitor source table\n",
    "    # We can use former target_end_date\n",
    "    feature_df = get_feature_df(session, feature_cutoff_date=target_end_date)\n",
    "    predictions = registered_model.run(feature_df, function_name='PREDICT')\n",
    "    predictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\n",
    "    predictions = predictions.with_column('NEXT_MONTH_REVENUE_PREDICTION', F.col('NEXT_MONTH_REVENUE_PREDICTION').cast('number(38,2)'))\n",
    "    predictions.write.save_as_table(table_name=f'MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_{model_version}', mode='overwrite')\n",
    "    print(f'Source table for model monitor created with predictions between {target_start_date} and {target_end_date}.')\n",
    "\n",
    "    session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE MODEL MONITOR MLOPS_DEMO.MODEL_REGISTRY.MM_{model_version} WITH\n",
    "        MODEL=MLOPS_DEMO.MODEL_REGISTRY.CUSTOMER_REVENUE_MODEL VERSION={model_version} FUNCTION=PREDICT\n",
    "        SOURCE=MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_{model_version}\n",
    "        BASELINE=MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_{model_version},\n",
    "        TIMESTAMP_COLUMN='FEATURE_CUTOFF_DATE'\n",
    "        ID_COLUMNS=('CUSTOMER_ID')\n",
    "        PREDICTION_SCORE_COLUMNS=('NEXT_MONTH_REVENUE_PREDICTION')\n",
    "        ACTUAL_SCORE_COLUMNS=('NEXT_MONTH_REVENUE')\n",
    "        WAREHOUSE=COMPUTE_WH\n",
    "        REFRESH_INTERVAL='1 minute'\n",
    "        AGGREGATION_WINDOW='1 day'\"\"\").collect()\n",
    "    print(f'Model monitor created.')\n",
    "\n",
    "    # Enable once 1.7.3 with bugfix is available\n",
    "    # source_config = ModelMonitorSourceConfig(\n",
    "    #     source=f'MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_{model_version}',\n",
    "    #     timestamp_column='FEATURE_CUTOFF_DATE',\n",
    "    #     id_columns=['CUSTOMER_ID'],\n",
    "    #     prediction_score_columns=['NEXT_MONTH_REVENUE_PREDICTION'],\n",
    "    #     actual_score_columns=['NEXT_MONTH_REVENUE'],\n",
    "    #     baseline=f'MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_{model_version}'\n",
    "    # )\n",
    "    \n",
    "    # monitor_config = ModelMonitorConfig(\n",
    "    #     model_version=reg.get_model('CUSTOMER_REVENUE_MODEL').version('PRODUCTION'),\n",
    "    #     model_function_name='predict',\n",
    "    #     background_compute_warehouse_name='COMPUTE_WH',\n",
    "    #     refresh_interval='1 minute',\n",
    "    #     aggregation_window='1 day'\n",
    "    # )\n",
    "    \n",
    "    # reg.add_monitor(\n",
    "    #     name=f'MLOPS_DEMO.MODEL_REGISTRY.MM_{model_version}',\n",
    "    #     source_config=source_config,\n",
    "    #     model_monitor_config=monitor_config\n",
    "    # )\n",
    "    return\n",
    "\n",
    "def compare_two_models(session, version_name_1, version_name_2):\n",
    "    reg = Registry(\n",
    "        session=session, \n",
    "        database_name=session.get_current_database(), \n",
    "        schema_name='MODEL_REGISTRY', \n",
    "        options={'enable_monitoring':True},\n",
    "    )\n",
    "    local_model_object1 = reg.get_model('CUSTOMER_REVENUE_MODEL').version(version_name_1).load()\n",
    "    feature_cols1 = local_model_object1.input_cols\n",
    "    plot_data1 = pd.DataFrame(\n",
    "        list(zip(feature_cols1, local_model_object1.to_xgboost().feature_importances_)), \n",
    "        columns=['FEATURE','IMPORTANCE']\n",
    "    )\n",
    "    local_model_object2 = reg.get_model('CUSTOMER_REVENUE_MODEL').version(version_name_2).load()\n",
    "    feature_cols2 = local_model_object2.input_cols\n",
    "    plot_data2 = pd.DataFrame(\n",
    "        list(zip(feature_cols2, local_model_object2.to_xgboost().feature_importances_)), \n",
    "        columns=['FEATURE','IMPORTANCE']\n",
    "    )\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        fig = px.bar(\n",
    "            plot_data1.sort_values('IMPORTANCE', ascending=False).head(10),\n",
    "            x=\"IMPORTANCE\",\n",
    "            y=\"FEATURE\",\n",
    "            title=f\"Feature Importance Model {version_name_1}\",\n",
    "            labels={\"FEATURE\": \"Feature\", \"IMPORTANCE\": \"Importance\"},\n",
    "            orientation=\"h\"\n",
    "        )\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    with col2:\n",
    "        fig = px.bar(\n",
    "            plot_data2.sort_values('IMPORTANCE', ascending=False).head(10),\n",
    "            x=\"IMPORTANCE\",\n",
    "            y=\"FEATURE\",\n",
    "            title=f\"Feature Importance Model {version_name_2}\",\n",
    "            labels={\"FEATURE\": \"Feature\", \"IMPORTANCE\": \"Importance\"},\n",
    "            orientation=\"h\"\n",
    "        )\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Function that extracts the actual Python code returned by mistral\n",
    "def extract_python_code(text):\n",
    "    # Regular expression pattern to extract content between triple backticks with 'python' as language identifier\n",
    "    pattern = r\"```python(.*?)```\"\n",
    "\n",
    "    # re.DOTALL allows the dot (.) to match newlines as well\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        # Return the matched group, stripping any leading or trailing whitespace\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return \"No Python code found in the input string.\"\n",
    "\n",
    "def plot_inshop_vs_online_revenue(transactions_df):\n",
    "    aggregated_df = (\n",
    "        transactions_df.with_column(\"MONTH\", F.date_trunc(\"month\", F.col(\"DATE\")))\n",
    "        .group_by(\"MONTH\", \"TRANSACTION_CHANNEL\")\n",
    "        .agg(F.sum(\"TRANSACTION_AMOUNT\").alias(\"TOTAL_REVENUE\"))\n",
    "    )\n",
    "    \n",
    "    # 2. Bring the aggregated results into a Pandas DataFrame for further processing\n",
    "    pdf = aggregated_df.to_pandas()\n",
    "    \n",
    "    # Optional: Convert the MONTH column to a more readable string format (e.g., 'YYYY-MM')\n",
    "    pdf[\"MONTH\"] = pdf[\"MONTH\"].dt.strftime('%Y-%m')\n",
    "    \n",
    "    # 3. Compute the monthly total revenue and calculate percentage for each transaction channel\n",
    "    pdf[\"monthly_total\"] = pdf.groupby(\"MONTH\")[\"TOTAL_REVENUE\"].transform(\"sum\")\n",
    "    pdf[\"TOTAL_REVENUE\"] = pdf[\"TOTAL_REVENUE\"] / pdf[\"monthly_total\"] * 100\n",
    "    \n",
    "    # 4. Create a stacked bar chart using the computed percentage values\n",
    "    fig = px.bar(\n",
    "        pdf,\n",
    "        x=\"MONTH\",\n",
    "        y=\"TOTAL_REVENUE\",\n",
    "        color=\"TRANSACTION_CHANNEL\",\n",
    "        barmode=\"stack\",\n",
    "        labels={\n",
    "            \"TOTAL_REVENUE\": \"Percentage of Revenue\",\n",
    "            \"MONTH\": \"Month\",\n",
    "            \"TRANSACTION_CHANNEL\": \"Transaction Channel\"\n",
    "        },\n",
    "        text=pdf['TOTAL_REVENUE'].apply(lambda x: f\"{x/100:.0%}\"),\n",
    "        title=\"Monthly Revenue Distribution by Transaction Channel (Normalized to 100%)\"\n",
    "    )\n",
    "    \n",
    "    # Update y-axis to display percentage signs\n",
    "    fig.update_layout(yaxis=dict(ticksuffix=\"%\"))\n",
    "    \n",
    "    fig.update_xaxes(\n",
    "        dtick=\"M1\",\n",
    "        tickformat=\"%b %Y\"  # Format tick labels as \"Jan 2023\", adjust as needed\n",
    "    )\n",
    "    \n",
    "    # 5. Display the figure\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "import json\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def visualize_lineage(df: pd.DataFrame, short_names: bool = False, initial_zoom: float = 1.0):\n",
    "    \"\"\"\n",
    "    Visualize a lineage graph given a DataFrame with columns:\n",
    "      - SOURCE_OBJECT (JSON string)\n",
    "      - TARGET_OBJECT (JSON string)\n",
    "      - DIRECTION (e.g. \"Upstream\")\n",
    "      - DISTANCE (an integer: the number of steps from the ultimate target)\n",
    "    \n",
    "    The ultimate target is taken from row 0's TARGET_OBJECT and is assigned distance 0.\n",
    "    \n",
    "    Parameters:\n",
    "      df: pandas DataFrame containing the lineage information.\n",
    "      short_names: If True, node labels will be shortened (e.g. by taking the last dot‐separated part).\n",
    "      initial_zoom: A scale factor for the initial zoom level (default 1.0). Values > 1 zoom in;\n",
    "                    values < 1 zoom out.\n",
    "    \n",
    "    Nodes are arranged in vertical columns by distance (with nodes farthest from the target on the left).\n",
    "    Each node is colored based on its domain, and a legend is added for the node colors.\n",
    "    \"\"\"\n",
    "    # Create an empty directed graph.\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Parse the ultimate target from row 0's TARGET_OBJECT and add it with distance 0.\n",
    "    ultimate_target = json.loads(df.iloc[0][\"TARGET_OBJECT\"])\n",
    "    ultimate_target_id = ultimate_target[\"name\"]\n",
    "    G.add_node(ultimate_target_id, domain=ultimate_target.get(\"domain\", \"Unknown\"), distance=0)\n",
    "\n",
    "    # Loop through each row to add nodes and edges.\n",
    "    # We assume that the \"DISTANCE\" column applies to the SOURCE_OBJECT.\n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse the source object.\n",
    "        try:\n",
    "            src_obj = json.loads(row[\"SOURCE_OBJECT\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing SOURCE_OBJECT at row {idx}: {e}\")\n",
    "            continue\n",
    "        src_id = src_obj.get(\"name\")\n",
    "        src_domain = src_obj.get(\"domain\", \"Unknown\")\n",
    "        src_distance = row[\"DISTANCE\"]  # distance from ultimate target\n",
    "\n",
    "        # Add or update source node with its distance (keeping the smaller distance if node exists).\n",
    "        if src_id in G.nodes:\n",
    "            G.nodes[src_id][\"distance\"] = min(G.nodes[src_id][\"distance\"], src_distance)\n",
    "        else:\n",
    "            G.add_node(src_id, domain=src_domain, distance=src_distance)\n",
    "\n",
    "        # Parse the target object.\n",
    "        try:\n",
    "            tgt_obj = json.loads(row[\"TARGET_OBJECT\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing TARGET_OBJECT at row {idx}: {e}\")\n",
    "            continue\n",
    "        tgt_id = tgt_obj.get(\"name\")\n",
    "        tgt_domain = tgt_obj.get(\"domain\", \"Unknown\")\n",
    "        # For non-ultimate targets we assign distance = (source distance - 1).\n",
    "        # (This works as long as the lineage chain is consistent.)\n",
    "        if tgt_id == ultimate_target_id:\n",
    "            tgt_distance = 0\n",
    "        else:\n",
    "            tgt_distance = row[\"DISTANCE\"] - 1\n",
    "\n",
    "        if tgt_id in G.nodes:\n",
    "            G.nodes[tgt_id][\"distance\"] = min(G.nodes[tgt_id][\"distance\"], tgt_distance)\n",
    "        else:\n",
    "            G.add_node(tgt_id, domain=tgt_domain, distance=tgt_distance)\n",
    "\n",
    "        # Add an edge from source to target (i.e. upstream relationship).\n",
    "        G.add_edge(src_id, tgt_id)\n",
    "\n",
    "    # --- Compute layout positions ------------------------------------------------\n",
    "    # Arrange nodes in vertical columns by distance.\n",
    "    # Get the maximum distance (farthest from the ultimate target).\n",
    "    distances = [data[\"distance\"] for _, data in G.nodes(data=True)]\n",
    "    max_distance = max(distances)\n",
    "\n",
    "    # Group nodes by their distance value.\n",
    "    distance_groups = {}  # distance -> list of node ids.\n",
    "    for node, data in G.nodes(data=True):\n",
    "        d = data[\"distance\"]\n",
    "        distance_groups.setdefault(d, []).append(node)\n",
    "\n",
    "    # Assign positions:\n",
    "    #   x-coordinate: use max_distance - d so that nodes with highest d appear on the left.\n",
    "    #   y-coordinate: for nodes with the same d, spread them evenly vertically.\n",
    "    pos = {}\n",
    "    for d, nodes in distance_groups.items():\n",
    "        nodes_sorted = sorted(nodes)  # sort alphabetically for stability.\n",
    "        n = len(nodes_sorted)\n",
    "        # Create y positions so that they are centered around 0.\n",
    "        y_positions = np.linspace((n - 1) / 2, -(n - 1) / 2, n)\n",
    "        x = max_distance - d  # ultimate target (d=0) gets the rightmost x value.\n",
    "        for i, node in enumerate(nodes_sorted):\n",
    "            pos[node] = (x, y_positions[i])\n",
    "\n",
    "    # --- Determine axis ranges based on initial_zoom -----------------------------\n",
    "    # Compute the min and max for x and y positions.\n",
    "    xs = [p[0] for p in pos.values()]\n",
    "    ys = [p[1] for p in pos.values()]\n",
    "    if xs:\n",
    "        x_min, x_max = min(xs), max(xs)\n",
    "    else:\n",
    "        x_min, x_max = -1, 1\n",
    "    if ys:\n",
    "        y_min, y_max = min(ys), max(ys)\n",
    "    else:\n",
    "        y_min, y_max = -1, 1\n",
    "\n",
    "    # Compute center and half-width/half-height.\n",
    "    x_center = (x_min + x_max) / 2\n",
    "    y_center = (y_min + y_max) / 2\n",
    "    # Add a margin factor (here 1.2) so nodes are not at the very edge.\n",
    "    margin_factor = 1.2\n",
    "    x_half = ((x_max - x_min) / 2) * margin_factor / initial_zoom\n",
    "    y_half = ((y_max - y_min) / 2) * margin_factor / initial_zoom\n",
    "\n",
    "    x_range = [x_center - x_half, x_center + x_half]\n",
    "    y_range = [y_center - y_half, y_center + y_half]\n",
    "\n",
    "    # --- Create Plotly traces for edges ------------------------------------------\n",
    "    # Prepare a single trace for edges (drawn as line segments).\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for u, v in G.edges():\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, \n",
    "        y=edge_y,\n",
    "        line=dict(width=1, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "\n",
    "    # --- Create Plotly traces for nodes ------------------------------------------\n",
    "    # Define a color mapping for domains (customize as needed)\n",
    "    color_map = {\n",
    "        \"MODEL\": \"#FF5733\",         # reddish\n",
    "        \"DATASET\": \"#33C3FF\",        # blueish\n",
    "        \"TABLE\": \"#33FF57\",          # greenish\n",
    "        \"FEATURE_VIEW\": \"#FF33F6\",   # magenta-ish\n",
    "    }\n",
    "\n",
    "    # Group nodes by domain so that a separate trace (and legend entry) is created per domain.\n",
    "    domain_nodes = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        domain = data.get(\"domain\", \"Unknown\")\n",
    "        # Shorten the label if required.\n",
    "        label = node.split('.')[-1] if short_names else node\n",
    "        domain_nodes.setdefault(domain, {\"x\": [], \"y\": [], \"text\": []})\n",
    "        x, y = pos[node]\n",
    "        domain_nodes[domain][\"x\"].append(x)\n",
    "        domain_nodes[domain][\"y\"].append(y)\n",
    "        domain_nodes[domain][\"text\"].append(label)\n",
    "\n",
    "    node_traces = []\n",
    "    for domain, values in domain_nodes.items():\n",
    "        trace = go.Scatter(\n",
    "            x=values[\"x\"],\n",
    "            y=values[\"y\"],\n",
    "            mode='markers+text',\n",
    "            name=domain,  # legend entry will show the domain name.\n",
    "            text=values[\"text\"],\n",
    "            textposition=\"bottom center\",\n",
    "            hoverinfo='text',\n",
    "            marker=dict(\n",
    "                showscale=False,\n",
    "                color=color_map.get(domain, \"#CCCCCC\"),\n",
    "                size=30,\n",
    "                line_width=2\n",
    "            )\n",
    "        )\n",
    "        node_traces.append(trace)\n",
    "\n",
    "    # --- Create and show the figure ----------------------------------------------\n",
    "    fig = go.Figure(\n",
    "        data=[edge_trace] + node_traces,\n",
    "        layout=go.Layout(\n",
    "            title=\"Lineage Visualization\",\n",
    "            titlefont_size=16,\n",
    "            showlegend=True,\n",
    "            hovermode='closest',\n",
    "            margin=dict(b=20, l=20, r=20, t=40),\n",
    "            xaxis=dict(range=x_range, showgrid=False, zeroline=False, showticklabels=False),\n",
    "            yaxis=dict(range=y_range, showgrid=False, zeroline=False, showticklabels=False)\n",
    "        )\n",
    "    )\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "\n",
    "print('Demo Setup finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abcc37-22dd-466e-8f77-4863ada40536",
   "metadata": {
    "collapsed": false,
    "name": "cell22"
   },
   "source": [
    "## 1 - Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf8e10-cadd-48f9-a1bf-51c859946b46",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Import Snowflake packages\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import lit, col\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.monitoring.entities.model_monitor_config import ModelMonitorSourceConfig, ModelMonitorConfig\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.cortex import Complete\n",
    "\n",
    "# Create a session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23627ed5-7e8c-4017-a270-c27af9c06676",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "## 2 - Data Exploration & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15bb06-d009-458a-8627-fe407729cbf6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "transactions_df = session.table('MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n",
    "\n",
    "print(f'Number of transactions: {transactions_df.count()}')\n",
    "print('Transactions Data:')\n",
    "transactions_df.order_by(col('DATE').desc()).show()\n",
    "\n",
    "print('Quick Variable Analysis:')\n",
    "transactions_df.describe().order_by('SUMMARY').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d374a-b912-4600-a0a4-a3f82d2d1b4d",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "### Plotting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce90ae8-6848-46da-b967-f2103d3ff1de",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": [
    "model = 'mistral-large2'\n",
    "prompt = f\"\"\"\n",
    "I have a Snowpark Dataframe called transactions_df with the following columns: {transactions_df.columns}\n",
    "Write code using Snowpark Python to aggregate the data showing the total monthly revenue (TOTAL_REVENUE) from all channels and month (MONTH).\n",
    "Afterwards use the data to create a plotly bar chart to show total revenue per month. For the x-axis use dtick=\"M1\".\n",
    "Make sure to use the container-width for the plotly chart.\n",
    "Only return the code to transform the dataframe and plot the data using Plotly in Streamlit.\n",
    "\"\"\"\n",
    "try:\n",
    "    result = Complete(model, prompt)\n",
    "    result = extract_python_code(result)\n",
    "    exec(result)\n",
    "except Exception as e:\n",
    "    st.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44144167-3e25-4fe6-b2e9-e52407d89d0a",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell48"
   },
   "outputs": [],
   "source": [
    "# BACKUP\n",
    "# Aggregate the data to show total monthly revenue\n",
    "monthly_revenue_df = (\n",
    "    transactions_df\n",
    "    .with_column(\"MONTH\", F.date_trunc(\"month\", F.col(\"DATE\")))\n",
    "    .group_by(\"MONTH\")\n",
    "    .agg(F.sum(\"TRANSACTION_AMOUNT\").as_(\"TOTAL_REVENUE\"))\n",
    ").to_pandas()\n",
    "\n",
    "# Create a Plotly bar chart\n",
    "fig = px.bar(\n",
    "    monthly_revenue_df, \n",
    "    x=\"MONTH\", \n",
    "    y=\"TOTAL_REVENUE\", \n",
    "    title=\"Total Revenue per Month\", \n",
    "    labels={\"MONTH\": \"Month\", \"TOTAL_REVENUE\": \"Total Revenue\"},\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    dtick=\"M1\",\n",
    "    tickformat=\"%b %Y\"  # Format tick labels as \"Jan 2023\", adjust as needed\n",
    ")\n",
    "\n",
    "st.plotly_chart(fig, use_container_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181530c-0dba-48aa-9489-771be296842b",
   "metadata": {
    "language": "python",
    "name": "cell47"
   },
   "outputs": [],
   "source": [
    "plot_inshop_vs_online_revenue(transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0114ca-d50b-4a6b-a33d-ed29b98eb692",
   "metadata": {
    "collapsed": false,
    "name": "cell17"
   },
   "source": [
    "## 3 - Feature Store & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c188a-b9d0-4da7-b6ab-7b85c3f3172b",
   "metadata": {
    "collapsed": false,
    "name": "cell26"
   },
   "source": [
    "### Setup the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db378dc4-44e5-4b5b-b55c-e1a3e14ec663",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=session.get_current_database(), \n",
    "    name='FEATURE_STORE', \n",
    "    default_warehouse=session.get_current_warehouse(),\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00471e-18fd-47d0-99ae-8b945e7b0bdb",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "### Create a Feature Store Entity \"CUSTOMER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286517a-2a63-4565-aa22-63104d78f21b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "# Create a new entity for the Feature Store\n",
    "entity = Entity(name=\"CUSTOMER\", join_keys=[\"CUSTOMER_ID\"], desc='Unique identifier for customers.')\n",
    "fs.register_entity(entity)\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc666d-c1be-484d-b203-4fc26b99cf88",
   "metadata": {
    "collapsed": false,
    "name": "cell31"
   },
   "source": [
    "### Add Transaction Features about Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cdfdb0-10fb-4891-8c06-81c1c7dd0af3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "def col_formatter(input_col, agg, window):\n",
    "    feature_name = f\"{agg.replace('SUM','TOTAL')}_{input_col}_{window.replace('-', 'past_').replace('MM','_MONTHS')}\"\n",
    "    return feature_name\n",
    "\n",
    "in_shop_transaction_features = (\n",
    "    transactions_df.filter(col('TRANSACTION_CHANNEL') == 'IN_SHOP')\n",
    "    .group_by(['CUSTOMER_ID','DATE']).agg(F.sum('TRANSACTION_AMOUNT').as_('REVENUE'))\n",
    "    .rename({'REVENUE':'REVENUE_IN_SHOP'})\n",
    "    .analytics.time_series_agg(\n",
    "        aggs={'REVENUE_IN_SHOP':['SUM']},\n",
    "        windows=['-1MM','-2MM','-3MM'],\n",
    "        sliding_interval=\"1D\",\n",
    "        group_by=['CUSTOMER_ID'],\n",
    "        time_col='DATE',\n",
    "        col_formatter=col_formatter\n",
    "    ).drop(['SLIDING_POINT','REVENUE_IN_SHOP'])\n",
    ")\n",
    "\n",
    "online_transaction_features = (\n",
    "    transactions_df.filter(col('TRANSACTION_CHANNEL') == 'ONLINE')\n",
    "    .group_by(['CUSTOMER_ID','DATE']).agg(F.sum('TRANSACTION_AMOUNT').as_('REVENUE'))\n",
    "    .rename({'REVENUE':'REVENUE_ONLINE'})\n",
    "    .analytics.time_series_agg(\n",
    "        aggs={'REVENUE_ONLINE':['SUM']},\n",
    "        windows=['-1MM','-2MM','-3MM'],\n",
    "        sliding_interval=\"1D\",\n",
    "        group_by=['CUSTOMER_ID'],\n",
    "        time_col='DATE',\n",
    "        col_formatter=col_formatter\n",
    "    ).drop(['SLIDING_POINT','REVENUE_ONLINE'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38c5c7-0004-4280-887f-a2255a5d8c67",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "in_shop_transaction_features.filter(col('CUSTOMER_ID') == 1).order_by(col('DATE').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff3e4b-b8dc-4eb8-b110-563b587d8142",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": [
    "# Use LLM to generate feature descriptions\n",
    "model = 'mistral-large2'\n",
    "\n",
    "feature_columns = in_shop_transaction_features.drop('CUSTOMER_ID','DATE').columns\n",
    "prompt = f'Return a JSON string with column names as keys and a short business description as values. The columns are: {feature_columns}. Do not wrap the json codes in JSON markers.'\n",
    "llm_response = Complete(model, prompt, stream=False)\n",
    "feature_descriptions_in_shop_transactions = json.loads(llm_response)\n",
    "\n",
    "feature_columns = online_transaction_features.drop('CUSTOMER_ID','DATE').columns\n",
    "prompt = f'Return a JSON string with column names as keys and a short business description as values. The columns are: {feature_columns}. Do not wrap the json codes in JSON markers.'\n",
    "llm_response = Complete(model, prompt, stream=False)\n",
    "feature_descriptions_online_transactions = json.loads(llm_response)\n",
    "\n",
    "st.json(feature_descriptions_in_shop_transactions)\n",
    "st.json(feature_descriptions_online_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d69f38-03c7-435e-8240-7795f752e418",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# Create Feature View\n",
    "in_shop_transaction_fv = FeatureView(\n",
    "    name=\"IN_SHOP_REVENUE_FEATURES\", \n",
    "    entities=[entity],\n",
    "    timestamp_col='DATE',\n",
    "    feature_df=in_shop_transaction_features, \n",
    "    refresh_freq=\"1 minute\",\n",
    "    refresh_mode='AUTO',\n",
    "    desc=\"Features for in-shop transactions\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Add descriptions for some features\n",
    "in_shop_transaction_fv = in_shop_transaction_fv.attach_feature_desc(feature_descriptions_in_shop_transactions)\n",
    "\n",
    "in_shop_transaction_fv = fs.register_feature_view(\n",
    "    feature_view=in_shop_transaction_fv, \n",
    "    version=\"V1\", \n",
    "    block=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Create Feature View\n",
    "online_transaction_fv = FeatureView(\n",
    "    name=\"ONLINE_REVENUE_FEATURES\", \n",
    "    entities=[entity],\n",
    "    timestamp_col='DATE',\n",
    "    feature_df=online_transaction_features, \n",
    "    refresh_freq=\"1 minute\",\n",
    "    refresh_mode='AUTO',\n",
    "    desc=\"Features for online transactions\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Add descriptions for some features\n",
    "online_transaction_fv = online_transaction_fv.attach_feature_desc(feature_descriptions_online_transactions)\n",
    "\n",
    "online_transaction_fv = fs.register_feature_view(\n",
    "    feature_view=online_transaction_fv, \n",
    "    version=\"V1\", \n",
    "    block=True,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade4851-3069-4d9a-af29-8c650c1f1cf5",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "## 4 - Model Training\n",
    "\n",
    "### Generate the Training Dataset with Features from Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ebd1e-0700-4646-a3ff-d871e51d3fc6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "# Target: Predict total revenue per customer for October 2023\n",
    "target_df = session.table('MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n",
    "target_df = (\n",
    "    target_df.filter(col('DATE').between('2024-04-01','2024-04-30'))    # Generate Target Variable for April 2024\n",
    "    .group_by('CUSTOMER_ID')\n",
    "    .agg(F.sum('TRANSACTION_AMOUNT').as_('NEXT_MONTH_REVENUE'))\n",
    "    .with_column('FEATURE_CUTOFF_DATE', F.to_date(lit('2024-03-31')))   # Features until End of March 2024\n",
    ")\n",
    "\n",
    "# Get list of all customers\n",
    "customers_df = session.table('MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n",
    "\n",
    "# Create spine dataframe\n",
    "spine_df = target_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\n",
    "spine_df = spine_df.fillna(0, subset='NEXT_MONTH_REVENUE')\n",
    "spine_df.order_by('CUSTOMER_ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43830e-d929-4fdf-a94a-f6761e09fa6b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "train_dataset = fs.generate_dataset(\n",
    "    name=\"MLOPS_DEMO.FEATURE_STORE.NEXT_MONTH_REVENUE_DATASET\",\n",
    "    spine_df=spine_df,\n",
    "    features=[in_shop_transaction_fv, online_transaction_fv],\n",
    "    version=\"V1\",\n",
    "    spine_timestamp_col=\"FEATURE_CUTOFF_DATE\",\n",
    "    spine_label_cols=[\"NEXT_MONTH_REVENUE\"],\n",
    "    include_feature_view_timestamp_col=False,\n",
    "    desc=\"Initial Training Dataset\"\n",
    ")\n",
    "\n",
    "df = train_dataset.read.to_snowpark_dataframe()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d081cd-ff03-4efa-9134-ae34a29e0190",
   "metadata": {
    "collapsed": false,
    "name": "cell32"
   },
   "source": [
    "### Train an XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbea5f6-0020-4465-add6-8a87653644ac",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df, test_df = df.random_split(weights=[0.9, 0.1], seed=0)\n",
    "\n",
    "print(f'Number of samples in train: {train_df.count()}')\n",
    "print(f'Number of samples in test: {test_df.count()}')\n",
    "\n",
    "feature_columns = train_df.drop(['CUSTOMER_ID','FEATURE_CUTOFF_DATE','NEXT_MONTH_REVENUE']).columns\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    input_cols=feature_columns,\n",
    "    label_cols=['NEXT_MONTH_REVENUE'],\n",
    "    output_cols=['NEXT_MONTH_REVENUE_PREDICTION'],\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_model = xgb_model.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca74d9a-0504-48ae-a6ab-b399a56f2d56",
   "metadata": {
    "collapsed": false,
    "name": "cell24"
   },
   "source": [
    "### Evaluate the XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737ef58-d313-4138-91e7-bb5becf601ea",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict(test_df)\n",
    "# Analyze results\n",
    "mape = mean_absolute_percentage_error(\n",
    "    df=predictions, \n",
    "    y_true_col_names=\"NEXT_MONTH_REVENUE\", \n",
    "    y_pred_col_names=\"NEXT_MONTH_REVENUE_PREDICTION\"\n",
    ")\n",
    "\n",
    "print(f\"Mean absolute percentage error: {mape}\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    # Plot Feature Importance\n",
    "    plot_data = pd.DataFrame(\n",
    "        list(zip(feature_columns, xgb_model.to_xgboost().feature_importances_)), \n",
    "        columns=['FEATURE','IMPORTANCE']\n",
    "    )\n",
    "    \n",
    "    fig = px.bar(\n",
    "        plot_data.sort_values('IMPORTANCE', ascending=False).head(10),\n",
    "        x=\"IMPORTANCE\",\n",
    "        y=\"FEATURE\",\n",
    "        title=\"Feature Importance\",\n",
    "        labels={\"FEATURE\": \"Feature\", \"IMPORTANCE\": \"Importance\"},\n",
    "        orientation=\"h\"\n",
    "    )\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "with col2:\n",
    "    # Plot Predictions\n",
    "    fig = px.scatter(\n",
    "        predictions[\"NEXT_MONTH_REVENUE\", \"NEXT_MONTH_REVENUE_PREDICTION\"].to_pandas().astype(\"float64\"),\n",
    "        x=\"NEXT_MONTH_REVENUE\",\n",
    "        y=\"NEXT_MONTH_REVENUE_PREDICTION\",\n",
    "        title=\"Actual vs Predicted Revenue\",\n",
    "        labels={\n",
    "            \"NEXT_MONTH_REVENUE\": \"Actual Revenue\",\n",
    "            \"NEXT_MONTH_REVENUE_PREDICTION\": \"Predicted Revenue\"\n",
    "        },\n",
    "        trendline=\"ols\",\n",
    "        trendline_color_override=\"red\"\n",
    "    )\n",
    "    st.plotly_chart(fig, use_container_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9967c9-8fb1-4622-b6a6-2615be08a1bc",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "# Save baseline predictions\n",
    "predictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\n",
    "predictions = predictions.with_column('NEXT_MONTH_REVENUE_PREDICTION', F.col('NEXT_MONTH_REVENUE_PREDICTION').cast('number(38,2)'))\n",
    "predictions = predictions.with_column('NEXT_MONTH_REVENUE', F.col('NEXT_MONTH_REVENUE').cast('number(38,2)'))\n",
    "predictions.write.save_as_table('MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad317-43d5-44d8-a66e-18abf67e4a28",
   "metadata": {
    "collapsed": false,
    "name": "cell33"
   },
   "source": [
    "## 5 - Snowflake Model Registry\n",
    "### Setup Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570b13e-2b1d-480f-8a0b-e73488f7e3c6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "# Create reference to model registry\n",
    "reg = Registry(\n",
    "    session=session, \n",
    "    database_name=session.get_current_database(), \n",
    "    schema_name='MODEL_REGISTRY', \n",
    "    options={'enable_monitoring':True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d2ca7-7f49-4692-ad0a-aedcc614d322",
   "metadata": {
    "collapsed": false,
    "name": "cell34"
   },
   "source": [
    "### Register Model in Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18434961-96fe-482a-90c5-40ea4ba8402b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "registered_model = reg.log_model(\n",
    "    xgb_model,\n",
    "    model_name=\"CUSTOMER_REVENUE_MODEL\",\n",
    "    version_name='V1',\n",
    "    metrics={\n",
    "        'MAPE':mape, \n",
    "        'FEATURE_IMPORTANCE':dict(zip(feature_columns, xgb_model.to_xgboost().feature_importances_.astype('float'))),\n",
    "        \"TRAINING_DATA\":{'FEATURE_CUTOFF_DATE':'2024-03-31'}\n",
    "    },\n",
    "    comment=\"Model trained using XGBoost to predict revenue per customer for next month.\",\n",
    "    conda_dependencies=['xgboost'],\n",
    "    sample_input_data=train_df.select(feature_columns).limit(10),\n",
    "    options={\"relax_version\": False, \"enable_explainability\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141827b-939f-4e43-983d-7854519e5ab7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": [
    "# Set this model version as PRODUCTION\n",
    "registered_model.set_alias('PRODUCTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408dc362-8497-4f08-8e34-9c7f7cdf7aed",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "explanations = registered_model.run(test_df, function_name=\"explain\")\n",
    "explanations = explanations.rename({col:col.replace('\"\"\"', '').upper() for col in explanations.columns})\n",
    "explanations = explanations.select([col for col in explanations.columns if '_EXPLANATION' in col])\n",
    "explanations = explanations.to_pandas()\n",
    "\n",
    "import shap\n",
    "shap_exp = shap._explanation.Explanation(explanations.values, feature_names = explanations.columns) # wrapping them into a SHAP recognized object\n",
    "shap.plots.bar(shap_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c55671-9204-4e9c-a815-54fe94821165",
   "metadata": {
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": [
    "trace = session.lineage.trace(\n",
    "    object_name='MLOPS_DEMO.MODEL_REGISTRY.CUSTOMER_REVENUE_MODEL',\n",
    "    object_version='V1',\n",
    "    object_domain='model',\n",
    "    direction='both',\n",
    "    distance=2\n",
    ")\n",
    "trace.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb19d03-afd8-451e-b678-d40daf99facd",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "visualize_lineage(trace.to_pandas(), short_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a85689-b808-48ca-8c7c-cb3c307b59c0",
   "metadata": {
    "collapsed": false,
    "name": "cell35"
   },
   "source": [
    "### Continious Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01cb20-5055-48b1-a78a-9f70dd9d4a84",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": [
    "feature_df = get_feature_df(session, feature_cutoff_date='2024-04-30')\n",
    "feature_df.show()\n",
    "\n",
    "# Predict May values\n",
    "predictions = registered_model.run(feature_df, function_name='PREDICT')\n",
    "predictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\n",
    "predictions = predictions.with_column('NEXT_MONTH_REVENUE_PREDICTION', F.col('NEXT_MONTH_REVENUE_PREDICTION').cast('number(38,2)'))\n",
    "predictions.write.save_as_table(table_name='MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca5a9b-1bbb-44a4-a8c0-05d360c8f5e8",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": [
    "### Create a Model Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f33571a-500e-4f9b-b591-2e177c9c7224",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell51"
   },
   "outputs": [],
   "source": [
    "# Enable once 1.7.3 with bugfix is available\n",
    "# source_config = ModelMonitorSourceConfig(\n",
    "#     source='MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE',\n",
    "#     timestamp_column='FEATURE_CUTOFF_DATE',\n",
    "#     id_columns=['CUSTOMER_ID'],\n",
    "#     prediction_score_columns=['NEXT_MONTH_REVENUE_PREDICTION'],\n",
    "#     actual_score_columns=['NEXT_MONTH_REVENUE'],\n",
    "#     baseline='MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1'\n",
    "# )\n",
    "\n",
    "# monitor_config = ModelMonitorConfig(\n",
    "#     model_version=reg.get_model('CUSTOMER_REVENUE_MODEL').version('PRODUCTION'),\n",
    "#     model_function_name='predict',\n",
    "#     background_compute_warehouse_name='COMPUTE_WH',\n",
    "#     refresh_interval='1 minute',\n",
    "#     aggregation_window='1 day'\n",
    "# )\n",
    "\n",
    "# reg.add_monitor(\n",
    "#     name='MLOPS_DEMO.MODEL_REGISTRY.MM_V1',\n",
    "#     source_config=source_config,\n",
    "#     model_monitor_config=monitor_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398f68d-5182-473d-9079-c42ca4d6bd12",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "cell43"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE MODEL MONITOR MLOPS_DEMO.MODEL_REGISTRY.MM_V1 WITH\n",
    "    MODEL=MLOPS_DEMO.MODEL_REGISTRY.CUSTOMER_REVENUE_MODEL VERSION=V1 FUNCTION=PREDICT\n",
    "    SOURCE=MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1\n",
    "    BASELINE=MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1,\n",
    "    TIMESTAMP_COLUMN='FEATURE_CUTOFF_DATE'\n",
    "    ID_COLUMNS=('CUSTOMER_ID')\n",
    "    PREDICTION_SCORE_COLUMNS=('NEXT_MONTH_REVENUE_PREDICTION')\n",
    "    ACTUAL_SCORE_COLUMNS=('NEXT_MONTH_REVENUE')\n",
    "    WAREHOUSE=COMPUTE_WH\n",
    "    REFRESH_INTERVAL='1 minute'\n",
    "    AGGREGATION_WINDOW='1 day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa1731-d7d4-4aff-bf0a-1e4f2b36a286",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell49"
   },
   "outputs": [],
   "source": [
    "# Add new transactions\n",
    "new_transactions = session.table('MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').filter(col('DATE').between('2024-05-01','2024-05-31'))\n",
    "new_transactions.write.save_as_table(table_name='MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS', mode='append')\n",
    "\n",
    "# Calculate actual values\n",
    "actual_values_df = (\n",
    "    session.table('MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n",
    "    .filter(col('DATE').between('2024-05-01','2024-05-31'))\n",
    "    .group_by(['CUSTOMER_ID'])\n",
    "    .agg(F.sum('TRANSACTION_AMOUNT').as_('TOTAL_REVENUE'))\n",
    "    .with_column('DATE', F.to_date(lit('2024-04-30')))\n",
    ")\n",
    "\n",
    "# Get list of all customers\n",
    "customers_df = session.table('MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n",
    "\n",
    "# Assume 0 revenue for customers without transactions\n",
    "actual_values_df = actual_values_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\n",
    "actual_values_df = actual_values_df.fillna(0,subset='TOTAL_REVENUE')\n",
    "\n",
    "# Update source table from model monitor\n",
    "source_table = session.table('MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1')\n",
    "source_table.update(\n",
    "    condition=(\n",
    "        (source_table['FEATURE_CUTOFF_DATE'] == actual_values_df['DATE']) &\n",
    "        (source_table['CUSTOMER_ID'] == actual_values_df['CUSTOMER_ID'])\n",
    "    ),\n",
    "    assignments={\n",
    "        \"NEXT_MONTH_REVENUE\": actual_values_df['TOTAL_REVENUE'],\n",
    "    },\n",
    "    source=actual_values_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e684bbe-dac6-4f97-8bc4-ca1e6afdd5ee",
   "metadata": {
    "collapsed": false,
    "name": "cell39"
   },
   "source": [
    "## Simulate the rest of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd81b52-4d83-4a22-be1d-39cbe2d9ec12",
   "metadata": {
    "language": "python",
    "name": "cell75"
   },
   "outputs": [],
   "source": [
    "start_date = '2024-06-01'\n",
    "end_date = '2025-01-31'\n",
    "model_version = 'V1'\n",
    "\n",
    "simulate_model_performance(session, start_date, end_date, model_version, generate_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c08704-b178-40b7-888b-7b4b0c75eac7",
   "metadata": {
    "collapsed": false,
    "name": "cell46"
   },
   "source": [
    "## Explore the Model Monitor\n",
    "Navigate to the Model Monitor and observe the `MAPE` and `Difference of means`  for the last months.  \n",
    "\n",
    "You will notice the following:\n",
    "* Declining Model Performance\n",
    "    * :arrow_up_small: MAPE (Mean Average Percentage Error)\n",
    "* Feature Drift\n",
    "    * :arrow_down_small: Difference of means for TOTAL_REVENUE_IN_SHOP_PAST_1_MONTHS (less in shop transaction volume)\n",
    "    * :arrow_up_small: Difference of means for TOTAL_REVENUE_ONLINE_PAST_1_MONTHS (more online transaction volume)\n",
    "\n",
    "If we visualize the monthly revenue distribution, we can see that online revenue grew while in-shop transaction declined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aa7e2-971d-4479-9d17-570bd066cc0b",
   "metadata": {
    "language": "python",
    "name": "cell50"
   },
   "outputs": [],
   "source": [
    "plot_inshop_vs_online_revenue(transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2686a6c-4210-45f7-ab2d-32fb61a9d11f",
   "metadata": {
    "collapsed": false,
    "name": "cell78"
   },
   "source": [
    "## Train a new version\n",
    "Given that the user behavior changed, we'll train a new version of our model with fresh data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02127e0-da97-460b-9b3c-bbf82c6dc7ba",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell90"
   },
   "outputs": [],
   "source": [
    "feature_cutoff_date = '2024-08-31'\n",
    "target_start_date = '2024-09-01'\n",
    "target_end_date = '2024-09-30'\n",
    "model_version = 'V2'\n",
    "\n",
    "train_new_model(session, feature_cutoff_date, target_start_date, target_end_date, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd030b6-2def-46c6-b659-84ad47380963",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "compare_two_models(session,'V1','V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781793a-c85f-40a5-8264-bbb7e9e8b4d7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell61"
   },
   "outputs": [],
   "source": [
    "start_date = '2024-10-01'\n",
    "end_date = '2025-01-31'\n",
    "model_version = 'V2'\n",
    "\n",
    "simulate_model_performance(session, start_date, end_date, model_version, generate_data=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "michael.gorkow@snowflake.com",
   "authorId": "61864603178",
   "authorName": "ADMIN",
   "lastEditTime": 1739144135219,
   "notebookId": "4wrqrw7c5bg7u7nyinl2",
   "sessionId": "dc5d3171-ca58-4d9a-99ed-78f53eb30951"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
