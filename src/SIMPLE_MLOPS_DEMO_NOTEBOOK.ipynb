{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "syqcv57n3oimxw2df73a",
   "authorId": "4993726023393",
   "authorName": "ADMIN",
   "authorEmail": "michael.gorkow@snowflake.com",
   "sessionId": "ae6fe164-d9ee-4715-832d-789c7cc091c5",
   "lastEditTime": 1741373323993
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e090cc90-29da-47e1-bd9b-2f7ac5eedad5",
   "metadata": {
    "name": "SETUP_0",
    "collapsed": false
   },
   "source": "# Use Case: Predicting Future Customer Revenue Using Historical Transaction Data üìä"
  },
  {
   "cell_type": "markdown",
   "id": "3f59214b-4111-4c5d-8a06-1e9f091a4620",
   "metadata": {
    "collapsed": false,
    "name": "SETUP_1"
   },
   "source": "# 1 - Setup Demo üõ†Ô∏è\n* Import required libraries\n* Create a Snowpark session\n\n| Library    | Use |\n| -------- | ------- |\n| `snowflake.snowpark` | Main Python Developer Framework for Snowflake including the DataFrame-API     |\n| `snowflake.ml`    | Snowflake ML specific functions including Feature Store & Model Registry APIs    |\n| `snowflake.cortex`    | Snowflake APIs to access Cortex Services (e.g. LLMs)    |\n| `helper_functions`  | Demo-specific functions that are nort part of any official module    |\n| `notebook_copilot`  | Convenience Functions for Snowflake Notebooks. More details [here](google.com).    |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fbb66-2845-488b-9878-1a90da9edc53",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "SETUP_2"
   },
   "outputs": [],
   "source": "# Helper functions for this demo\nfrom helper_functions.setup_environment import setup_demo\nfrom helper_functions.plotting import plot_inshop_vs_online_revenue, visualize_lineage, compare_two_models\nfrom helper_functions.mlops import train_new_model, simulate_model_performance\nfrom helper_functions.misc import get_function_source_recursively, get_snowsight_url\n\n\n# Import python packages\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport streamlit as st\nfrom streamlit import dataframe as sdf\nimport pandas as pd\nimport json\nimport shap\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Import Snowflake packages\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark.functions import lit, col\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.monitoring.entities.model_monitor_config import ModelMonitorSourceConfig, ModelMonitorConfig\nfrom snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,\n    CreationMode\n)\nfrom snowflake.cortex import complete\n\n# Create a session\nsession = get_active_session()\nsetup_demo(session)"
  },
  {
   "cell_type": "markdown",
   "id": "23627ed5-7e8c-4017-a270-c27af9c06676",
   "metadata": {
    "collapsed": false,
    "name": "EXPLORATION_0"
   },
   "source": "# 2 - Data Exploration & Visualization\n\n* `session.table()` creates a reference to a table\n* `count()`, `order_by()`, `describe()` are dataframe operations\n* `describe()` gives us insights into the transaction amounts (e.g. min, average, max, count).\n\nWe can see that we have roughly 50K transactions across 350 customers."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e15bb06-d009-458a-8627-fe407729cbf6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "EXPLORATION_1"
   },
   "outputs": [],
   "source": "transactions_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n\nprint(f'Number of transactions: {transactions_df.count()}')\nprint(f'Number of customers: {transactions_df.select(\"CUSTOMER_ID\").distinct().count()}')\n\nprint('Transactions Data:')\ntransactions_df.order_by(col('DATE').desc()).show()\n\nprint('Quick Variable Analysis:')\ntransactions_df.describe().order_by('SUMMARY').show()"
  },
  {
   "cell_type": "code",
   "id": "1a9250ac-a958-4fa5-b3f2-35523c3cc525",
   "metadata": {
    "language": "python",
    "name": "cell59"
   },
   "outputs": [],
   "source": "import streamlit as st\nimport json\nfrom snowflake.cortex import complete, CompleteOptions\nimport inspect\nimport re\n\nclass NotebookPilot():\n    def __init__(self, llm='mistral-large2', temperature=0, top_p=0):\n        self.llm = llm\n        self.llm_options = CompleteOptions(\n            temperature=temperature,\n            top_p=top_p\n        )\n        self.dataframe_type_icons = {\n            \"pandas.core.frame.DataFrame\": \"üêº\",\n            \"snowflake.snowpark.dataframe.DataFrame\": \"‚ùÑÔ∏è\",\n            \"snowflake.snowpark.table.Table\": \"‚ùÑÔ∏è\"\n        }\n\n    # Function that extracts the actual Python code returned by mistral\n    def extract_python_code(self, text):\n        # Regular expression pattern to extract content between triple backticks with 'python' as language identifier\n        pattern = r\"```python(.*?)```\"\n        # re.DOTALL allows the dot (.) to match newlines as well\n        match = re.search(pattern, text, re.DOTALL)\n        return match.group(1).strip() if match else \"No JSON code found in the input string.\"\n        \n    # Function to extract JSON code from a string using regex\n    def extract_json_code(self, response_text):\n        pattern = r\"```json(.*?)```\"  # Matches content enclosed in triple backticks labeled 'json'\n        match = re.search(pattern, response_text, re.DOTALL)  # DOTALL ensures newlines are captured\n        return match.group(1).strip() if match else \"No JSON code found in the input string.\"\n\n    def get_dataframes(self):\n        \"\"\"\n        Identifies supported DataFrame variables available in the global scope.\n        Allows the user to select one via Streamlit UI.\n        \"\"\"\n        #\n        global_notebook_variables = inspect.currentframe().f_back.f_back.f_globals\n        # Retrieve DataFrame variables from the global scope\n        available_dataframes = {\n            var_name: (var_obj, self.dataframe_type_icons.get(f\"{type(var_obj).__module__}.{type(var_obj).__name__}\"))\n            #for var_name, var_obj in globals().items()\n            for var_name, var_obj in global_notebook_variables.items()\n            if f\"{type(var_obj).__module__}.{type(var_obj).__name__}\" in self.dataframe_type_icons\n        }\n        return available_dataframes\n\n    def suggest_llm_prompts(self):\n        \"\"\"\n        Generates and displays suggested prompts for analyzing the selected DataFrame.\n        \"\"\"\n        if not hasattr(self, 'suggested_prompts'):\n            st.info('X')\n            #with st.form(\"Prompt Suggestions:\", border=False):\n            if st.button(\"ü§ñ What can I ask?\"):\n                system_prompt = \"\"\"\n                You will later be tasked to create Plotly charts and display them in Streamlit. \n                But first the user wants to understand what kind of questions they can ask.\n                For that, you are provided the first 10 rows of a dataframe.\n    \n                Make sure to suggest prompts that:\n                * Generate analytical insights based on dataframe transformations.\n                * Include instructions about how the plot should look.\n    \n                Return the suggested prompts as JSON using the following format:\n                {'prompt': prompt, 'prompt_explanation': prompt_explanation}\n                Only return the JSON, no other content.\n                \"\"\"\n                user_prompt = f\"\"\"\n                I have the following data:\n                {self.sample_data.to_markdown()}\n                Suggest 3 prompts that I could use.\n                \"\"\"\n                llm_input = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n                llm_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n                #suggested_prompts = json.loads(self.extract_json_code(llm_response))\n                self.suggested_prompts = json.loads(self.extract_json_code(llm_response))\n                \n                #for prompt in suggested_prompts:\n                #    st.code(prompt['prompt'], language=None)\n        else:\n            st.info('Y')\n            for prompt in self.suggested_prompts:\n                st.code(prompt['prompt'], language=None)\n        \n    def ui(self):\n        available_dataframes = self.get_dataframes()\n        # Create a dropdown for selecting a DataFrame\n        dataframe_options = [f\"[{icon}] {name}\" for name, (_, icon) in available_dataframes.items() if not name.startswith(\"_\")]\n        selected_option = st.selectbox(\"Select DataFrame:\", dataframe_options)\n        \n        # Extract the selected DataFrame's name\n        selected_dataframe_name = selected_option.split(\"] \")[1]\n        selected_dataframe, dataframe_type_icon = available_dataframes[selected_dataframe_name]\n        \n        # Convert the icon representation back to the actual DataFrame type\n        dataframe_type = \"snowflake.snowpark.dataframe.DataFrame\" if dataframe_type_icon == \"‚ùÑÔ∏è\" else \"pandas.core.frame.DataFrame\"\n        \n        # Display a sample of the selected DataFrame\n        with st.expander(\"Sample Data:\", expanded=True):\n            try:\n                self.sample_data = selected_dataframe.limit(10).to_pandas() if dataframe_type.startswith(\"snowflake\") else selected_dataframe.head(10)\n                st.dataframe(self.sample_data, use_container_width=True)\n            except Exception as error:\n                st.error(\"Error displaying sample data\")\n                st.error(error)\n\n        if not hasattr(self, 'suggested_prompts'):\n            if st.button(\"ü§ñ What can I ask?\"):\n                self.suggested_prompts = ['Prompt1','Prompt2']\n        else:\n            for prompt in self.suggested_prompts:\n                st.code(prompt, language=None)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb8d6c33-5ec0-4239-9774-8733ec828733",
   "metadata": {
    "language": "python",
    "name": "cell69"
   },
   "outputs": [],
   "source": "notebook_pilot = NotebookPilot()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24290a68-7dc5-4412-bde9-f3ba1ddf22a8",
   "metadata": {
    "language": "python",
    "name": "cell60"
   },
   "outputs": [],
   "source": "notebook_pilot.ui()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bcb7d29-6fd6-4912-91b5-97fe1ccb92df",
   "metadata": {
    "language": "python",
    "name": "cell76"
   },
   "outputs": [],
   "source": "class TEST:\n    def __init__(self):\n        self.a = 'abc{}def'\n\n    def formatted_text(self):\n        return self.a.format(1)\n\nx = TEST()\nx.formatted_text()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c42ccf3-482c-4a05-be32-baad182bb157",
   "metadata": {
    "language": "python",
    "name": "cell51"
   },
   "outputs": [],
   "source": "import streamlit as st\nimport json\nfrom snowflake.cortex import complete, CompleteOptions\nimport inspect\nimport re\nimport pandas as pd\n\nst.session_state['hist'] = []\n\nSYSTEM_PROMPT_TEMPLATE_SUGGEST_PROMPTS = \"\"\"\nYou will later be tasked to create Plotly charts and display them in Streamlit. \nBut first the user wants to understand what kind of questions they can ask.\nFor that, you are provided the first 10 rows of a dataframe.\n\nMake sure to suggest prompts that:\n* Generate analytical insights based on dataframe transformations.\n* Include instructions about how the plot should look.\n\nReturn the suggested prompts as JSON using the following format:\n{prompts:[{'prompt': prompt, 'prompt_explanation': prompt_explanation}]}\nOnly return the JSON, no other content.\n\"\"\"\n\nSYSTEM_PROMPT_TEMPLATE_CREATE_PLOT = \"\"\"\nYou will be tasked to create Plotly charts and display them in Streamlit. \nThe environment is already set up, so only return code to manipulate the given dataframe and afterwards plot it using Plotly and Streamlit.\nThe dataframe is of type {dataframe_type}.\n\nIf the dataframe is pandas, use pandas transformations.\nIf it is Snowpark, use Snowpark Python transformations.\nWhen using Snowpark functions, import them using:\nfrom snowflake.snowpark import functions as F.\nReference functions like:\nF.sum(), F.max(), etc.\n\nBefore creating plots, make sure to order the relevant date/timestamp column if the dataframe has such a column.\nWhen you create new dataframes, make sure that the object starts with an underscore.\nUse df in your code to reference the dataframe. The dataframe has the following columns: {dataframe_columns}\n\nThe first 5 rows of the dataframe look like this:\n{dataframe_sample}\n\"\"\"\n\nclass CortexPilot():\n    def __init__(self, llm='mistral-large2', temperature=0, top_p=0):\n        self.llm = llm\n        self.llm_options = CompleteOptions(\n            temperature=temperature,\n            top_p=top_p\n        )\n        self.dataframe_type_icons = {\n            \"pandas.core.frame.DataFrame\": \"üêº\",\n            \"snowflake.snowpark.dataframe.DataFrame\": \"‚ùÑÔ∏è\",\n            \"snowflake.snowpark.table.Table\": \"‚ùÑÔ∏è\"\n        }\n        st.session_state['suggested_prompts'] = None\n        \n\n    def _extract_python_code(self, text):\n        \"\"\"\n        Function to extract Python code from LLM responses.\n        \"\"\"\n        # Regular expression pattern to extract content between triple backticks with 'python' as language identifier\n        pattern = r\"```python(.*?)```\"\n        # re.DOTALL allows the dot (.) to match newlines as well\n        match = re.search(pattern, text, re.DOTALL)\n        return match.group(1).strip() if match else \"No JSON code found in the input string.\"\n        \n    def _extract_json_code(self, text):\n        \"\"\"\n        Function to extract JSON contents from LLM responses.\n        \"\"\"\n        # sonnet usually returns JSONs directly\n        if self.llm == 'claude-3-5-sonnet':\n            return text.strip()\n        # mistral-large2 wraps it in triple backticks labeled 'json'\n        elif self.llm == 'mistral-large2':\n            pattern = r\"```json(.*?)```\"\n            match = re.search(pattern, text, re.DOTALL)  # DOTALL ensures newlines are captured\n            return match.group(1).strip() if match else \"No JSON code found in the input string.\"\n        elif self.llm == 'llama3.1-70b':\n            st.info(text)\n            return text.strip()\n\n    def _select_dataframe(self):\n        \"\"\"\n        Identifies supported DataFrame variables available in the global scope.\n        Allows the user to select one via Streamlit UI.\n        \"\"\"\n        #\n        global_notebook_variables = inspect.currentframe().f_back.f_back.f_globals\n        # Retrieve DataFrame variables from the global scope\n        available_dataframes = {\n            var_name: (var_obj, self.dataframe_type_icons.get(f\"{type(var_obj).__module__}.{type(var_obj).__name__}\"))\n            #for var_name, var_obj in globals().items()\n            for var_name, var_obj in global_notebook_variables.items()\n            if f\"{type(var_obj).__module__}.{type(var_obj).__name__}\" in self.dataframe_type_icons\n        }\n        \n        # Create a dropdown for selecting a DataFrame\n        dataframe_options = [f\"[{icon}] {name}\" for name, (_, icon) in available_dataframes.items() if not name.startswith(\"_\")]\n        selected_option = st.selectbox(\"Select DataFrame:\", dataframe_options)\n        \n        # Extract the selected DataFrame's name\n        selected_dataframe_name = selected_option.split(\"] \")[1]\n        selected_dataframe, dataframe_type_icon = available_dataframes[selected_dataframe_name]\n        \n        # Convert the icon representation back to the actual DataFrame type\n        dataframe_type = \"snowflake.snowpark.dataframe.DataFrame\" if dataframe_type_icon == \"‚ùÑÔ∏è\" else \"pandas.core.frame.DataFrame\"\n        \n        # Display a sample of the selected DataFrame\n        with st.expander(\"Sample Data:\", expanded=True):\n            try:\n                self.sample_data = selected_dataframe.limit(10).to_pandas() if dataframe_type.startswith(\"snowflake\") else selected_dataframe.head(10)\n                st.dataframe(self.sample_data, use_container_width=True)\n            except Exception as error:\n                st.error(\"Error displaying sample data\")\n                st.error(error)\n        \n        return selected_dataframe\n\n    def _suggest_llm_prompts(self):\n        \"\"\"\n        Generates and displays suggested prompts for analyzing the selected DataFrame.\n        \"\"\"\n        with st.form(\"Prompt Suggestions:\", border=False):\n            if st.form_submit_button(\"ü§ñ What can I ask?\"):\n                user_prompt = f\"\"\"\n                I have the following data:\n                {self.sample_data.to_markdown()}\n                Suggest 3 prompts that I could use.\n                \"\"\"\n                llm_input = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT_TEMPLATE_SUGGEST_PROMPTS}, {\"role\": \"user\", \"content\": user_prompt}]\n                llm_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n                st.session_state['hist'].append(llm_response)\n                suggested_prompts = json.loads(self._extract_json_code(llm_response))\n                st.session_state['suggested_prompts'] = suggested_prompts\n        if st.session_state['suggested_prompts'] is not None:\n            with st.expander(\"Sample Questions:\", expanded=True):\n                for prompt in st.session_state['suggested_prompts']['prompts']:\n                    st.code(prompt['prompt'], language=None)\n\n    def _generate_plotly_code(self, df):\n        \"\"\"\n        Asks the LLM to generate Plotly visualization code based on user input and the selected DataFrame.\n        \"\"\"\n        with st.form(\"Ask LLM\"):\n            user_query = st.text_area(\"What can I help you with?\", height=4*34)\n            \n            if st.form_submit_button(\"ü§ñ Ask Cortex!\"):\n                system_prompt = SYSTEM_PROMPT_TEMPLATE_CREATE_PLOT.format(dataframe_type=type(df).__module__, dataframe_columns=df.columns, dataframe_sample=self.sample_data.to_markdown())\n                llm_input = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_query}]\n                llm_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n                \n                try:\n                    generated_code = self._extract_python_code(llm_response)\n                    exec(generated_code)\n                    with st.expander(\"View Code generated by LLM\"):\n                        st.code(generated_code)\n                except Exception as e:\n                    st.info(\"First LLM response contained invalid code. Retrying with error context...\")\n                    with st.expander(\"View Error and LLM Response\"):\n                        st.error(e)\n                        st.info(llm_response)\n                    \n                    llm_input.append({\"role\": \"assistant\", \"content\": llm_response})\n                    llm_input.append({\"role\": \"user\", \"content\": f\"The generated code resulted in an error: {str(e)}. Please adjust it.\"})\n                    retry_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n                    \n                    try:\n                        retry_code = self._extract_python_code(retry_response)\n                        exec(retry_code)\n                        with st.expander(\"View Adjusted Code\"):\n                            st.code(retry_code)\n                    except Exception as retry_error:\n                        st.error(\"Adjusted code also contains errors.\")\n                        st.error(retry_error)\n\n    def f_cortex_helper_visualize_query(self, df, user_query, verbose=False):\n        \"\"\"\n        Asks the LLM to generate Plotly visualization code based on user input and the selected DataFrame.\n        \"\"\"\n        if isinstance(df, pd.DataFrame):\n            system_prompt = SYSTEM_PROMPT_TEMPLATE_CREATE_PLOT.format(dataframe_type=type(df).__module__, dataframe_columns=df.columns, dataframe_sample=df.head(5).to_markdown())\n        else:\n            system_prompt = SYSTEM_PROMPT_TEMPLATE_CREATE_PLOT.format(dataframe_type=type(df).__module__, dataframe_columns=df.columns, dataframe_sample=df.sample(n=5).to_pandas().to_markdown())\n        \n        llm_input = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_query}]\n        llm_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n        \n        try:\n            generated_code = self._extract_python_code(llm_response)\n            exec(generated_code)\n            with st.expander(\"View Code generated by LLM\"):\n                st.code(generated_code)\n        except Exception as e:\n            if verbose:\n                st.info(\"First LLM response contained invalid code. Retrying with error context...\")\n                with st.expander(\"View Error and LLM Response\"):\n                    st.error(e)\n                    st.info(llm_response)\n            \n            llm_input.append({\"role\": \"assistant\", \"content\": llm_response})\n            llm_input.append({\"role\": \"user\", \"content\": f\"The generated code resulted in an error: {str(e)}. Please adjust it.\"})\n            retry_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n            \n            try:\n                retry_code = self._extract_python_code(retry_response)\n                exec(retry_code)\n                with st.expander(\"View Adjusted Code\"):\n                    st.code(retry_code)\n            except Exception as retry_error:\n                st.error(\"Adjusted code also contains errors.\")\n                st.error(retry_error)\n\n    def ui_plotting(self):\n        \"\"\"\n        Opens a UI to select a dataframe and use Cortex LLMs to automatically visualize data based on user prompts.\n        \"\"\"\n        st.subheader('ü§ñ Ask Cortex about your Data! ', help='Select a dataframe and ask Cortex for generating plots.')\n        dataframe = self._select_dataframe()\n        self._suggest_llm_prompts()\n        self._generate_plotly_code(dataframe)\n    \n    def f_describe_columns(self, df, columns=None, exclude_columns=None):\n        \"\"\"\n        Function to use Cortex LLMs to generate business descriptions for columns of a dataframe.\n        \"\"\"\n        if columns:\n            df = df.select(columns)\n        if exclude_columns:\n            df = df.drop(exclude_columns)\n        prompt = f'Return a JSON string with column names as keys and a short business description as values. The columns are: {df.columns}. Do not wrap the json codes in JSON markers.'\n        llm_response = complete(self.llm, prompt, stream=False)\n        feature_descriptions = json.loads(llm_response)\n        return feature_descriptions\n    \n    def f_explain_column_sql(self, sql, column):\n        \"\"\"\n        Function to use Cortex LLMs to explain the calculation of a column based on the provided SQL.\n        \"\"\"\n        prompt = f'You are given a SQL query. Explain how the column {column} is calculated. The SQL query: {sql}'\n        resp = complete(self.llm, prompt)\n        return resp",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edfb0353-9229-4a14-a7c9-b3acc53edcb7",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": "import re\nimport inspect\nimport streamlit as st\nimport json\nimport pandas as pd\n\nclass CortexPilot:\n    \"\"\"\n    A class to interface with Cortex LLMs for data visualization and analysis.\n    \"\"\"\n    def __init__(self, llm='mistral-large2', temperature=0, top_p=0):\n        self.llm = llm\n        self.llm_options = CompleteOptions(\n            temperature=temperature,\n            top_p=top_p\n        )\n        # Mapping of DataFrame types to display icons\n        self.dataframe_type_icons = {\n            \"pandas.core.frame.DataFrame\": \"üêº\",\n            \"snowflake.snowpark.dataframe.DataFrame\": \"‚ùÑÔ∏è\",\n            \"snowflake.snowpark.table.Table\": \"‚ùÑÔ∏è\"\n        }\n        st.session_state['suggested_prompts'] = None\n\n    def _extract_python_code(self, text):\n        \"\"\"\n        Extract Python code enclosed in triple backticks labeled with 'python'.\n        \"\"\"\n        pattern = r\"```python(.*?)```\"  # Match a markdown code block\n        match = re.search(pattern, text, re.DOTALL)\n        return match.group(1).strip() if match else \"No Python code found in the input string.\"\n\n    def _extract_json_code(self, text):\n        \"\"\"\n        Extract JSON content from LLM responses depending on the LLM type.\n        \"\"\"\n        if self.llm == 'claude-3-5-sonnet':\n            return text.strip()\n        elif self.llm == 'mistral-large2':\n            pattern = r\"```json(.*?)```\"\n            match = re.search(pattern, text, re.DOTALL)\n            return match.group(1).strip() if match else \"No JSON code found in the input string.\"\n        elif self.llm == 'llama3.1-70b':\n            st.info(text)\n            return text.strip()\n\n    def _select_dataframe(self):\n        \"\"\"\n        Identify and let the user select a supported DataFrame from global variables.\n        Displays a sample of the selected DataFrame.\n        \"\"\"\n        # Retrieve global variables from the notebook context\n        global_vars = inspect.currentframe().f_back.f_back.f_globals\n        # Filter for supported DataFrame types using the defined icons\n        available_dataframes = {\n            var_name: (var_obj, self.dataframe_type_icons.get(f\"{type(var_obj).__module__}.{type(var_obj).__name__}\"))\n            for var_name, var_obj in global_vars.items()\n            if f\"{type(var_obj).__module__}.{type(var_obj).__name__}\" in self.dataframe_type_icons\n        }\n        \n        # Prepare options (ignoring private variables)\n        dataframe_options = [f\"[{icon}] {name}\" for name, (_, icon) in available_dataframes.items() if not name.startswith(\"_\")]\n        selected_option = st.selectbox(\"Select DataFrame:\", dataframe_options)\n        selected_name = selected_option.split(\"] \")[1]\n        selected_dataframe, icon = available_dataframes[selected_name]\n        \n        # Determine DataFrame type based on the icon (‚ùÑÔ∏è indicates Snowflake)\n        dataframe_type = \"snowflake.snowpark.dataframe.DataFrame\" if icon == \"‚ùÑÔ∏è\" else \"pandas.core.frame.DataFrame\"\n        \n        # Display a sample of the DataFrame\n        with st.expander(\"Sample Data:\", expanded=True):\n            try:\n                if dataframe_type.startswith(\"snowflake\"):\n                    self.sample_data = selected_dataframe.limit(10).to_pandas()\n                else:\n                    self.sample_data = selected_dataframe.head(10)\n                st.dataframe(self.sample_data, use_container_width=True)\n            except Exception as error:\n                st.error(\"Error displaying sample data\")\n                st.error(error)\n        \n        return selected_dataframe\n\n    def _suggest_llm_prompts(self):\n        \"\"\"\n        Generate and display suggested prompts for analyzing the selected DataFrame.\n        \"\"\"\n        with st.form(\"Prompt Suggestions:\", clear_on_submit=True):\n            if st.form_submit_button(\"ü§ñ What can I ask?\"):\n                user_prompt = f\"\"\"\n                I have the following data:\n                {self.sample_data.to_markdown()}\n                Suggest 3 prompts that I could use.\n                \"\"\"\n                llm_input = [\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT_TEMPLATE_SUGGEST_PROMPTS},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ]\n                llm_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n                st.session_state['hist'].append(llm_response)\n                suggested_prompts = json.loads(self._extract_json_code(llm_response))\n                st.session_state['suggested_prompts'] = suggested_prompts\n\n        if st.session_state['suggested_prompts'] is not None:\n            with st.expander(\"Sample Questions:\", expanded=True):\n                for prompt in st.session_state['suggested_prompts']['prompts']:\n                    st.code(prompt['prompt'], language=None)\n\n    def _execute_llm_generated_code(self, llm_input, llm_response, error_verbose=True):\n        \"\"\"\n        Extract, execute, and display Python code from an LLM response.\n        If execution fails, retry with error context.\n        \"\"\"\n        try:\n            code = self._extract_python_code(llm_response)\n            exec(code)\n            with st.expander(\"View Code generated by LLM\"):\n                st.code(code)\n        except Exception as e:\n            if error_verbose:\n                st.info(\"Initial LLM response contained errors. Retrying with error context...\")\n                with st.expander(\"View Error and LLM Response\"):\n                    st.error(e)\n                    st.info(llm_response)\n            # Retry by appending error context to the conversation\n            llm_input.append({\"role\": \"assistant\", \"content\": llm_response})\n            llm_input.append({\"role\": \"user\", \"content\": f\"The generated code resulted in an error: {str(e)}. Please adjust it.\"})\n            retry_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n            try:\n                retry_code = self._extract_python_code(retry_response)\n                exec(retry_code)\n                with st.expander(\"View Adjusted Code\"):\n                    st.code(retry_code)\n            except Exception as retry_error:\n                st.error(\"Adjusted code also contains errors.\")\n                st.error(retry_error)\n\n    def _generate_plotly_code(self, df):\n        \"\"\"\n        Request Plotly visualization code from the LLM based on user input and execute it.\n        \"\"\"\n        with st.form(\"Ask LLM\", clear_on_submit=True):\n            user_query = st.text_area(\"What can I help you with?\", height=136)\n            if st.form_submit_button(\"ü§ñ Ask Cortex!\"):\n                system_prompt = SYSTEM_PROMPT_TEMPLATE_CREATE_PLOT.format(\n                    dataframe_type=type(df).__module__,\n                    dataframe_columns=df.columns,\n                    dataframe_sample=self.sample_data.to_markdown()\n                )\n                llm_input = [\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_query}\n                ]\n                llm_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n                self._execute_llm_generated_code(llm_input, llm_response)\n\n    def f_cortex_helper_visualize_query(self, df, user_query, verbose=False):\n        \"\"\"\n        Generate and execute Plotly visualization code based on a user query.\n        Handles both pandas and Snowflake DataFrames.\n        \"\"\"\n        # Prepare a sample of the DataFrame for context\n        if isinstance(df, pd.DataFrame):\n            sample = df.head(5).to_markdown()\n        else:\n            sample = df.sample(n=5).to_pandas().to_markdown()\n\n        system_prompt = SYSTEM_PROMPT_TEMPLATE_CREATE_PLOT.format(\n            dataframe_type=type(df).__module__,\n            dataframe_columns=df.columns,\n            dataframe_sample=sample\n        )\n        llm_input = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": user_query}\n        ]\n        llm_response = complete(model=self.llm, prompt=llm_input, options=self.llm_options)\n        self._execute_llm_generated_code(llm_input, llm_response, error_verbose=verbose)\n\n    def ui_plotting(self):\n        \"\"\"\n        Display a UI to select a DataFrame and generate visualizations using Cortex LLMs.\n        \"\"\"\n        st.subheader('ü§ñ Ask Cortex about your Data!', help='Select a dataframe and ask Cortex to generate plots.')\n        dataframe = self._select_dataframe()\n        self._suggest_llm_prompts()\n        self._generate_plotly_code(dataframe)\n\n    def f_describe_columns(self, df, columns=None, exclude_columns=None):\n        \"\"\"\n        Generate business descriptions for DataFrame columns using the LLM.\n        Allows selecting or excluding specific columns.\n        \"\"\"\n        if columns:\n            df = df.select(columns)\n        if exclude_columns:\n            df = df.drop(exclude_columns)\n        prompt = (\n            f\"Return a JSON string with column names as keys and a short business description as values. \"\n            f\"The columns are: {df.columns}. Do not wrap the json codes in JSON markers.\"\n        )\n        llm_response = complete(self.llm, prompt, stream=False)\n        return json.loads(llm_response)\n\n    def f_explain_column_sql(self, sql, column):\n        \"\"\"\n        Explain how a specific column is calculated based on the provided SQL query.\n        \"\"\"\n        prompt = f\"You are given a SQL query. Explain how the column {column} is calculated. The SQL query: {sql}\"\n        return complete(self.llm, prompt)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f72c98a9-63c1-4ce1-b4f6-87b3f64a6e0d",
   "metadata": {
    "language": "python",
    "name": "cell52"
   },
   "outputs": [],
   "source": "cortex_pilot = CortexPilot(llm='claude-3-5-sonnet')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f1ea212-8609-4083-9204-782432973205",
   "metadata": {
    "language": "python",
    "name": "cell73"
   },
   "outputs": [],
   "source": "cortex_pilot.ui_plotting()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df4b8d51-cd05-47df-a1d5-8f2925d3a4fc",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "cortex_pilot.f_cortex_helper_visualize_query(transactions_df, 'What was the total transaction amount per channel? Use a pie chart.')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "894d374a-b912-4600-a0a4-a3f82d2d1b4d",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": "### Plotting Data\nYou can use libraries such as plotly or matplotlib to visualize your data. However, instead of coding the plots manually, we'll leverage GenAI models hosted natively in Snowflake to automatically generate the visualizations.\n\n* For this notebook I developed two convenience functions for you:  \n    * `cortex_helper_ui()` which will open a simple user interface (based on Streamlit) to Snowflake Cortex\n    * `cortex_helper_visualize_query()` receives a Snowpark or Pandas Dataframe and a prompt (in case you already know the dataframe and query)\n\nBoth functions utilize Snowflake's [complete()](https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/api/cortex/snowflake.cortex.complete) function to access LLMs natively hosted in Snowflake.\n\nTry asking the following questions:  \n* ***What was the overall revenue per channel and month? Use a stacked bar plot and use YY-Monthname for the x-axis.***\n* ***What was the total transaction amount per channel? Use a pie chart.***"
  },
  {
   "cell_type": "code",
   "id": "a257574f-0e1e-49ed-9482-b5eb583c07b2",
   "metadata": {
    "language": "python",
    "name": "VIEW_SOURCE_CODE1",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "with st.expander('**Source Code:** cortex_helper_visualize_query()'):\n    st.code(get_function_source_recursively(cortex_helper_visualize_query, max_depth=0), language='python')\n\nwith st.expander('**Source Code:** get_cortex_helper()'):\n    st.code(get_function_source_recursively(cortex_helper_ui, max_depth=1), language='python')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f0b1910-6162-49c2-adf8-3294baa80b89",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": "cortex_helper_ui()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec7ed6e6-70a3-4f8b-a61c-446cf9eaada8",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "When we plot the distribution of ONLINE vs. IN_SHOP revenue, we can see that 75% of our revenue comes from customer transactions that go into our shops.  \nA model trained on this data should recognize that IN_SHOP transactions are the major driver of future customer revenue."
  },
  {
   "cell_type": "code",
   "id": "4ca5466e-03ee-4b0d-a851-6e132ca07f1d",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "cortex_helper_visualize_query(transactions_df, 'What was the total transaction amount per channel? Use a pie chart.')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b0114ca-d50b-4a6b-a33d-ed29b98eb692",
   "metadata": {
    "collapsed": false,
    "name": "cell17"
   },
   "source": "# 3 - Feature Store & Feature Engineering\nThe Snowflake Feature Store enables data scientists and ML engineers to create, manage, and utilize machine learning features within machine learning pipelines.  \nA feature store consists of feature views, which encapsulate Python or SQL pipelines that transform raw data into one or more related features.  \nAll features within a feature view are refreshed simultaneously from the source data.\n\nFeature store objects are implemented as Snowflake objects and all feature store objects are therefore subject to Snowflake access control rules.\n| Feature Store Object    | Snowflake Object |\n| -------- | ------- |\n| `FeatureStore` | Schema     |\n| `Entity`    | Tag    |\n| `FeatureView`  | Dynamic Table or View    |\n| `Feature`  | Column in a Dynamic Table or View    |"
  },
  {
   "cell_type": "markdown",
   "id": "c55c188a-b9d0-4da7-b6ab-7b85c3f3172b",
   "metadata": {
    "collapsed": false,
    "name": "cell26"
   },
   "source": "### Setup the Feature Store\nWe are creating (or referencing if it already exists) a Feature Store that is stored in the schema `FEATURE_STORE`.  \nThe `default_warehouse` will be used to refresh features automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db378dc4-44e5-4b5b-b55c-e1a3e14ec663",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "fs = FeatureStore(\n    session=session, \n    database=session.get_current_database(), \n    name='FEATURE_STORE', \n    default_warehouse='FEATURE_STORE_WH',\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "8c00471e-18fd-47d0-99ae-8b945e7b0bdb",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": "### Create a Feature Store Entity\nFeature views are organized in the feature store according to the entities to which they apply. An entity is a higher-level abstraction that represents the subject matter of a feature.  \nIn our example, the main entity is the `CUSTOMER` and the features we will create will be linked to this entity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286517a-2a63-4565-aa22-63104d78f21b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "# Create a new entity for the Feature Store\n",
    "entity = Entity(name=\"CUSTOMER\", join_keys=[\"CUSTOMER_ID\"], desc='Unique identifier for customers.')\n",
    "fs.register_entity(entity)\n",
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc666d-c1be-484d-b203-4fc26b99cf88",
   "metadata": {
    "collapsed": false,
    "name": "cell31"
   },
   "source": "### Develop Features for Customer Transactions\n\nThe Snowpark Python API provides analytics functions for easily defining many common feature types, such as windowed aggregations.  \nWe will use `analytics.time_series_agg()` to quickly generate revenue for the past 1, 2 and 3 months per customer per channel which we will use as features for our machine learning model.\n\nThe feature dataframe should have the following columns:\n| Column    | Purpose |\n| -------- | ------- |\n| `CUSTOMER_ID` | Identify relevant rows for the calculated feature (Join-Criteria)     |\n| `DATE`    | Allow correct Point-in-Time Joins   |\n| `Feature columns`  | Actual features per entity    |  \n\nYou can find more functions for quickly generating featueres here:  \n[Common feature and query patterns](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/examples)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cdfdb0-10fb-4891-8c06-81c1c7dd0af3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "def col_formatter(input_col, agg, window):\n",
    "    feature_name = f\"{agg.replace('SUM','TOTAL')}_{input_col}_{window.replace('-', 'past_').replace('MM','_MONTHS')}\"\n",
    "    return feature_name\n",
    "\n",
    "in_shop_transaction_features = (\n",
    "    transactions_df.filter(col('TRANSACTION_CHANNEL') == 'IN_SHOP')\n",
    "    .group_by(['CUSTOMER_ID','DATE']).agg(F.sum('TRANSACTION_AMOUNT').as_('REVENUE'))\n",
    "    .rename({'REVENUE':'REVENUE_IN_SHOP'})\n",
    "    .analytics.time_series_agg(\n",
    "        aggs={'REVENUE_IN_SHOP':['SUM']},\n",
    "        windows=['-1MM','-2MM','-3MM'],\n",
    "        sliding_interval=\"1D\",\n",
    "        group_by=['CUSTOMER_ID'],\n",
    "        time_col='DATE',\n",
    "        col_formatter=col_formatter\n",
    "    ).drop(['SLIDING_POINT','REVENUE_IN_SHOP'])\n",
    ")\n",
    "\n",
    "online_transaction_features = (\n",
    "    transactions_df.filter(col('TRANSACTION_CHANNEL') == 'ONLINE')\n",
    "    .group_by(['CUSTOMER_ID','DATE']).agg(F.sum('TRANSACTION_AMOUNT').as_('REVENUE'))\n",
    "    .rename({'REVENUE':'REVENUE_ONLINE'})\n",
    "    .analytics.time_series_agg(\n",
    "        aggs={'REVENUE_ONLINE':['SUM']},\n",
    "        windows=['-1MM','-2MM','-3MM'],\n",
    "        sliding_interval=\"1D\",\n",
    "        group_by=['CUSTOMER_ID'],\n",
    "        time_col='DATE',\n",
    "        col_formatter=col_formatter\n",
    "    ).drop(['SLIDING_POINT','REVENUE_ONLINE'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "id": "452f6ecc-65d2-4b18-9386-ee4dbd649a2a",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "online_transaction_features.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "32994363-a331-4cf0-b156-aa3575776d69",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": "**Feature Descriptions**  \nTo avoid manually writing descriptions, we can use `complete()` to have an LLM generate JSON files containing business descriptions.  \nThis notebook also offers a convenient function `cortex_helper_describe_columns()` based on the complete() function.  \nThese descriptions are stored in the Feature Store alongside our features."
  },
  {
   "cell_type": "code",
   "id": "7f25bcab-5a03-4745-bb60-399b3d4a2947",
   "metadata": {
    "language": "python",
    "name": "cell71"
   },
   "outputs": [],
   "source": "feature_descriptions_in_shop_transactions = cortex_helper_describe_columns(in_shop_transaction_features, exclude_columns=['CUSTOMER_ID','DATE'])\nfeature_descriptions_online_transactions = cortex_helper_describe_columns(online_transaction_features, exclude_columns=['CUSTOMER_ID','DATE'])\n\nst.json(feature_descriptions_in_shop_transactions)\nst.json(feature_descriptions_online_transactions)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39bfb016-65ea-4690-813f-54084735c0a0",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "### Registering Feature Views\nThe `FeatureView` class accepts a Snowpark DataFrame object that contains the feature transformation logic. This allows you to define your features using any method supported by the Snowpark DataFrame API or Snowflake SQL. You can pass the DataFrame directly to the `FeatureView` constructor.  \n\nEach `FeatureView` is associated with the corresponding `Entity`.  \nThe `refresh_freq` parameter determines how often the Feature Store checks for new data and updates the features automatically. For demonstration purposes, this value is set to 1 minute, but it should be adjusted based on the specific use case."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d69f38-03c7-435e-8240-7795f752e418",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "# Create Feature View\nin_shop_transaction_fv = FeatureView(\n    name=\"IN_SHOP_REVENUE_FEATURES\", \n    entities=[entity],\n    timestamp_col='DATE',\n    feature_df=in_shop_transaction_features, \n    refresh_freq=\"1 minute\",\n    refresh_mode='AUTO',\n    desc=\"Features for in-shop transactions\",\n    overwrite=True\n)\n\n# Add descriptions for features\nin_shop_transaction_fv = in_shop_transaction_fv.attach_feature_desc(feature_descriptions_in_shop_transactions)\n\nin_shop_transaction_fv = fs.register_feature_view(\n    feature_view=in_shop_transaction_fv, \n    version=\"V1\", \n    block=True,\n    overwrite=True\n)\n\n# Create Feature View\nonline_transaction_fv = FeatureView(\n    name=\"ONLINE_REVENUE_FEATURES\", \n    entities=[entity],\n    timestamp_col='DATE',\n    feature_df=online_transaction_features, \n    refresh_freq=\"1 minute\",\n    refresh_mode='AUTO',\n    desc=\"Features for online transactions\",\n    overwrite=True\n)\n\n# Add descriptions for features\nonline_transaction_fv = online_transaction_fv.attach_feature_desc(feature_descriptions_online_transactions)\n\nonline_transaction_fv = fs.register_feature_view(\n    feature_view=online_transaction_fv, \n    version=\"V1\", \n    block=True,\n    overwrite=True\n)"
  },
  {
   "cell_type": "markdown",
   "id": "70565143-a46f-4dd6-aaa4-483ea055f37f",
   "metadata": {
    "name": "cell63",
    "collapsed": false
   },
   "source": "### Discovering Features via Feature Store UI\nAfter creating entities and feature views, you can utilize the [Feature Store User Interface](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/feature-store-ui) in Snowsight to locate the objects you need.  \n\nExample of the Feature Store UI:  \n![text](https://github.com/michaelgorkow/snowflake_simple_mlops/blob/main/resources/feature_store.png?raw=true)"
  },
  {
   "cell_type": "code",
   "id": "cc112af7-52a7-423f-8161-d5b27a20f53d",
   "metadata": {
    "language": "python",
    "name": "URL_FEATURE_STORE",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "get_snowsight_url(session, 'Link to Feature Store', '#/features/database/SIMPLE_MLOPS_DEMO/store/FEATURE_STORE/entities')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51761481-2cbe-45d8-9bb7-6529577b0871",
   "metadata": {
    "name": "cell64",
    "collapsed": false
   },
   "source": "### Discovering Features via Feature Store API"
  },
  {
   "cell_type": "code",
   "id": "f2cc6acb-fd8b-4c41-bf39-24e5fe74affb",
   "metadata": {
    "language": "python",
    "name": "cell66"
   },
   "outputs": [],
   "source": "st.markdown('### List of all Feature Views:')\nsdf(fs.list_feature_views())\n\n# Retrieve a Feature View\nretrieved_feature_view = fs.get_feature_view(name='IN_SHOP_REVENUE_FEATURES',version='V1')\n\nst.markdown('### Feature View Columns:')\nsdf(retrieved_feature_view.list_columns())#.show(max_width=200)\n\n# Manually refresh a Feature View\nfs.refresh_feature_view(retrieved_feature_view)\n\nst.markdown('### Feature View Refresh History:')\nsdf(fs.get_refresh_history(retrieved_feature_view).limit(3))\n\n# Explore lineage information\nst.markdown('### Feature View Lineage:')\nst.json(retrieved_feature_view.lineage(direction='both'))\n\n# Use an LLM and the underlying SQL query to explain how the feature is calculated\nsql_explanation = cortex_helper_explain_column_sql(sql=retrieved_feature_view.query, column='TOTAL_REVENUE_IN_SHOP_PAST_1_MONTHS')\nst.markdown(sql_explanation)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ade4851-3069-4d9a-af29-8c650c1f1cf5",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": "# 4 - Model Training\n\n### Generate the Training Dataset with Features from Feature Store\nOur goal is to predict each customer's revenue for the next month based on their transactions from the past three months.  \n\nWe have data from January to April 2024. To define our target variable, `NEXT_MONTH_REVENUE`, we sum all transactions from April for each customer. To ensure proper point-in-time feature retrieval and avoid using future data, we only include transaction features up to **March 31, 2024**, and mark this cutoff with the `FEATURE_CUTOFF_DATE` column.  \n\nThe DataFrame you just created is a **spine DataFrame**, which acts as a reference table linking customers (`CUSTOMER_ID`) with a timestamp (`FEATURE_CUTOFF_DATE`). It ensures consistent and reproducible feature retrieval in a **feature store**.  \n\nUsing this spine, you can generate a training dataset with [`generate_dataset()`](https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/modeling#generating-snowflake-datasets-for-training). The Feature Store will automatically retrieve features as they were valid on that date and add them to the dataset.  \n\nA [Snowflake Dataset](https://docs.snowflake.com/en/developer-guide/snowflake-ml/dataset) is a schema-level object designed for machine learning. It stores data in versions, ensuring immutability, efficient access, and compatibility with ML frameworks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ebd1e-0700-4646-a3ff-d871e51d3fc6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "target_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\ntarget_df = (\n    target_df.filter(col('DATE').between('2024-04-01','2024-04-30'))    # Generate Target Variable for April 2024\n    .group_by('CUSTOMER_ID')\n    .agg(F.sum('TRANSACTION_AMOUNT').as_('NEXT_MONTH_REVENUE'))\n    .with_column('FEATURE_CUTOFF_DATE', F.to_date(lit('2024-03-31')))   # Features until End of March 2024\n)\n\n# Get list of all customers\ncustomers_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n\n# Create spine dataframe\nspine_df = target_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\nspine_df = spine_df.fillna(0, subset='NEXT_MONTH_REVENUE')\nspine_df.order_by('CUSTOMER_ID').show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43830e-d929-4fdf-a94a-f6761e09fa6b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "train_dataset = fs.generate_dataset(\n",
    "    name=\"SIMPLE_MLOPS_DEMO.FEATURE_STORE.NEXT_MONTH_REVENUE_DATASET\",\n",
    "    spine_df=spine_df,\n",
    "    features=[in_shop_transaction_fv, online_transaction_fv],\n",
    "    version=\"V1\",\n",
    "    spine_timestamp_col=\"FEATURE_CUTOFF_DATE\",\n",
    "    spine_label_cols=[\"NEXT_MONTH_REVENUE\"],\n",
    "    include_feature_view_timestamp_col=False,\n",
    "    desc=\"Initial Training Dataset\"\n",
    ")\n",
    "\n",
    "df = train_dataset.read.to_snowpark_dataframe()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d081cd-ff03-4efa-9134-ae34a29e0190",
   "metadata": {
    "collapsed": false,
    "name": "cell32"
   },
   "source": "### Train an XGBoost Model\nWe randomly split the data, allocating **90% for training** and **10% for validation**.  \nThe training data is then used to train an **XGBoost regression model** with the `XGBRegressor` from the **Snowflake ML library**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbea5f6-0020-4465-add6-8a87653644ac",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df, test_df = df.random_split(weights=[0.9, 0.1], seed=0)\n",
    "\n",
    "print(f'Number of samples in train: {train_df.count()}')\n",
    "print(f'Number of samples in test: {test_df.count()}')\n",
    "\n",
    "feature_columns = train_df.drop(['CUSTOMER_ID','FEATURE_CUTOFF_DATE','NEXT_MONTH_REVENUE']).columns\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    input_cols=feature_columns,\n",
    "    label_cols=['NEXT_MONTH_REVENUE'],\n",
    "    output_cols=['NEXT_MONTH_REVENUE_PREDICTION'],\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_model = xgb_model.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca74d9a-0504-48ae-a6ab-b399a56f2d56",
   "metadata": {
    "collapsed": false,
    "name": "cell24"
   },
   "source": "### Evaluate the XGBoost Model\nYou can immediately use the model‚Äôs `predict()` function to generate predictions on the test data.  \nSnowflake ML also provides built-in metric functions, such as **Mean Absolute Percentage Error (MAPE)**, for evaluating model performance.  \n\nAdditionally, you can convert the model back to its native open-source format using `xgb_model.to_xgboost()`.  \nThis allows you to access feature importance values, which we visualize to better understand what influences the model‚Äôs predictions.  \n\nAs shown in the plot, the model correctly identified that **IN_SHOP transactions** are the primary driver of the target variable, `NEXT_MONTH_REVENUE`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737ef58-d313-4138-91e7-bb5becf601ea",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict(test_df)\n",
    "# Analyze results\n",
    "mape = mean_absolute_percentage_error(\n",
    "    df=predictions, \n",
    "    y_true_col_names=\"NEXT_MONTH_REVENUE\", \n",
    "    y_pred_col_names=\"NEXT_MONTH_REVENUE_PREDICTION\"\n",
    ")\n",
    "\n",
    "print(f\"Mean absolute percentage error: {mape}\")\n",
    "\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    # Plot Feature Importance\n",
    "    plot_data = pd.DataFrame(\n",
    "        list(zip(feature_columns, xgb_model.to_xgboost().feature_importances_)), \n",
    "        columns=['FEATURE','IMPORTANCE']\n",
    "    )\n",
    "    \n",
    "    fig = px.bar(\n",
    "        plot_data.sort_values('IMPORTANCE', ascending=False).head(10),\n",
    "        x=\"IMPORTANCE\",\n",
    "        y=\"FEATURE\",\n",
    "        title=\"Feature Importance\",\n",
    "        labels={\"FEATURE\": \"Feature\", \"IMPORTANCE\": \"Importance\"},\n",
    "        orientation=\"h\"\n",
    "    )\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "with col2:\n",
    "    # Plot Predictions\n",
    "    fig = px.scatter(\n",
    "        predictions[\"NEXT_MONTH_REVENUE\", \"NEXT_MONTH_REVENUE_PREDICTION\"].to_pandas().astype(\"float64\"),\n",
    "        x=\"NEXT_MONTH_REVENUE\",\n",
    "        y=\"NEXT_MONTH_REVENUE_PREDICTION\",\n",
    "        title=\"Actual vs Predicted Revenue\",\n",
    "        labels={\n",
    "            \"NEXT_MONTH_REVENUE\": \"Actual Revenue\",\n",
    "            \"NEXT_MONTH_REVENUE_PREDICTION\": \"Predicted Revenue\"\n",
    "        },\n",
    "        trendline=\"ols\",\n",
    "        trendline_color_override=\"red\"\n",
    "    )\n",
    "    st.plotly_chart(fig, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad317-43d5-44d8-a66e-18abf67e4a28",
   "metadata": {
    "collapsed": false,
    "name": "cell33"
   },
   "source": "# 5 - Snowflake Model Registry\n### Setup Model Registry\nAfter training a model, the first step in operationalizing it and running inference in Snowflake is to **log the model in the Snowflake Model Registry**.  \n\nThe **Model Registry** allows you to securely manage models and their metadata in Snowflake, regardless of their origin or type, while also simplifying inference.  \nIt stores machine learning models as **first-class schema-level objects** within Snowflake.  \n\nBy setting `enable_monitoring` to True, the **Model Registry** can also be used for model monitoring, which we will implement in the next step.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570b13e-2b1d-480f-8a0b-e73488f7e3c6",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "# Create reference to model registry\n",
    "reg = Registry(\n",
    "    session=session, \n",
    "    database_name=session.get_current_database(), \n",
    "    schema_name='MODEL_REGISTRY', \n",
    "    options={'enable_monitoring':True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d2ca7-7f49-4692-ad0a-aedcc614d322",
   "metadata": {
    "collapsed": false,
    "name": "cell34"
   },
   "source": "### Register Model in Model Registry\nThe Model Registry's `log_model()` function takes the model object and logs it to the registry.  \nThe **name** and **version** help ensure the correct model is retrieved for inference.  \n\nAdditionally, we log relevant metrics/information, including:  \n- **MAPE (Mean Absolute Percentage Error)** calculated on the test dataset  \n- **Feature importance values**  \n- **FEATURE_CUTOFF_DATE**   \n\nWe also specify the following parameters:  \n\n| Variable               | Description  |\n|------------------------|-------------|\n| `sample_input_data`    | Sample input data used to infer model signatures, serve as background data for explanations, and capture data lineage. |\n| `conda_dependencies`   | Specifies model dependencies, such as the XGBoost library. |\n| `relax_version`        | Enforces specific dependency versions for compatibility and reproducibility. |\n| `enable_explainability` | Adds an explainability function to the model, allowing us to better understand its predictions using SHAP values. |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18434961-96fe-482a-90c5-40ea4ba8402b",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": "registered_model = reg.log_model(\n    xgb_model,\n    model_name=\"CUSTOMER_REVENUE_MODEL\",\n    version_name='V1',\n    metrics={\n        'MAPE':mape, \n        'FEATURE_IMPORTANCE':dict(zip(feature_columns, xgb_model.to_xgboost().feature_importances_.astype('float'))),\n        \"TRAINING_DATA\":{'FEATURE_CUTOFF_DATE':'2024-03-31'}\n    },\n    comment=\"Model trained using XGBoost to predict revenue per customer for next month.\",\n    conda_dependencies=['xgboost'],\n    sample_input_data=train_df.select(feature_columns).limit(100),\n    options={\"relax_version\": False, \"enable_explainability\": True}\n)"
  },
  {
   "cell_type": "markdown",
   "id": "e6492927-bcca-4f8d-93c2-bb7fe490d466",
   "metadata": {
    "name": "cell41",
    "collapsed": false
   },
   "source": "### Operationalize Models\nThere are multiple ways to operationalize models using Snowflake's Model Registry.  \nOne simple approach is to use **aliases** for the model. By assigning the alias **`PRODUCTION`**, any inference pipeline referencing this alias will automatically use the correct production-ready model.  \n\nWhen a new model version is trained and ready for deployment, you can seamlessly update production by **removing the alias from the current model** and **assigning it to the new model**.  \nThis method ensures that existing ML pipelines remain unchanged, reducing the need for manual updates while maintaining a smooth model deployment process."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141827b-939f-4e43-983d-7854519e5ab7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": "registered_model.set_alias('PRODUCTION')"
  },
  {
   "cell_type": "code",
   "id": "357c8eac-0ad9-4ed6-9f7d-c31c45b91cc8",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": "# Retrieve the production model in your pipelines\nproduction_model = reg.get_model('CUSTOMER_REVENUE_MODEL').version('PRODUCTION')\nproduction_model.show_metrics()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0ebd58e-640b-4442-8155-c64f8bad43d5",
   "metadata": {
    "name": "cell62",
    "collapsed": false
   },
   "source": "### Explore Models in the Model Registry UI\nThe [Model Registry UI]((https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/snowsight-ui)) in Snowsight enables you to discover and explore machine learning models available for use in Snowflake.  \n\nTo view a model's details, click on its corresponding row in the Models list.  \nThe details page provides essential information, including the model's description, tags, and versions.\n\nExample of the Model Registry UI:  \n![text](https://github.com/michaelgorkow/snowflake_simple_mlops/blob/main/resources/model_registry_ui.png?raw=true)"
  },
  {
   "cell_type": "code",
   "id": "dc08a60d-bdc8-45e8-9558-8b999d94e2b0",
   "metadata": {
    "language": "python",
    "name": "URL_MODEL_REGISTRY",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "get_snowsight_url(session, 'Link to Model Registry', '#/models')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0ca6642c-5fe5-4361-b970-2955611795cb",
   "metadata": {
    "name": "cell48",
    "collapsed": false
   },
   "source": "### Model explainability\nSince we enabled model_explainibility when registering the model, we can now call the explain function of the model that was auto-generated.  \nThe standard SHAP library is then used to visualise the SHAP values.\n\n**What are SHAP (SHapley Additive exPlanations) values?**  \n* SHAP (SHapley Additive exPlanations) values measure how much each feature contributes to the prediction.\n* The x-axis represents the mean absolute SHAP value, indicating the magnitude of a feature's impact on the model's predictions.\n* The y-axis lists the feature names.\n* Longer bars mean the feature has a greater impact on predictions.\n\n**Interpretation**  \nWhat is the meaning of the values?  \nFor the General Feature Importance on the left, think about it like this:  \nOn average, the feature (e.g. TOTAL_REVENUE_IN_SHOP_PAST_1_MONTH) affects the model‚Äôs output by approximately X units of revenue.  \n\nOn a more general note, in our case IN_SHOP revenue consistently ranks higher than online revenue, implying that the model sees in-shop purchases as a stronger signal for future revenue prediction.\n\nOn the right side we are plotting the local Feature Importance for single customers.\nThat means it can happen that certain customers have a strong record of ONLINE transactions and therefore are much more influenced by in-shop features than online features."
  },
  {
   "cell_type": "code",
   "id": "96c32cb8-a183-4f21-809b-941e9ac51842",
   "metadata": {
    "language": "python",
    "name": "cell65"
   },
   "outputs": [],
   "source": "# Calculate Shap values\nexplanations = production_model.run(test_df, function_name=\"explain\")\nexplanations = explanations.rename({col:col.replace('\"\"\"', '').upper() for col in explanations.columns})\n\nshap_columns = [col for col in explanations.columns if '_EXPLANATION' in col]\n\nexplanations = explanations.select('CUSTOMER_ID', *shap_columns)\nexplanations = explanations.to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4e92b79-f35f-4dcd-97e7-6f00a62d4e7a",
   "metadata": {
    "language": "python",
    "name": "cell67"
   },
   "outputs": [],
   "source": "# Plot Shap values\nselected_customer = st.selectbox('Select Customer:', options=explanations[['CUSTOMER_ID']].sort_values(by='CUSTOMER_ID'))\nselected_explanation = explanations[explanations['CUSTOMER_ID'] == selected_customer]\n\ncol1, col2 = st.columns(2)\nwith col1:\n    st.markdown(f'### Global Feature Importance:')\n    shap_exp = shap._explanation.Explanation(selected_explanation[shap_columns].values, feature_names = [col.replace('_EXPLANATION','') for col in shap_columns]) # wrapping them into a SHAP recognized object\n    shap.plots.bar(shap_exp)\nwith col2:\n    st.markdown(f'### Local Feature Importance for CUSTOMER_ID {int(selected_customer)}:')\n    shap.plots.bar(shap_exp[0])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e0a85689-b808-48ca-8c7c-cb3c307b59c0",
   "metadata": {
    "collapsed": false,
    "name": "cell35"
   },
   "source": "### Continious Model Monitoring\nModel behavior can change over time due to factors such as **input drift, stale training assumptions, data pipeline issues, hardware and software updates**.\n\n**ML Observability** enables you to monitor the quality of models registered in the **Snowflake Model Registry** across multiple dimensions, including **performance, drift, and volume**.  \n\nTo measure drift for model monitoring, we use two tables:  \n\n| Table      | Description  |\n|------------|-------------|\n| `BASELINE` | Contains a snapshot of data similar to `SOURCE`. It is used as a reference for comparing future feature values and predictions. |\n| `SOURCE`   | Stores future predictions and feature values for monitoring. |"
  },
  {
   "cell_type": "code",
   "id": "10552343-1009-4e62-989d-6afbbef0e1cb",
   "metadata": {
    "language": "python",
    "name": "cell44",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Save baseline predictions\npredictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\npredictions = predictions.with_column('NEXT_MONTH_REVENUE_PREDICTION', F.col('NEXT_MONTH_REVENUE_PREDICTION').cast('number(38,2)'))\npredictions = predictions.with_column('NEXT_MONTH_REVENUE', F.col('NEXT_MONTH_REVENUE').cast('number(38,2)'))\npredictions.write.save_as_table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1', mode='overwrite')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d9d3f43e-0b16-4e68-a0f8-79d9be5edd1c",
   "metadata": {
    "name": "cell54",
    "collapsed": false
   },
   "source": "### Creating predictions for the next month\nWe use the trained model on our **April data** to predict each customer's **revenue for May**.  \nThe `get_feature_df()` function is a helper utility that constructs the **spine DataFrame** and retrieves the correct **point-in-time features** based on the `FEATURE_CUTOFF_DATE`.  \nThe predictions are then stored in the `SOURCE` table, which we will link to the **model monitor** for tracking and evaluation."
  },
  {
   "cell_type": "code",
   "id": "7c73ee7d-61ec-4842-b435-fd975061d2ea",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "def build_feature_df(session, feature_cutoff_date, feature_views):\n    # Initialize the Feature Store.\n    fs = FeatureStore(\n        session=session, \n        database=session.get_current_database(), \n        name='FEATURE_STORE', \n        default_warehouse=session.get_current_warehouse(),\n        creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n    )\n    \n    # Retrieve all feature views (version 'V1') from the Feature Store.\n    fvs = [fs.get_feature_view(name=feature_view_name,version=feature_view_version) for feature_view_name, feature_view_version in feature_views.items()]\n    \n    # Create a base (spine) DataFrame containing distinct CUSTOMER_IDs and the feature cutoff date.\n    feature_df = session.table(f'{session.get_current_database()}.RETAIL_DATA.CUSTOMERS') \\\n                        .select('CUSTOMER_ID') \\\n                        .distinct() \\\n                        .with_column('FEATURE_CUTOFF_DATE', F.to_date(lit(feature_cutoff_date)))\n    \n    # Retrieve feature values from the Feature Store for the specified cutoff date.\n    feature_df = fs.retrieve_feature_values(\n        spine_df=feature_df,\n        features=fvs,\n        spine_timestamp_col=\"FEATURE_CUTOFF_DATE\"\n    )\n    \n    # Add a placeholder column for NEXT_MONTH_REVENUE\n    feature_df = feature_df.with_column('NEXT_MONTH_REVENUE', lit(None).cast('number(38,2)'))\n    \n    return feature_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01cb20-5055-48b1-a78a-9f70dd9d4a84",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": "feature_df = build_feature_df(\n    session, \n    feature_cutoff_date='2024-04-30', \n    feature_views={'IN_SHOP_REVENUE_FEATURES':'V1', 'ONLINE_REVENUE_FEATURES':'V1'}\n)\n\nprint('Feature DataFrame:')\nfeature_df.show()\n\n# Predict May values\npredictions = production_model.run(feature_df, function_name='PREDICT')\npredictions = predictions.with_column('FEATURE_CUTOFF_DATE', F.col('FEATURE_CUTOFF_DATE').cast('timestamp'))\npredictions = predictions.with_column('NEXT_MONTH_REVENUE_PREDICTION', F.col('NEXT_MONTH_REVENUE_PREDICTION').cast('number(38,2)'))\npredictions.write.save_as_table(table_name='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1', mode='overwrite')\n\n# Predictions\nprint('Predictions [column=NEXT_MONTH_REVENUE_PREDICTION]:')\nsession.table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1').show()"
  },
  {
   "cell_type": "markdown",
   "id": "b9ca5a9b-1bbb-44a4-a8c0-05d360c8f5e8",
   "metadata": {
    "collapsed": false,
    "name": "cell6"
   },
   "source": "### Creating a Model Monitor  \n\nWe are setting up a **model monitor** to continuously calculate and track model performance and drift over time.  \n\nThese calculations are based on the **`BASELINE`** and **`SOURCE`** tables created earlier.  \nEach model requires its own dedicated **model monitor** to ensure accurate tracking and evaluation."
  },
  {
   "cell_type": "code",
   "id": "d2614e2f-4856-4e5b-a5a5-d78b4f3eeaea",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "# Enable once 1.7.3 with bugfix is available\nsource_config = ModelMonitorSourceConfig(\n    source='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1',\n    timestamp_column='FEATURE_CUTOFF_DATE',\n    id_columns=['CUSTOMER_ID'],\n    prediction_score_columns=['NEXT_MONTH_REVENUE_PREDICTION'],\n    actual_score_columns=['NEXT_MONTH_REVENUE'],\n    baseline='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_REVENUE_BASELINE_V1'\n)\n\nmonitor_config = ModelMonitorConfig(\n    model_version=reg.get_model('CUSTOMER_REVENUE_MODEL').version('PRODUCTION'),\n    model_function_name='predict',\n    background_compute_warehouse_name='COMPUTE_WH',\n    refresh_interval='1 minute',\n    aggregation_window='1 day'\n)\n\nmodel_monitor = reg.add_monitor(\n    name='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_V1',\n    source_config=source_config,\n    model_monitor_config=monitor_config\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f64990a-9de7-42f5-af03-525cf41a057a",
   "metadata": {
    "name": "cell53",
    "collapsed": false
   },
   "source": "### Simulating the next month of Customer Transactions\nOur model has predicted each customer's **revenue for May 2024** and stored the results in the **`SOURCE`** table.  \nNext, we simulate the actual transactions for May and update the **true revenue values** for each customer in the **`SOURCE`** table.  \nWhen the **model monitor** refreshes, it will use these updated values to calculate various **model performance metrics**, including the MAPE."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa1731-d7d4-4aff-bf0a-1e4f2b36a286",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell49"
   },
   "outputs": [],
   "source": "# Add new transactions (created as part of the initial demo setup)\nnew_transactions = session.table('SIMPLE_MLOPS_DEMO._DATA_GENERATION._TRANSACTIONS').filter(col('DATE').between('2024-05-01','2024-05-31'))\nnew_transactions.write.save_as_table(table_name='SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS', mode='append')\n\n# Calculate actual values\nactual_values_df = (\n    session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.TRANSACTIONS')\n    .filter(col('DATE').between('2024-05-01','2024-05-31'))\n    .group_by(['CUSTOMER_ID'])\n    .agg(F.sum('TRANSACTION_AMOUNT').as_('TOTAL_REVENUE'))\n    .with_column('DATE', F.to_date(lit('2024-04-30')))\n)\n\n# Get list of all customers\ncustomers_df = session.table('SIMPLE_MLOPS_DEMO.RETAIL_DATA.CUSTOMERS').select('CUSTOMER_ID').distinct()\n\n# Assume 0 revenue for customers without transactions\nactual_values_df = actual_values_df.join(customers_df, on=['CUSTOMER_ID'], how='outer')\nactual_values_df = actual_values_df.fillna(0,subset='TOTAL_REVENUE')\n\n# Update source table from model monitor\nsource_table = session.table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V1')\nsource_table.update(\n    condition=(\n        (source_table['FEATURE_CUTOFF_DATE'] == actual_values_df['DATE']) &\n        (source_table['CUSTOMER_ID'] == actual_values_df['CUSTOMER_ID'])\n    ),\n    assignments={\n        \"NEXT_MONTH_REVENUE\": actual_values_df['TOTAL_REVENUE'],\n    },\n    source=actual_values_df\n)"
  },
  {
   "cell_type": "markdown",
   "id": "4e684bbe-dac6-4f97-8bc4-ca1e6afdd5ee",
   "metadata": {
    "collapsed": false,
    "name": "cell39"
   },
   "source": "## Simulate Customer Transactions until 2025-01-31\nFor convenience, I encapsulated all the logic for simulating future months into the helper function `simulate_model_performance()`.  \nWe use this function to simulate the model's behavior until January 2025."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd81b52-4d83-4a22-be1d-39cbe2d9ec12",
   "metadata": {
    "language": "python",
    "name": "cell75",
    "collapsed": false
   },
   "outputs": [],
   "source": "start_date = '2024-06-01'\nend_date = '2025-01-31'\nmodel_version = 'PRODUCTION'\n\nsimulate_model_performance(session, start_date, end_date, model_version, generate_data=True)"
  },
  {
   "cell_type": "markdown",
   "id": "87c08704-b178-40b7-888b-7b4b0c75eac7",
   "metadata": {
    "collapsed": false,
    "name": "cell46"
   },
   "source": "## Explore the Model Monitor\nNavigate to the Model Monitor and observe the `MAPE` and `Jensen-Shannon Distance`  for the last months.  \n\nYou will notice the following:\n* Declining Model Performance\n    * :arrow_up_small: MAPE (Mean Average Percentage Error)\n* Feature Drift\n    * :arrow_down_small: Difference of means for TOTAL_REVENUE_IN_SHOP_PAST_1_MONTHS (less in shop transaction volume)\n    * :arrow_up_small: Difference of means for TOTAL_REVENUE_ONLINE_PAST_1_MONTHS (more online transaction volume)\n\nWhy is that?  \nWell, if we visualize the monthly revenue distribution, we can see that online revenue grew while in-shop transaction declined.\n\nInstead of using the builtin UI, you can also query model monitor metrics using the following table functions and build your own visuals:\n* [MODEL_MONITOR_PERFORMANCE_METRIC](https://docs.snowflake.com/en/sql-reference/functions/model-monitor-performance-metric)\n* [MODEL_MONITOR_DRIFT_METRIC](https://docs.snowflake.com/en/sql-reference/functions/model-monitor-drift-metric)\n* [MODEL_MONITOR_STAT_METRIC](https://docs.snowflake.com/en/sql-reference/functions/model-monitor-stat-metric)"
  },
  {
   "cell_type": "code",
   "id": "15cd70e5-503c-4952-89b8-f80a1a33434e",
   "metadata": {
    "language": "python",
    "name": "URL_MODEL_MONITOR",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "get_snowsight_url(session, 'Link to Model Monitor', '#/data/databases/SIMPLE_MLOPS_DEMO/schemas/MODEL_REGISTRY/model/CUSTOMER_REVENUE_MODEL/version/V1/monitors/MM_V1/dashboard')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35973caf-79c3-4378-a258-f7ace897cba9",
   "metadata": {
    "language": "python",
    "name": "cell58",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "with st.expander('**Need help deciding for the right metric?**', expanded=False):\n    text = \"\"\"### **Overview of Feature Drift Metrics:**\n\n|                      | **Jensen-Shannon Distance** | **Wasserstein Distance** | **Difference of Means** |\n|-----------------------------|----------------|--------------------------|-------------------------|\n| **What It Measures**        | Difference in probability distributions | Amount of movement needed to align two distributions | Simple difference between means of two distributions |\n| **Intuition**               | Measures how **different** two distributions are (based on KL divergence, but smoothed and symmetric). | Measures the **work needed** to \"move\" one distribution to match the other. | Measures the shift in the **central tendency** of the feature values. |\n| **Range**                   | 0 to 1 (bounded) | 0 to ‚àû (can grow indefinitely) | -‚àû to ‚àû (unbounded) |\n| **Interpretability**        | 0 = identical, 1 = completely different | Larger values mean greater distribution shift | Positive = mean has increased, Negative = mean has decreased |\n| **Computational Complexity** | Faster, works well with discrete values | Slower, requires solving an optimization problem | Very fast (simple arithmetic) |\n| **Small shifts in values**  | May not detect it well if probability distributions overlap a lot. | Captures even small shifts because it looks at the actual distance between values. | Only detects shifts in the mean, not overall distribution changes. |\n| **Major changes in shape**  | Captures well if distributions change significantly. | Captures well if mass shifts significantly. | ‚ùå No, only captures mean changes. |\n| **Outliers or extreme shifts** | May be less sensitive if distributions overlap in many places. | More sensitive because it considers the actual movement of values. | Very sensitive to outliers (mean can shift significantly). |\n| **Best for categorical distributions** (e.g., customer segments) | ‚úÖ Yes | ‚ùå No | ‚ùå No |\n| **Best for continuous features** (e.g., age, income) | ‚ùå No | ‚úÖ Yes | ‚úÖ Yes |\n| **Best for detecting gradual numerical shifts** | ‚ùå No | ‚úÖ Yes | ‚úÖ Yes, but only if the mean is shifting. |\n| **Best for interpretable (bounded 0-1) metric** | ‚úÖ Yes | ‚ùå No | ‚ùå No |\"\"\"\n    st.markdown(text)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ace518d-69fb-4eeb-ba99-ef17a5707655",
   "metadata": {
    "name": "cell36",
    "collapsed": false
   },
   "source": "### Query the model monitor"
  },
  {
   "cell_type": "code",
   "id": "5ae94833-4b55-463d-877c-8425010dae1f",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "session.table_function(\n    \"MODEL_MONITOR_DRIFT_METRIC\",\n    lit('MM_V1'),\n    lit('WASSERSTEIN'),\n    lit('TOTAL_REVENUE_IN_SHOP_PAST_1_MONTHS'),\n    lit('1 day'),\n    lit('2024-01-01'),\n    lit('2024-12-31')\n).show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "305ffbe8-5efe-433f-819f-d2fabaaeabf5",
   "metadata": {
    "language": "python",
    "name": "cell47"
   },
   "outputs": [],
   "source": "import ast\nclass ModelRegistryHelper:\n    def __init__(self, session, registry):\n        self.session = session\n        self.registry = registry\n\n        # Allowed model performance metrics\n        self.ALLOWED_MODEL_TYPES_METRICS = {\n            'TASK.TABULAR_BINARY_CLASSIFICATION': ['PRECISION', 'F1_SCORE', 'CLASSIFICATION_ACCURACY', 'ROC_AUC', 'RECALL'],\n            'Task.TABULAR_REGRESSION': ['MSE', 'RMSE', 'MAPE', 'MAE']\n        }\n        \n        # Allowed drift metrics\n        self.ALLOWED_DRIFT_METRICS = ['JENSEN_SHANNON', 'WASSERSTEIN', 'DIFFERENCE_OF_MEANS']\n\n    # Function to retrieve performance metrics for models from the model registry\n    def get_model_performance_metrics(self, models, metrics, start_date, end_date, aggregation):\n        \"\"\"\n        Fetches model performance metrics from the model registry for given models and metrics.\n        \n        Args:\n            session: Active database session.\n            models (list): List of models to fetch metrics for.\n            metrics (list): List of performance metrics to retrieve.\n            start_date (str): Start date for metrics retrieval.\n            end_date (str): End date for metrics retrieval.\n            aggregation (str): Aggregation window (e.g., '1 day').\n        \n        Returns:\n            DataFrame: Aggregated model performance metrics.\n        \"\"\"\n        \n        all_models_metrics = []\n        \n        # Iterate through each model\n        for model in models:\n            model_name = model.model_name\n            model_version_name = model.version_name\n            monitor_name = str(self.registry.get_monitor(model_version=model).name)\n            \n            model_metrics_dfs = []\n            \n            # Fetch each metric for the model\n            for metric in metrics:\n                df_metric = (\n                    self.session.table_function(\n                        \"MODEL_MONITOR_PERFORMANCE_METRIC\",\n                        lit(monitor_name), lit(metric), lit(aggregation), lit(start_date), lit(end_date)\n                    )\n                    .with_column('MODEL_NAME', lit(model_name))\n                    .with_column('MODEL_VERSION_NAME', lit(model_version_name))\n                    .with_column('MODEL_MONITOR_NAME', lit(monitor_name))\n                    .rename({'METRIC_VALUE': metric})\n                    .select(['MODEL_NAME', 'MODEL_VERSION_NAME', 'MODEL_MONITOR_NAME', 'EVENT_TIMESTAMP', metric])\n                )\n                model_metrics_dfs.append(df_metric)\n            \n            # Combine metrics for the model\n            model_metrics_df = model_metrics_dfs[0]\n            for df in model_metrics_dfs[1:]:\n                model_metrics_df = model_metrics_df.join(df, on=['MODEL_NAME', 'MODEL_VERSION_NAME', 'MODEL_MONITOR_NAME', 'EVENT_TIMESTAMP'], how='inner')\n            \n            all_models_metrics.append(model_metrics_df)\n        \n        # Combine all models' metrics\n        final_df = all_models_metrics[0]\n        for df in all_models_metrics[1:]:\n            final_df = final_df.union_all(df)\n        \n        return final_df.order_by(['MODEL_NAME', 'MODEL_VERSION_NAME', 'MODEL_MONITOR_NAME', 'EVENT_TIMESTAMP'])\n    \n    # Function to retrieve drift metrics for features from the model registry\n    def get_model_drift_metrics(self, models, metrics, start_date, end_date, aggregation, columns):\n        \"\"\"\n        Fetches drift metrics from the model registry for given models and metrics.\n        \n        Args:\n            session: Active database session.\n            models (list): List of models to fetch metrics for.\n            metrics (list): List of performance metrics to retrieve.\n            start_date (str): Start date for metrics retrieval.\n            end_date (str): End date for metrics retrieval.\n            aggregation (str): Aggregation window (e.g., '1 day').\n            columns (list): List of columns to fetch metrics for.\n        \n        Returns:\n            DataFrame: Aggregated drift metrics.\n        \"\"\"\n        \n        # Validate requested metrics\n        invalid_metrics = set(metrics) - set(self.ALLOWED_DRIFT_METRICS)\n        if invalid_metrics:\n            raise ValueError(f\"Invalid metric(s) found: {invalid_metrics}\")\n        \n        all_models_metrics = []\n        \n        # Iterate through each model\n        for model in models:\n            model_name = model.model_name\n            monitor_name = str(self.registry.get_monitor(model_version=model).name)\n            \n            model_metrics_dfs = []\n            \n            # Fetch each metric for the model\n            for column in columns:\n                column_metrics_dfs = []\n                for metric in metrics:\n                    df_metric = (\n                        self.session.table_function(\n                            \"MODEL_MONITOR_DRIFT_METRIC\",\n                            lit(monitor_name), lit(metric), lit(column), lit(aggregation), lit(start_date), lit(end_date)\n                        )\n                        .with_column('MODEL_NAME', lit(model_name))\n                        .with_column('MODEL_VERSION_NAME', lit(model.version_name))\n                        .with_column('MODEL_MONITOR_NAME', lit(monitor_name))\n                        .rename({'METRIC_VALUE': metric})\n                        .select(['MODEL_NAME', 'MODEL_VERSION_NAME', 'MODEL_MONITOR_NAME', 'EVENT_TIMESTAMP', 'COLUMN_NAME', metric])\n                    )\n                    column_metrics_dfs.append(df_metric)\n                column_metrics_df = column_metrics_dfs[0]\n                for df in column_metrics_dfs[1:]:\n                    column_metrics_df = column_metrics_df.join(df, on=['MODEL_NAME', 'MODEL_VERSION_NAME', 'MODEL_MONITOR_NAME', 'EVENT_TIMESTAMP', 'COLUMN_NAME'], how='inner')\n                model_metrics_dfs.append(column_metrics_df)\n\n            # Combine metrics for the model\n            model_metrics_df = model_metrics_dfs[0]\n            for df in model_metrics_dfs[1:]:\n                model_metrics_df = model_metrics_df.union_all(df)\n        \n            all_models_metrics.append(model_metrics_df)\n        \n        # Combine all models' metrics\n        final_df = all_models_metrics[0]\n        for df in all_models_metrics[1:]:\n            final_df = final_df.union_all(df)\n        \n        return final_df.order_by(['MODEL_NAME', 'MODEL_VERSION_NAME', 'MODEL_MONITOR_NAME', 'COLUMN_NAME', 'EVENT_TIMESTAMP'])\n\n    def get_all_models(self):\n        all_models = self.registry.show_models()\n        all_models['model_task'] = all_models['name'].apply(lambda x: str(self.registry.get_model(x).version('default').get_model_task()))\n        all_models['versions'] = all_models['versions'].apply(lambda x: ast.literal_eval(x))\n        all_models['aliases'] = all_models['aliases'].apply(lambda x: ast.literal_eval(x))\n        all_models = all_models.explode('versions')\n        all_models = all_models.rename(columns={'versions': 'model_version', 'name': 'model_name'})\n        all_models = all_models.sort_values(['model_name', 'created_on', 'model_version'])\n        all_models = all_models[['model_name', 'model_version', 'aliases', 'model_task']]\n        self.all_models = all_models\n        return all_models\n\n    def get_all_monitors(self):\n        all_monitors = pd.DataFrame(self.registry.show_model_monitors())\n        all_monitors['model'] = all_monitors['model'].apply(lambda x: ast.literal_eval(x))\n        all_monitors['source'] = all_monitors['source'].apply(lambda x: ast.literal_eval(x))\n        all_monitors['model_name'] = all_monitors['model'].apply(lambda x: x['model_name'])\n        all_monitors['model_version'] = all_monitors['model'].apply(lambda x: x['version_name'])\n        all_monitors['monitor_columns'] = all_monitors['source'].apply(lambda x: self.session.table(f\"{x['database_name']}.{x['schema_name']}.{x['name']}\").columns)\n        all_monitors = all_monitors[['model_name', 'model_version', 'monitor_columns']]\n        \n        self.all_monitors = all_monitors\n        return all_monitors\n\n    def update_all_models(self):\n        self.get_all_models()\n\n    def update_all_monitors(self):\n        self.get_all_monitors()\n\n    def update_registry_data(self):\n        self.get_all_models()\n        self.get_all_monitors()\n\n    def plot_model_performance(self):\n        if not hasattr(self, 'all_models'):\n            self.get_all_models()\n        if not hasattr(self, 'all_monitors'):\n            self.get_all_monitors()\n\n        all_models = self.all_models\n        all_monitors = self.all_monitors\n            \n        with st.expander('Select Models:', expanded=True):\n            selection = st.dataframe(all_models, selection_mode='multi-row', on_select=\"rerun\", hide_index=True, use_container_width=True)\n        \n        if len(selection['selection']['rows']) == 0:\n            st.info('Select models.')\n        else:\n            selected_models = all_models.iloc[selection['selection']['rows']]\n            if selected_models['model_task'].nunique() > 1:\n                st.error('All selected models must have the same task.')\n            else:\n                with st.form(\"my_form\"):\n                    col1, col2 = st.columns(2)\n                    if selected_models.iloc[0]['model_task'] == 'Task.TABULAR_REGRESSION':\n                        selected_performance_metric = col1.selectbox('Select Model Performance Metric:', self.ALLOWED_MODEL_TYPES_METRICS['Task.TABULAR_REGRESSION'])\n                    else:\n                        selected_performance_metric = col1.selectbox('Select Model Performance Metric:', self.ALLOWED_MODEL_TYPES_METRICS['Task.TABULAR_BINARY_CLASSIFICATION'])\n                    selected_drift_metric = col2.selectbox('Select Model Drift Metric:', self.ALLOWED_DRIFT_METRICS)\n\n                    models = selected_models.apply(lambda row: self.registry.get_model(row['model_name']).version(row['model_version']), axis=1).tolist()\n                    \n                    selected_columns = st.multiselect('Select Drift columns:', all_monitors['monitor_columns'].explode().unique())\n                    submitted = st.form_submit_button(\"Visualize Model Performance\")\n                    \n                    if submitted:\n                        df_model = self.get_model_performance_metrics(\n                            models=models,\n                            metrics=[selected_performance_metric],\n                            start_date='2024-01-01',\n                            end_date='2024-12-31',\n                            aggregation='1 day'\n                        ).to_pandas()\n        \n                        df_drift = self.get_model_drift_metrics(\n                            models=models,\n                            metrics=[selected_drift_metric],\n                            start_date='2024-01-01',\n                            end_date='2024-12-31',\n                            aggregation='1 day',\n                            columns=selected_columns\n                        ).to_pandas()\n\n                        df_drift[\"EVENT_TIMESTAMP\"] = pd.to_datetime(df_drift[\"EVENT_TIMESTAMP\"])\n                        df_model[\"EVENT_TIMESTAMP\"] = pd.to_datetime(df_model[\"EVENT_TIMESTAMP\"])\n        \n                        fig = go.Figure()\n                        \n                        for model_version in df_model[\"MODEL_VERSION_NAME\"].unique():\n                            df_subset = df_model[df_model[\"MODEL_VERSION_NAME\"] == model_version]\n                            fig.add_trace(go.Scatter(\n                                x=df_subset[\"EVENT_TIMESTAMP\"],\n                                y=df_subset[selected_performance_metric],\n                                mode='lines+markers',\n                                line=dict(dash='solid', width=4),\n                                marker=dict(symbol='diamond', size=12),\n                                name=f\"{df_subset.iloc[0]['MODEL_NAME']} - {model_version}\",\n                                yaxis='y1',\n                                legendgroup='model_metrics',\n                                legendgrouptitle_text='Model Metrics:'\n                            ))\n                        \n                        for model_version in df_drift[\"MODEL_VERSION_NAME\"].unique():\n                            df_subset = df_drift[df_drift[\"MODEL_VERSION_NAME\"] == model_version]\n                            for column_name in df_subset[\"COLUMN_NAME\"].unique():\n                                df_subsubset = df_subset[df_subset[\"COLUMN_NAME\"] == column_name]\n                                fig.add_trace(go.Scatter(\n                                    x=df_subsubset[\"EVENT_TIMESTAMP\"],\n                                    y=df_subsubset[selected_drift_metric],\n                                    mode='lines+markers',\n                                    line=dict(dash='dot', width=2),\n                                    marker=dict(symbol='square', size=8),\n                                    name=f'{column_name}',\n                                    yaxis='y2',\n                                    legendgroup=f\"{df_subsubset.iloc[0]['MODEL_NAME']} - {df_subsubset.iloc[0]['MODEL_VERSION_NAME']}\",\n                                    legendgrouptitle_text=f\"Drift: {df_subsubset.iloc[0]['MODEL_NAME']} - {df_subsubset.iloc[0]['MODEL_VERSION_NAME']}\"\n                                ))\n        \n                        fig.update_layout(\n                            title = {'text': \"Model Performance & Feature Drift Over Time\", 'font': {'size':24,'family':'Arial, sans-serif'}, 'xanchor':'center', 'x':0.5},\n                            xaxis_title=\"Event Timestamp\",\n                            xaxis=dict(type='date'),\n                            yaxis=dict(title=selected_performance_metric, side=\"left\", showgrid=False),\n                            yaxis2=dict(title=selected_drift_metric, overlaying=\"y\", side=\"right\", showgrid=False),\n                            legend=dict(orientation=\"h\", yanchor=\"top\", y=-0.2, xanchor=\"center\", x=0.5, traceorder=\"grouped\", itemwidth=30),\n                            margin=dict(t=50),\n                            legend_tracegroupgap=10,\n                            template=\"plotly_white\"\n                        )\n                        \n                        st.plotly_chart(fig)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be47bf4f-2462-46a4-819f-3f6629e89165",
   "metadata": {
    "language": "python",
    "name": "cell70"
   },
   "outputs": [],
   "source": "model_registry_helper = ModelRegistryHelper(session, reg)\nmodel_registry_helper.plot_model_performance()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aa7e2-971d-4479-9d17-570bd066cc0b",
   "metadata": {
    "language": "python",
    "name": "cell50",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_inshop_vs_online_revenue(transactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2686a6c-4210-45f7-ab2d-32fb61a9d11f",
   "metadata": {
    "collapsed": false,
    "name": "NEW_MODEL_1"
   },
   "source": "## Train a new Model Version  \n\nSince **user behavior has changed**, we will train a **new version of our model** using fresh data.  \n\nTo streamline this process, I have encapsulated the entire training workflow into the helper function `train_new_model()`, which automates the following steps:  \n\n- **Creates the spine DataFrame**, including the target variable.  \n- **Retrieves features** from the Feature Store.  \n- **Creates a Snowflake Dataset** from the training data (ensuring reproducibility with a snapshot).  \n- **Trains a new XGBoost model**.  \n- **Registers the model** in the Snowflake Model Registry.  \n- **Sets up a new model monitor** to track performance and drift.  \n- **Compares model performance** against the existing production model.  \n- **Deploys the new model** if it outperforms the current one by assigning it the **\"PRODUCTION\"** alias.  \n\nSince the training data includes **June, July, and August 2024** (covering training data up to **August 31, 2024**, and looking back three months), the model should recognize that **ONLINE transactions** have become a major driver of customer revenue."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02127e0-da97-460b-9b3c-bbf82c6dc7ba",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "NEW_MODEL_2"
   },
   "outputs": [],
   "source": [
    "feature_cutoff_date = '2024-08-31'\n",
    "target_start_date = '2024-09-01'\n",
    "target_end_date = '2024-09-30'\n",
    "model_version = 'V2'\n",
    "\n",
    "train_new_model(session, feature_cutoff_date, target_start_date, target_end_date, model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75783fe-e8ed-435f-ab39-8d592166576d",
   "metadata": {
    "name": "NEW_MODEL_3",
    "collapsed": false
   },
   "source": "### Simulate Model performance for Model Version V2 until 2025-01-31\nOnce again, we are simulating **model performance** based on customer transactions up to **January 2025**.  \nMake sure to check the **model monitor** to evaluate whether the new model version trained on more recent data performs better.  \nAdditionally, analyze the **feature drift**, where you‚Äôll notice that the trend for the **V2 model** is much more stable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781793a-c85f-40a5-8264-bbb7e9e8b4d7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell61"
   },
   "outputs": [],
   "source": "start_date = '2024-10-01'\nend_date = '2025-01-31'\nmodel_version = 'V2'\n\nsimulate_model_performance(session, start_date, end_date, model_version, generate_data=False)"
  },
  {
   "cell_type": "markdown",
   "id": "55a76de0-74ac-4f25-84f3-4717b33c51fb",
   "metadata": {
    "name": "cell56",
    "collapsed": false
   },
   "source": "### Comparing the two Model Versions\nWe have already observed that the new model provides **significantly better predictions** for future customer revenue. However, we want to gain deeper insights into **why** this improvement occurred.  \n\nTo analyze this, I am plotting the **feature importance** for both models. This reveals that the new model recognizes a **much stronger influence** of past **ONLINE transactions** on future customer revenue.  \n\nAdditionally, we can leverage the model's **explainability features**, using **SHAP values**, to further visualize and understand these relationships."
  },
  {
   "cell_type": "code",
   "id": "694fbbff-0f53-4a6e-a83b-c2074f695ca9",
   "metadata": {
    "language": "python",
    "name": "cell57",
    "collapsed": false
   },
   "outputs": [],
   "source": "compare_two_models(session,'V1','V2')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64eaba4b-dfed-4268-8c47-fb3a5a59414e",
   "metadata": {
    "language": "python",
    "name": "cell9",
    "collapsed": false
   },
   "outputs": [],
   "source": "import shap\ncol1, col2 = st.columns(2)\n\nwith col1:\n    st.markdown('### Global Feature Importance: Model V1')\n    explaination_df = session.table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V2').filter(col('FEATURE_CUTOFF_DATE') == '2025-01-31')\n    mv = reg.get_model('CUSTOMER_REVENUE_MODEL').version('V1')\n    explanations = mv.run(explaination_df, function_name=\"explain\")\n    explanations = explanations.rename({col:col.replace('\"\"\"', '').upper() for col in explanations.columns})\n    shap_columns = [col for col in explanations.columns if '_EXPLANATION' in col]\n    explanations = explanations.select('CUSTOMER_ID', *shap_columns)\n    explanations = explanations.to_pandas()\n    \n    shap_exp = shap._explanation.Explanation(explanations[shap_columns].values, feature_names = [col.replace('_EXPLANATION','') for col in shap_columns]) \n    shap.plots.bar(shap_exp)\nwith col2:\n    st.markdown('### Global Feature Importance: Model V2')\n    explaination_df = session.table('SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.MM_TRANS_SOURCE_V2').filter(col('FEATURE_CUTOFF_DATE') == '2025-01-31')\n    mv = reg.get_model('CUSTOMER_REVENUE_MODEL').version('V2')\n    explanations = mv.run(explaination_df, function_name=\"explain\")\n    explanations = explanations.rename({col:col.replace('\"\"\"', '').upper() for col in explanations.columns})\n    shap_columns = [col for col in explanations.columns if '_EXPLANATION' in col]\n    explanations = explanations.select('CUSTOMER_ID', *shap_columns)\n    explanations = explanations.to_pandas()\n    \n    shap_exp = shap._explanation.Explanation(explanations[shap_columns].values, feature_names = [col.replace('_EXPLANATION','') for col in shap_columns]) \n    shap.plots.bar(shap_exp)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6177a3c1-8d98-4900-83ea-f0ec4a910b20",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "## ML Lineage\nEven though you may not have noticed, you‚Äôve been capturing **lineage information** throughout the development of your machine learning pipeline.  \n\nYou can retrieve this information using the built-in function `lineage.trace()` for further analysis.  \nFor example, you can use this data to **visualize the lineage directly in the notebook**.  \n\nAdditionally, Snowflake provides a **more user-friendly and interactive UI** that allows you to explore and monitor your machine learning pipeline:  \n![text](https://github.com/michaelgorkow/snowflake_simple_mlops/blob/main/resources/ml_lineage3.png?raw=true)\n\nAs shown, the lineage captures a **comprehensive view** of your pipeline, tracking data transformations and dependencies from the **source tables**, through the **feature view**, the **training dataset**, and ultimately the **registered model** in the Model Registry."
  },
  {
   "cell_type": "code",
   "id": "84fab3ee-19e7-4a34-a86a-cd09efec9065",
   "metadata": {
    "language": "python",
    "name": "cell45",
    "collapsed": false
   },
   "outputs": [],
   "source": "trace = session.lineage.trace(\n    object_name='SIMPLE_MLOPS_DEMO.MODEL_REGISTRY.CUSTOMER_REVENUE_MODEL',\n    object_version='V1',\n    object_domain='model',\n    direction='both',\n    distance=2\n)\ntrace.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3924a8b0-75ca-46b3-bd57-c3edd59a16a4",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "collapsed": false
   },
   "outputs": [],
   "source": "visualize_lineage(trace.to_pandas(), short_names=True)",
   "execution_count": null
  }
 ]
}